{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/llandrew/anaconda3/bin/python\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import pdb\n",
    "\n",
    "import italian_vocab\n",
    "import chinese\n",
    "\n",
    "n = 4   # number of letters as context\n",
    "\n",
    "def build_model(word_file, n):\n",
    "    \"\"\"\n",
    "    Build n-gram model of the words in  words_lang.txt\n",
    "    \"\"\"\n",
    "    # read in all n+1 grams\n",
    "    n_plus_1_gram_counts = {}\n",
    "    with open(word_file, \"r\") as f:\n",
    "        for word in f:\n",
    "            if not word or word[0] == \"#\" or \" \" in word:\n",
    "                continue\n",
    "            word = \"^\" * n + word.strip() + \"$\"\n",
    "            for i in range(len(word) - n):\n",
    "                n_plus_1_gram = word[i:i + n + 1]\n",
    "                if n_plus_1_gram not in n_plus_1_gram_counts:\n",
    "                    n_plus_1_gram_counts[n_plus_1_gram] = 1\n",
    "                else:\n",
    "                    n_plus_1_gram_counts[n_plus_1_gram] += 1\n",
    "    # with open (\"tmpppp\", \"w\") as f :\n",
    "    #  for key in sorted(n_plus_1_gram_counts) :\n",
    "    #    print (str({key: n_plus_1_gram_counts[key]})[1:-1], file=f)\n",
    "\n",
    "    # Find conditional probability of n+1st from previous n-gram\n",
    "    counts = {}\n",
    "    n_grams = {x: {} for x in range(0, n + 1)}\n",
    "    for N in range(n, -1, -1):\n",
    "        n_gram = None\n",
    "        for ng in sorted(n_plus_1_gram_counts):\n",
    "            next_n = ng[:-1]\n",
    "            if n_gram != next_n:\n",
    "                if n_gram == None:\n",
    "                    n_gram = next_n\n",
    "                else:\n",
    "                    s = sum([counts[x] for x in counts])\n",
    "                    if not n_gram.startswith('^^'):  # at most one ^ at the start\n",
    "                        n_grams[N][n_gram] = {c: counts[c] for c in counts}\n",
    "\n",
    "                    n_gram = next_n\n",
    "                    counts = {}\n",
    "\n",
    "            counts[ng[-1]] = n_plus_1_gram_counts[ng]\n",
    "\n",
    "        # process last case.  Should we put a sentinel in n_plus_1_gram_counts?\n",
    "        s = sum([counts[x] for x in counts])\n",
    "        n_grams[N][n_gram] = {c: counts[c] for c in counts}\n",
    "\n",
    "        if N > 0:\n",
    "            new_n = {}\n",
    "            for key in n_plus_1_gram_counts:\n",
    "                suff = key[1:]\n",
    "                if suff in new_n:\n",
    "                    new_n[suff] += 1\n",
    "                else:\n",
    "                    new_n[suff] = 1\n",
    "            n_plus_1_gram_counts = new_n\n",
    "\n",
    "    # compress to variable n?\n",
    "    # print(n_grams)\n",
    "\n",
    "    for N in range(n):\n",
    "        n_grams[N + 1].update(n_grams[N])\n",
    "\n",
    "    return n_grams[n]\n",
    "\n",
    "def word_prob(word, model):\n",
    "    word = \"^\" + word + \"$\"\n",
    "    pos = 1  # char after ^\n",
    "    log_likelihood = 0\n",
    "    # print (\"   \", end=\"\")\n",
    "    while pos < len(word):\n",
    "        done = False\n",
    "        for i in range(pos):\n",
    "            history = word[i:pos]\n",
    "            if history in model:\n",
    "                if history not in model:\n",
    "                    # Should penalize this\n",
    "                    history = history.lower()\n",
    "                try:\n",
    "                    log_likelihood += math.log(model[history][word[pos]] / sum(model[history].values()))\n",
    "                except:\n",
    "                    low = word[pos].lower()\n",
    "                    if low in model[history]:\n",
    "                        # Should penalize this\n",
    "                        log_likelihood += math.log(model[history][low] / sum(model[history].values()))\n",
    "                    else:\n",
    "                        # pdb.set_trace ()\n",
    "                        log_likelihood += -20\n",
    "                # print (int(log_likelihood), end = \" \")\n",
    "                log_likelihood -= 3 * i  # penalize shorter histories\n",
    "                done = True\n",
    "                pos += 1\n",
    "                break\n",
    "\n",
    "        if not done:  # Transisition so unlikely, it was never seen\n",
    "            # Should use \"smoothing\", but this will disappear\n",
    "            # when the variable-length prefixes are implemented\n",
    "            log_likelihood += -20\n",
    "            # print (int(log_likelihood), end = \" \")\n",
    "            pos += 1\n",
    "\n",
    "    # print()\n",
    "\n",
    "    return log_likelihood / (len(word) - 1)  # didn't guess \"^\"\n",
    "\n",
    "\n",
    "def word_prob_blend(word, model, n=4):\n",
    "    word = \"^\" * n + word + \"$\"\n",
    "    pos = n  # char after n ^\n",
    "    log_likelihood = 0\n",
    "    # print (\"   \", end=\"\")\n",
    "    while pos < len(word):\n",
    "        prefix = word[pos - n:pos]\n",
    "        prob = blend(prefix, word[pos], model)\n",
    "        log_likelihood += math.log (prob)\n",
    "        pos += 1\n",
    "\n",
    "\n",
    "    return log_likelihood / (len(word) - 1)  # didn't guess \"^\"\n",
    "\n",
    "def blend(prefix, x, model, n=4):\n",
    "    prob = 0\n",
    "    alpha = 0.5\n",
    "    beta = 0.75\n",
    "    possible_x = 26 + 26 + 1\n",
    "\n",
    "    # first half of algorithm\n",
    "    if prefix in model:\n",
    "        base = (len(model[prefix])*beta + alpha) / (sum(model[prefix].values()) + alpha)\n",
    "        if x in model[prefix]:\n",
    "            prob += (model[prefix][x] - beta) / (sum(model[prefix].values()) + alpha)\n",
    "    else:\n",
    "        base = (0 + alpha) / (0 + alpha)\n",
    "        \n",
    "    if len(prefix) == 0:\n",
    "        prob += base / possible_x\n",
    "    else:\n",
    "        prob += base * blend(prefix[1:], x, model, n)\n",
    "        \n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081fbfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_gram_model = build_model(\"words_manual_en.txt\", 4)\n",
    "print(n_gram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "n_gram_model = build_model(\"words_manual_en.txt\", 4)\n",
    "cand = [chr(i) for i in range(ord('A'), ord('Z') + 1)] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\"$\"]\n",
    "\n",
    "def verify(prefix, model, n=4):\n",
    "    s = 0\n",
    "    for c in cand:\n",
    "        prob = blend(prefix, c, model)\n",
    "        s += blend(prefix, c, model)\n",
    "        \n",
    "    return s\n",
    "\n",
    "print(verify(\"ralv\", n_gram_model))\n",
    "print(verify(\"^All\", n_gram_model))\n",
    "print(verify(\"Trum\", n_gram_model))\n",
    "print(verify(\"Russ\", n_gram_model))\n",
    "print(verify(\"Koch\", n_gram_model))\n",
    "print(verify(\"Rack\", n_gram_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_model = build_model(\"words_manual_en.txt\", 4)\n",
    "count = 0\n",
    "with open (\"checked_words.txt\", \"r\") as f :\n",
    "    for word in f:\n",
    "        score = word_prob (word, n_gram_model)\n",
    "        blend_score = word_prob_blend (word, n_gram_model)\n",
    "        print(f\"word: {word}, original_score: {score}, blend_score: {blend_score}\")\n",
    "        \n",
    "        count += 1\n",
    "        if count == 100:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative first half (log P(s) / alpha)\n",
    "import numpy as np\n",
    "def derivative_of_P_to_alpha(alpha, beta, model, prefix, x):\n",
    "    if len(prefix) == 0:\n",
    "        return 0\n",
    "    next_prefix = prefix[1:]\n",
    "    \n",
    "    if prefix in model:\n",
    "        ms = sum(model[prefix].values())\n",
    "        us = len(model[prefix])\n",
    "        if x in model[prefix]:\n",
    "            ms_x = model[prefix][x]\n",
    "        else:\n",
    "            ms_x = 0\n",
    "    else:\n",
    "        ms = 0\n",
    "        us = 0\n",
    "        ms_x = 0\n",
    "        \n",
    "    eq1 = (beta-ms_x) / (ms+alpha)**2\n",
    "    eq2 = (ms-us*beta) / (ms+alpha)**2 * blend(next_prefix, x, model)\n",
    "    eq3 = (alpha + us*beta) / (ms+alpha) * derivative_of_P_to_beta(alpha, beta, model, next_prefix, x)\n",
    "    \n",
    "    return eq1 + eq2 + eq3\n",
    "        \n",
    "def derivative_of_P_to_beta(alpha, beta, model, prefix, x):\n",
    "    if len(prefix) == 0:\n",
    "        return 0\n",
    "    next_prefix = prefix[1:]\n",
    "    \n",
    "    if prefix in model:\n",
    "        ms = sum(model[prefix].values())\n",
    "        us = len(model[prefix])\n",
    "        if x in model[prefix]:\n",
    "            ms_x = model[prefix][x]\n",
    "        else:\n",
    "            ms_x = 0\n",
    "    else:\n",
    "        ms = 0\n",
    "        us = 0\n",
    "        ms_x = 0\n",
    "        \n",
    "    eq1 = -1 / (ms+alpha)\n",
    "    eq2 = us / (ms+alpha) * blend(next_prefix, x, model, n)\n",
    "    eq3 = (alpha + us*beta) / (ms+alpha) * derivative_of_P_to_alpha(alpha, beta, model, next_prefix, x)\n",
    "    \n",
    "    return eq1 + eq2 + eq3\n",
    "        \n",
    "\n",
    "def update_param(alpha_matrix, beta_matrix, prefix, x, model, n=4):\n",
    "    learning_rate = 0.003\n",
    "    param_derivative_mapping = [1, len(set(prefix))]\n",
    "    # add a computation for alpha beta\n",
    "    alpha = np.dot(alpha_matrix[n], param_derivative_mapping)\n",
    "    beta = np.dot(beta_matrix[n], param_derivative_mapping)\n",
    "    \n",
    "    \n",
    "    for ind, alpha_derivative in zip(range(len(alpha_matrix[n])), param_derivative_mapping):\n",
    "        derivative_alpha = 1 / blend(prefix, x, model, n) * derivative_of_P_to_alpha(alpha, beta, n_gram_model, prefix, x) * alpha_derivative\n",
    "        alpha_matrix[n][ind] += learning_rate * derivative_alpha\n",
    "        \n",
    "    for ind, beta_derivative in zip(range(len(beta_matrix[n])), param_derivative_mapping):\n",
    "        derivative_beta = 1 / blend(prefix, x, model, n) * derivative_of_P_to_beta(alpha, beta, n_gram_model, prefix, x) * beta_derivative\n",
    "        beta_matrix[n][ind] += learning_rate * derivative_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check derivative\n",
    "\n",
    "a1 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"epri\", \"m\")\n",
    "a2 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"epri\", \"x\")\n",
    "a3 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"epri\", \"f\")\n",
    "\n",
    "print(a1, a2, a3)\n",
    "\n",
    "b1 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"Acke\", \"l\")\n",
    "b2 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"Acke\", \"x\")\n",
    "b3 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"Acke\", \"f\")\n",
    "\n",
    "print(b1, b2, b3)\n",
    "\n",
    "c1 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"epri\", \"m\")\n",
    "c2 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"epri\", \"x\")\n",
    "c3 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"epri\", \"f\")\n",
    "\n",
    "print(c1, c2, c3)\n",
    "\n",
    "d1 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"Acke\", \"l\")\n",
    "d2 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"Acke\", \"x\")\n",
    "d3 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"Acke\", \"f\")\n",
    "\n",
    "print(d1, d2, d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16932e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_matrix = [[], [], [], [], [1, 0]]\n",
    "beta_matrix = [[], [], [], [], [1, 0]]\n",
    "\n",
    "update_param(alpha_matrix, beta_matrix, \"Acke\", \"l\", n_gram_model, n=4)\n",
    "\n",
    "print(alpha_matrix)\n",
    "print(beta_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
