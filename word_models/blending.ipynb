{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e7724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_CHAR = 2**16\n",
    "\n",
    "\n",
    "# derivative first half (log P(s) / alpha)\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "import italian_vocab\n",
    "import chinese\n",
    "\n",
    "# n = 4   # number of letters as context\n",
    "MAX_N = 4\n",
    "\n",
    "\n",
    "class Blending_with_linear_param:\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def derivative_of_P_to_alpha(self, alpha_matrix, beta_matrix, model, prefix, x):\n",
    "        \n",
    "        n = len(prefix)\n",
    "        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n",
    "        if prefix in model:\n",
    "            ms = sum(model[prefix].values())\n",
    "            us = len(model[prefix])\n",
    "            if x in model[prefix]:\n",
    "                ms_x = model[prefix][x]\n",
    "            else:\n",
    "                ms_x = 0\n",
    "        else:\n",
    "            ms = 0\n",
    "            us = 0\n",
    "            ms_x = 0\n",
    "            \n",
    "        eq1 = (beta-ms_x) / (ms+alpha)**2\n",
    "        \n",
    "        if len(prefix) == 0:\n",
    "            eq2 = (ms-us*beta) / (ms+alpha)**2 / TOTAL_CHAR\n",
    "            eq3 = 0\n",
    "        else:\n",
    "            next_prefix = prefix[1:]\n",
    "            eq2 = (ms-us*beta) / (ms+alpha)**2 * self.blend(alpha_matrix, beta_matrix, next_prefix, x, model)\n",
    "            eq3 = (alpha + us*beta) / (ms+alpha) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, next_prefix, x)\n",
    "        \n",
    "        return eq1 + eq2 + eq3\n",
    "            \n",
    "    def derivative_of_P_to_beta(self, alpha_matrix, beta_matrix, model, prefix, x):\n",
    "        \n",
    "        n = len(prefix)\n",
    "        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n",
    "        if prefix in model:\n",
    "            ms = sum(model[prefix].values())\n",
    "            us = len(model[prefix])\n",
    "            if x in model[prefix]:\n",
    "                ms_x = model[prefix][x]\n",
    "            else:\n",
    "                ms_x = 0\n",
    "        else:\n",
    "            ms = 0\n",
    "            us = 0\n",
    "            ms_x = 0\n",
    "            \n",
    "        eq1 = -1 / (ms+alpha)\n",
    "        if len(prefix) == 0:\n",
    "            eq2 = (ms-us*beta) / (ms+alpha)**2 / TOTAL_CHAR\n",
    "            eq3 = 0\n",
    "        else:\n",
    "            next_prefix = prefix[1:]\n",
    "            eq2 = us / (ms+alpha) * self.blend(alpha_matrix, beta_matrix, next_prefix, x, model, n)\n",
    "            eq3 = (alpha + us*beta) / (ms+alpha) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, next_prefix, x)\n",
    "        \n",
    "        return eq1 + eq2 + eq3\n",
    "            \n",
    "\n",
    "    def update_param(self, alpha_matrix, beta_matrix, prefix, x, model):\n",
    "        learning_rate = 0.003\n",
    "        n = len(prefix)\n",
    "        # add a computation for alpha beta\n",
    "        param_derivative_mapping = [1, len(set(prefix))]\n",
    "        \n",
    "        \n",
    "        for ind, alpha_derivative in zip(range(len(alpha_matrix[n])), param_derivative_mapping):\n",
    "            derivative_alpha = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, prefix, x) * alpha_derivative\n",
    "            alpha_matrix[n][ind] += learning_rate * derivative_alpha\n",
    "            \n",
    "        for ind, beta_derivative in zip(range(len(beta_matrix[n])), param_derivative_mapping):\n",
    "            derivative_beta = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, prefix, x) * beta_derivative\n",
    "            beta_matrix[n][ind] += learning_rate * derivative_beta\n",
    "            \n",
    "        beta_matrix[n][0] = max(0, min(1, beta_matrix[n][0]))\n",
    "        if beta_matrix[n][0] + n * beta_matrix[n][1] > 1:\n",
    "            beta_matrix[n][1] = (1 - beta_matrix[n][0]) / n\n",
    "        elif beta_matrix[n][0] + n * beta_matrix[n][1] < 0:\n",
    "            beta_matrix[n][1] = -beta_matrix[n][0] / n\n",
    "            \n",
    "\n",
    "    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix):\n",
    "        n = len(prefix)\n",
    "        param_derivative_mapping = [1, len(set(prefix))]\n",
    "        alpha = np.dot(alpha_matrix[n], param_derivative_mapping)\n",
    "        beta = np.dot(beta_matrix[n], param_derivative_mapping)\n",
    "        \n",
    "        #beta = max(0, min(1, beta))\n",
    "        alpha = max(-beta, alpha)\n",
    "        \n",
    "        \n",
    "        return (alpha, beta)\n",
    "\n",
    "    def build_model_with_train(self, alpha_matrix, beta_matrix, word_file, n):\n",
    "        \"\"\"\n",
    "        Build n-gram model of the words in  words_lang.txt\n",
    "        \"\"\"\n",
    "        n_grams = {}\n",
    "        # read in all n+1 grams\n",
    "        n_plus_1_gram_counts = {}\n",
    "        with open(word_file, \"r\") as f:\n",
    "            for word in f:\n",
    "                # update n-gram\n",
    "                if not word or word[0] == \"#\" or \" \" in word:\n",
    "                    continue\n",
    "                word = \"^\" * n + word.strip() + \"$\"\n",
    "                for i in range(len(word) - n):\n",
    "                    n_plus_1_gram = word[i:i + n + 1]\n",
    "                        \n",
    "                    for N in range(n, -1, -1):\n",
    "                        x = n_plus_1_gram[-1]\n",
    "                        next_n = n_plus_1_gram[:-1]\n",
    "                        \n",
    "                        if next_n not in n_grams:\n",
    "                            n_grams[next_n] = {}\n",
    "                            \n",
    "                        if x not in n_grams[next_n]:\n",
    "                            n_grams[next_n][x] = 1\n",
    "                        else:\n",
    "                            n_grams[next_n][x] += 1\n",
    "                            \n",
    "                        n_plus_1_gram = n_plus_1_gram[1:]\n",
    "                        \n",
    "                # update alpha and beta\n",
    "                for i in range(len(word) - n):\n",
    "                    n_plus_1_gram = word[i:i + n + 1]\n",
    "                        \n",
    "                    for N in range(n, -1, -1):\n",
    "                        x = n_plus_1_gram[-1]\n",
    "                        next_n = n_plus_1_gram[:-1]\n",
    "                        \n",
    "                        self.update_param(alpha_matrix, beta_matrix, next_n, x, n_grams)\n",
    "                            \n",
    "                        n_plus_1_gram = n_plus_1_gram[1:]\n",
    "\n",
    "            return n_grams\n",
    "    \n",
    "    def blend(self, alpha_matrix, beta_matrix, prefix, x, model, n=4):\n",
    "        prob = 0\n",
    "        \n",
    "        # compute alpha and beta\n",
    "        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n",
    "        possible_x = TOTAL_CHAR\n",
    "\n",
    "        # first half of algorithm\n",
    "        if prefix in model:\n",
    "            base = (len(model[prefix])*beta + alpha) / (sum(model[prefix].values()) + alpha)\n",
    "            if x in model[prefix]:\n",
    "                prob += (model[prefix][x] - beta) / (sum(model[prefix].values()) + alpha)\n",
    "        else:\n",
    "            base = (0 + alpha) / (0 + alpha)\n",
    "            \n",
    "        if len(prefix) == 0:\n",
    "            prob += base / possible_x\n",
    "        else:\n",
    "            prob += base * self.blend(alpha_matrix, beta_matrix, prefix[1:], x, model, n)\n",
    "            \n",
    "        return prob\n",
    "\n",
    "\n",
    "    def word_prob_blend(self, word, model, n=4):\n",
    "        word = \"^\" * n + word + \"$\"\n",
    "        pos = n  # char after n ^\n",
    "        log_likelihood = 0\n",
    "        # print (\"   \", end=\"\")\n",
    "        while pos < len(word):\n",
    "            prefix = word[pos - n:pos]\n",
    "            prob = self.blend(prefix, word[pos], model)\n",
    "            log_likelihood += math.log (prob)\n",
    "            pos += 1\n",
    "\n",
    "\n",
    "        return log_likelihood / (len(word) - 1)  # didn't guess \"^\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9e8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer with 2d matrix for d and f\n",
    "\n",
    "class Blending_with_2d_matrix(Blending_with_linear_param):\n",
    "    def init(self):\n",
    "        return None\n",
    "    \n",
    "    def update_param(self, alpha_matrix, beta_matrix, prefix, x, model):\n",
    "        max_f = len(alpha_matrix[0])\n",
    "        learning_rate = 0.003\n",
    "        d = len(prefix)\n",
    "        n = d\n",
    "        f = len(set(prefix))\n",
    "        \n",
    "        if f > max_f:\n",
    "            f = max_f\n",
    "            \n",
    "        derivative_beta = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, prefix, x)\n",
    "        beta_matrix[d][f] += learning_rate * derivative_beta\n",
    "        \n",
    "        beta_matrix[d][f] = max(0, min(1, beta_matrix[d][f]))\n",
    "        \n",
    "        derivative_alpha = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, prefix, x)\n",
    "        alpha_matrix[d][f] += learning_rate * derivative_alpha\n",
    "        \n",
    "        alpha_matrix[d][f] = max(-beta_matrix[d][f], alpha_matrix[d][f])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix):\n",
    "        max_f = len(alpha_matrix[0])\n",
    "        d = len(prefix)\n",
    "        f = len(set(prefix))\n",
    "        \n",
    "        if f > max_f:\n",
    "            f = max_f\n",
    "        \n",
    "        return (alpha_matrix[d][f], beta_matrix[d][f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "721ad0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0000000000000004\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# test build model and train\n",
    "alpha_matrix = [[0.5, 0], [0.5, 0], [0.5, 0], [0.5, 0], [0.5, 0]]\n",
    "beta_matrix = [[0.75, 0], [0.75, 0], [0.75, 0], [0.75, 0], [0.75, 0]]\n",
    "\n",
    "trainer_1 = Blending_with_linear_param()\n",
    "n_gram_model_1 = trainer_1.build_model_with_train(alpha_matrix, beta_matrix, \"words_manual_en.txt\", 4)\n",
    "\n",
    "# verify\n",
    "cand = [chr(i) for i in range(ord('A'), ord('Z') + 1)] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\"$\"]\n",
    "\n",
    "# alpha_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n",
    "# beta_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n",
    "def verify(trainer, prefix, model, n=4):\n",
    "    s = 0\n",
    "    for c in cand:\n",
    "        prob = trainer.blend(alpha_matrix, beta_matrix, prefix, c, model)\n",
    "        s += prob\n",
    "        \n",
    "    return s\n",
    "\n",
    "print(verify(trainer_1, \"ralv\", n_gram_model_1))\n",
    "print(verify(trainer_1, \"^All\", n_gram_model_1))\n",
    "print(verify(trainer_1, \"Trum\", n_gram_model_1))\n",
    "print(verify(trainer_1, \"Russ\", n_gram_model_1))\n",
    "print(verify(trainer_1, \"Koch\", n_gram_model_1))\n",
    "print(verify(trainer_1, \"Rack\", n_gram_model_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f6cf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9999999999999896\n",
      "1.0\n",
      "0.9999999999994359\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# test build model and train\n",
    "alpha_matrix = [[0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]]\n",
    "beta_matrix = [[0.75, 0.75, 0.75, 0.75, 0.75, 0.75], [0.75, 0.75, 0.75, 0.75, 0.75, 0.75], [0.75, 0.75, 0.75, 0.75, 0.75, 0.75], [0.75, 0.75, 0.75, 0.75, 0.75, 0.75], [0.75, 0.75, 0.75, 0.75, 0.75, 0.75]]\n",
    "\n",
    "trainer_2 = Blending_with_2d_matrix()\n",
    "n_gram_model_2 = trainer_2.build_model_with_train(alpha_matrix, beta_matrix, \"words_manual_en.txt\", 4)\n",
    "\n",
    "# verify\n",
    "cand = [chr(i) for i in range(ord('A'), ord('Z') + 1)] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\"$\"]\n",
    "\n",
    "def verify(trainer, prefix, model, n=4):\n",
    "    s = 0\n",
    "    for c in cand:\n",
    "        prob = trainer.blend(alpha_matrix, beta_matrix, prefix, c, model)\n",
    "        s += prob\n",
    "        \n",
    "    return s\n",
    "\n",
    "print(verify(trainer_2, \"ralv\", n_gram_model_2))\n",
    "print(verify(trainer_2, \"^All\", n_gram_model_2))\n",
    "print(verify(trainer_2, \"Trum\", n_gram_model_2))\n",
    "print(verify(trainer_2, \"Russ\", n_gram_model_2))\n",
    "print(verify(trainer_2, \"Koch\", n_gram_model_2))\n",
    "print(verify(trainer_2, \"Rack\", n_gram_model_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dfc535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.47800298113638034, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
       " [0.5, 0.0693369271729633, 0.5, 0.5, 0.5, 0.5],\n",
       " [0.5, 0.3779192188812493, -1.5900221451692142e-05, 0.5, 0.5, 0.5],\n",
       " [0.5, 0.45874650241120857, 0.05099735198599677, 0, 0.5, 0.5],\n",
       " [0.5, 0.4458847059206506, 0.360093002532607, 0.0012161018566216645, 0, 0.5]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7231b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999461523162747\n",
      "0.9999813837460723\n",
      "0.9984558384814151\n",
      "0.9998254423072425\n",
      "0.9994184450157699\n",
      "0.9998428132668576\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "alpha_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n",
    "beta_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n",
    "\n",
    "\n",
    "n_gram_model = n_gram_model_2\n",
    "cand = [chr(i) for i in range(ord('A'), ord('Z') + 1)] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\"$\"]\n",
    "\n",
    "def verify(prefix, model, n=4):\n",
    "    s = 0\n",
    "    for c in cand:\n",
    "        prob = blend(alpha_matrix, beta_matrix, prefix, c, model)\n",
    "        s += prob\n",
    "        \n",
    "    return s\n",
    "\n",
    "print(verify(\"ralv\", n_gram_model))\n",
    "print(verify(\"^All\", n_gram_model))\n",
    "print(verify(\"Trum\", n_gram_model))\n",
    "print(verify(\"Russ\", n_gram_model))\n",
    "print(verify(\"Koch\", n_gram_model))\n",
    "print(verify(\"Rack\", n_gram_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_model = build_model(\"words_manual_en.txt\", 4)\n",
    "count = 0\n",
    "with open (\"checked_words.txt\", \"r\") as f :\n",
    "    for word in f:\n",
    "        score = word_prob (word, n_gram_model)\n",
    "        blend_score = word_prob_blend (word, n_gram_model)\n",
    "        print(f\"word: {word}, original_score: {score}, blend_score: {blend_score}\")\n",
    "        \n",
    "        count += 1\n",
    "        if count == 100:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check derivative\n",
    "\n",
    "a1 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"epri\", \"m\")\n",
    "a2 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"epri\", \"x\")\n",
    "a3 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"epri\", \"f\")\n",
    "\n",
    "print(a1, a2, a3)\n",
    "\n",
    "b1 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"Acke\", \"l\")\n",
    "b2 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"Acke\", \"x\")\n",
    "b3 = derivative_of_P_to_alpha(0.5, 0.75, n_gram_model, \"Acke\", \"f\")\n",
    "\n",
    "print(b1, b2, b3)\n",
    "\n",
    "c1 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"epri\", \"m\")\n",
    "c2 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"epri\", \"x\")\n",
    "c3 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"epri\", \"f\")\n",
    "\n",
    "print(c1, c2, c3)\n",
    "\n",
    "d1 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"Acke\", \"l\")\n",
    "d2 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"Acke\", \"x\")\n",
    "d3 = derivative_of_P_to_beta(0.5, 0.75, n_gram_model, \"Acke\", \"f\")\n",
    "\n",
    "print(d1, d2, d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16932e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_matrix = [[], [], [], [], [1, 0]]\n",
    "beta_matrix = [[], [], [], [], [1, 0]]\n",
    "\n",
    "update_param(alpha_matrix, beta_matrix, \"Acke\", \"l\", n_gram_model)\n",
    "\n",
    "print(alpha_matrix)\n",
    "print(beta_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22c895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7855719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused code\n",
    "def build_model(self, word_file, n):\n",
    "        \"\"\"\n",
    "        Build n-gram model of the words in  words_lang.txt\n",
    "        \"\"\"\n",
    "        # read in all n+1 grams\n",
    "        n_plus_1_gram_counts = {}\n",
    "        with open(word_file, \"r\") as f:\n",
    "            for word in f:\n",
    "                if not word or word[0] == \"#\" or \" \" in word:\n",
    "                    continue\n",
    "                word = \"^\" * n + word.strip() + \"$\"\n",
    "                for i in range(len(word) - n):\n",
    "                    n_plus_1_gram = word[i:i + n + 1]\n",
    "                    if n_plus_1_gram not in n_plus_1_gram_counts:\n",
    "                        n_plus_1_gram_counts[n_plus_1_gram] = 1\n",
    "                    else:\n",
    "                        n_plus_1_gram_counts[n_plus_1_gram] += 1\n",
    "        # with open (\"tmpppp\", \"w\") as f :\n",
    "        #  for key in sorted(n_plus_1_gram_counts) :\n",
    "        #    print (str({key: n_plus_1_gram_counts[key]})[1:-1], file=f)\n",
    "\n",
    "        # Find conditional probability of n+1st from previous n-gram\n",
    "        counts = {}\n",
    "        n_grams = {x: {} for x in range(0, n + 1)}\n",
    "        for N in range(n, -1, -1):\n",
    "            n_gram = None\n",
    "            for ng in sorted(n_plus_1_gram_counts):\n",
    "                next_n = ng[:-1]\n",
    "                if n_gram != next_n:\n",
    "                    if n_gram == None:\n",
    "                        n_gram = next_n\n",
    "                    else:\n",
    "                        s = sum([counts[x] for x in counts])\n",
    "                        if not n_gram.startswith('^^'):  # at most one ^ at the start\n",
    "                            n_grams[N][n_gram] = {c: counts[c] for c in counts}\n",
    "\n",
    "                        n_gram = next_n\n",
    "                        counts = {}\n",
    "\n",
    "                counts[ng[-1]] = n_plus_1_gram_counts[ng]\n",
    "\n",
    "            # process last case.  Should we put a sentinel in n_plus_1_gram_counts?\n",
    "            s = sum([counts[x] for x in counts])\n",
    "            n_grams[N][n_gram] = {c: counts[c] for c in counts}\n",
    "\n",
    "            if N > 0:\n",
    "                new_n = {}\n",
    "                for key in n_plus_1_gram_counts:\n",
    "                    suff = key[1:]\n",
    "                    if suff in new_n:\n",
    "                        new_n[suff] += 1\n",
    "                    else:\n",
    "                        new_n[suff] = 1\n",
    "                n_plus_1_gram_counts = new_n\n",
    "\n",
    "        # compress to variable n?\n",
    "        # print(n_grams)\n",
    "\n",
    "        for N in range(n):\n",
    "            n_grams[N + 1].update(n_grams[N])\n",
    "\n",
    "        return n_grams[n]\n",
    "    \n",
    "def word_prob(self, word, model):\n",
    "    word = \"^\" + word + \"$\"\n",
    "    pos = 1  # char after ^\n",
    "    log_likelihood = 0\n",
    "    # print (\"   \", end=\"\")\n",
    "    while pos < len(word):\n",
    "        done = False\n",
    "        for i in range(pos):\n",
    "            history = word[i:pos]\n",
    "            if history in model:\n",
    "                if history not in model:\n",
    "                    # Should penalize this\n",
    "                    history = history.lower()\n",
    "                try:\n",
    "                    log_likelihood += math.log(model[history][word[pos]] / sum(model[history].values()))\n",
    "                except:\n",
    "                    low = word[pos].lower()\n",
    "                    if low in model[history]:\n",
    "                        # Should penalize this\n",
    "                        log_likelihood += math.log(model[history][low] / sum(model[history].values()))\n",
    "                    else:\n",
    "                        # pdb.set_trace ()\n",
    "                        log_likelihood += -20\n",
    "                # print (int(log_likelihood), end = \" \")\n",
    "                log_likelihood -= 3 * i  # penalize shorter histories\n",
    "                done = True\n",
    "                pos += 1\n",
    "                break\n",
    "\n",
    "        if not done:  # Transisition so unlikely, it was never seen\n",
    "            # Should use \"smoothing\", but this will disappear\n",
    "            # when the variable-length prefixes are implemented\n",
    "            log_likelihood += -20\n",
    "            # print (int(log_likelihood), end = \" \")\n",
    "            pos += 1\n",
    "\n",
    "    # print()\n",
    "\n",
    "    return log_likelihood / (len(word) - 1)  # didn't guess \"^\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
