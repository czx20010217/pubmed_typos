{"cells":[{"cell_type":"code","execution_count":1,"id":"10e7724a","metadata":{"id":"10e7724a","executionInfo":{"status":"ok","timestamp":1704321347365,"user_tz":-660,"elapsed":460,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}}},"outputs":[],"source":["TOTAL_CHAR = 2**16\n","\n","\n","# derivative first half (log P(s) / alpha)\n","import numpy as np\n","\n","import sys\n","import pickle\n","import math\n","import numpy as np\n","import pdb\n","\n","np.set_printoptions(suppress=True)\n","np.set_printoptions(linewidth=400)\n","# n = 4   # number of letters as context\n","MAX_N = 4\n","\n","\n","class Blending_with_linear_param:\n","    max_f = 0\n","\n","    def __init__(self, max_f):\n","        self.max_f = max_f\n","        return None\n","\n","    def derivative_of_P_to_alpha(self, alpha_matrix, beta_matrix, model, prefix, x):\n","\n","        n = len(prefix)\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix, model)\n","        if prefix in model:\n","            ms = sum(model[prefix].values())\n","            us = len(model[prefix])\n","            if x in model[prefix]:\n","                ms_x = model[prefix][x]\n","            else:\n","                ms_x = 0\n","        else:\n","            ms = 0\n","            us = 0\n","            ms_x = 0\n","\n","        eq1 = (beta-ms_x) / (ms+alpha)**2\n","\n","        if len(prefix) == 0:\n","            eq2 = (ms-us*beta) / (ms+alpha)**2 / TOTAL_CHAR\n","            eq3 = 0\n","        else:\n","            next_prefix = prefix[1:]\n","            eq2 = (ms-us*beta) / (ms+alpha)**2 * self.blend(alpha_matrix, beta_matrix, next_prefix, x, model)\n","            eq3 = (alpha + us*beta) / (ms+alpha) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, next_prefix, x)\n","\n","        return eq1 + eq2 + eq3\n","\n","    def derivative_of_P_to_beta(self, alpha_matrix, beta_matrix, model, prefix, x):\n","\n","        n = len(prefix)\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix, model)\n","        if prefix in model:\n","            ms = sum(model[prefix].values())\n","            us = len(model[prefix])\n","            if x in model[prefix]:\n","                ms_x = model[prefix][x]\n","            else:\n","                ms_x = 0\n","        else:\n","            ms = 0\n","            us = 0\n","            ms_x = 0\n","\n","        eq1 = -1 / (ms+alpha)\n","        if len(prefix) == 0:\n","            eq2 = (ms-us*beta) / (ms+alpha)**2 / TOTAL_CHAR\n","            eq3 = 0\n","        else:\n","            next_prefix = prefix[1:]\n","            eq2 = us / (ms+alpha) * self.blend(alpha_matrix, beta_matrix, next_prefix, x, model, n)\n","            eq3 = (alpha + us*beta) / (ms+alpha) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, next_prefix, x)\n","\n","        return eq1 + eq2 + eq3\n","\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x, model):\n","        learning_rate = 0.003\n","        n = len(prefix)\n","        # add a computation for alpha beta\n","        param_derivative_mapping = [1, len(set(prefix))]\n","\n","\n","        for ind, alpha_derivative in zip(range(len(alpha_matrix[n])), param_derivative_mapping):\n","            derivative_alpha = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, prefix, x) * alpha_derivative\n","            alpha_matrix[n][ind] += learning_rate * derivative_alpha\n","\n","        for ind, beta_derivative in zip(range(len(beta_matrix[n])), param_derivative_mapping):\n","            derivative_beta = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, prefix, x) * beta_derivative\n","            beta_matrix[n][ind] += learning_rate * derivative_beta\n","\n","        # restrict param for beta\n","        beta_matrix[n][0] = max(0, min(1, beta_matrix[n][0]))\n","\n","        if beta_matrix[n][0] + self.max_f * beta_matrix[n][1] > 1:\n","            beta_matrix[n][1] = (1 - beta_matrix[n][0]) / self.max_f\n","        elif beta_matrix[n][0] + self.max_f * beta_matrix[n][1] < 0:\n","            beta_matrix[n][1] = -beta_matrix[n][0] / self.max_f\n","\n","        max_beta = max(beta_matrix[n][0], self.max_f*beta_matrix[n][1])\n","        # restrict param for alpha\n","        alpha_matrix[n][0] = max(alpha_matrix[n][0], -max_beta)\n","\n","        if alpha_matrix[n][0] + self.max_f * alpha_matrix[n][1] < 0:\n","            alpha_matrix[n][1] = -alpha_matrix[n][0] / self.max_f\n","\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix, model):\n","        n = len(prefix)\n","        if prefix in model:\n","            f = len(model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        param_derivative_mapping = [1, f]\n","        # alpha1 = np.dot(alpha_matrix[n], param_derivative_mapping)\n","        # beta1 = np.dot(beta_matrix[n], param_derivative_mapping)\n","\n","        alpha = alpha_matrix[n][0] + f*  alpha_matrix[n][1]\n","        beta = beta_matrix[n][0] + f*  beta_matrix[n][1]\n","\n","        #beta = max(0, min(1, beta))\n","        # alpha = max(-beta, alpha)\n","\n","\n","        return (alpha, beta)\n","\n","    def build_model_with_train(self, data, alpha_matrix, beta_matrix, word_file, n):\n","        \"\"\"\n","        Build n-gram model of the words in  words_lang.txt\n","        \"\"\"\n","        n_grams = {}\n","        # read in all n+1 grams\n","        n_plus_1_gram_counts = {}\n","        for word in data:\n","            # update n-gram\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","            word = \"^\" * n + word.strip() + \"$\"\n","            for i in range(len(word) - n):\n","                n_plus_1_gram = word[i:i + n + 1]\n","\n","                for N in range(n, -1, -1):\n","                    x = n_plus_1_gram[-1]\n","                    next_n = n_plus_1_gram[:-1]\n","\n","                    if next_n not in n_grams:\n","                        n_grams[next_n] = {}\n","\n","                    if x not in n_grams[next_n]:\n","                        n_grams[next_n][x] = 1\n","                    else:\n","                        n_grams[next_n][x] += 1\n","\n","                    n_plus_1_gram = n_plus_1_gram[1:]\n","\n","            # update alpha and beta\n","            for i in range(len(word) - n):\n","                n_plus_1_gram = word[i:i + n + 1]\n","\n","                for N in range(n, -1, -1):\n","                    x = n_plus_1_gram[-1]\n","                    next_n = n_plus_1_gram[:-1]\n","\n","                    self.update_param(alpha_matrix, beta_matrix, next_n, x, n_grams)\n","\n","                    n_plus_1_gram = n_plus_1_gram[1:]\n","\n","        return n_grams\n","\n","    def blend(self, alpha_matrix, beta_matrix, prefix, x, model, n=4):\n","        prob = 0\n","\n","        # compute alpha and beta\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix, model)\n","        possible_x = TOTAL_CHAR\n","\n","        # first half of algorithm\n","        if prefix in model:\n","            base = (len(model[prefix])*beta + alpha) / (sum(model[prefix].values()) + alpha)\n","            if x in model[prefix]:\n","                prob += (model[prefix][x] - beta) / (sum(model[prefix].values()) + alpha)\n","        else:\n","            base = (0 + alpha) / (0 + alpha)\n","\n","        if len(prefix) == 0:\n","            prob += base / possible_x\n","        else:\n","            prob += base * self.blend(alpha_matrix, beta_matrix, prefix[1:], x, model, n)\n","\n","        return prob\n","\n","\n","    def word_prob_blend(self, alpha_matrix, beta_matrix, word, model, n=4):\n","        word = \"^\" * n + word.strip() + \"$\"\n","        pos = n  # char after n ^\n","        log_likelihood = 0\n","        # print (\"   \", end=\"\")\n","        while pos < len(word):\n","            prefix = word[pos - n:pos]\n","            prob = self.blend(alpha_matrix, beta_matrix, prefix, word[pos], model, n)\n","            # print(prefix, word[pos], prob)\n","            if prob:\n","                log_likelihood += math.log (prob)\n","            pos += 1\n","\n","\n","        return log_likelihood / (len(word) - 1)  # didn't guess \"^\"\n","\n"]},{"cell_type":"code","execution_count":2,"id":"9b9e8a7a","metadata":{"id":"9b9e8a7a","executionInfo":{"status":"ok","timestamp":1704321347365,"user_tz":-660,"elapsed":3,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}}},"outputs":[],"source":["# trainer with 2d matrix for d and f\n","\n","class Blending_with_2d_matrix(Blending_with_linear_param):\n","    def init(self, max_f):\n","        super(max_f)\n","        return None\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x, model):\n","        learning_rate = 0.003\n","        d = len(prefix)\n","        n = d\n","        if prefix in model:\n","            f = len(model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        derivative_beta = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, prefix, x)\n","        beta_matrix[d][f] += learning_rate * derivative_beta\n","\n","        beta_matrix[d][f] = max(0, min(1, beta_matrix[d][f]))\n","\n","        derivative_alpha = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, prefix, x)\n","        alpha_matrix[d][f] += learning_rate * derivative_alpha\n","\n","        alpha_matrix[d][f] = max(-beta_matrix[d][f], alpha_matrix[d][f])\n","\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix, model):\n","        d = len(prefix)\n","\n","        if prefix in model:\n","            f = len(model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        return (alpha_matrix[d][f], beta_matrix[d][f])"]},{"cell_type":"code","execution_count":null,"id":"721ad0ff","metadata":{"id":"721ad0ff","outputId":"ea207261-89df-403a-b232-fa39a815f04e"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9999999999999999\n","1.0\n","0.9999999999999999\n","1.0\n","0.9999999999999999\n","1.0\n"]}],"source":["# test build model and train\n","MAX_F = 10\n","n = 5\n","\n","# load data\n","data = []\n","with open (\"words_manual_en.txt\", \"r\") as f :\n","    for word in f:\n","        data.append(word)\n","\n","alpha_matrix = [[0.5, 0] for _ in range(n+1)]\n","beta_matrix = [[0.75, 0] for _ in range(n+1)]\n","\n","trainer_1 = Blending_with_linear_param(MAX_F)\n","n_gram_model_1 = trainer_1.build_model_with_train(alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","# verify\n","cand = [chr(i) for i in range(ord('A'), ord('Z') + 1)] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\"$\"]\n","\n","# alpha_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n","# beta_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n","def verify(trainer, prefix, model):\n","    s = 0\n","    for c in cand:\n","        prob = trainer.blend(alpha_matrix, beta_matrix, prefix, c, model, n)\n","        s += prob\n","\n","    return s\n","\n","print(verify(trainer_1, \"ralv\", n_gram_model_1))\n","print(verify(trainer_1, \"^All\", n_gram_model_1))\n","print(verify(trainer_1, \"Trum\", n_gram_model_1))\n","print(verify(trainer_1, \"Russ\", n_gram_model_1))\n","print(verify(trainer_1, \"Koch\", n_gram_model_1))\n","print(verify(trainer_1, \"Rack\", n_gram_model_1))"]},{"cell_type":"code","execution_count":null,"id":"b4f6cf3a","metadata":{"id":"b4f6cf3a","outputId":"4273adc3-2d15-40c4-e301-ce04f1c497b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n","0.999999999744496\n","1.0\n","1.0\n","1.0\n","1.0\n"]}],"source":["# test build model and train\n","MAX_F = 10\n","n = 5\n","\n","# load data\n","data = []\n","with open (\"words_manual_en.txt\", \"r\") as f :\n","    for word in f:\n","        data.append(word)\n","\n","alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","trainer_2 = Blending_with_2d_matrix(MAX_F)\n","n_gram_model_2 = trainer_2.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","# verify\n","cand = [chr(i) for i in range(ord('A'), ord('Z') + 1)] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\"$\"]\n","\n","def verify(trainer, prefix, model, n=n):\n","    s = 0\n","    for c in cand:\n","        prob = trainer.blend(alpha_matrix, beta_matrix, prefix, c, model)\n","\n","        s += prob\n","\n","    return s\n","\n","print(verify(trainer_2, \"ralv\", n_gram_model_2))\n","print(verify(trainer_2, \"^All\", n_gram_model_2))\n","print(verify(trainer_2, \"Trum\", n_gram_model_2))\n","print(verify(trainer_2, \"Russ\", n_gram_model_2))\n","print(verify(trainer_2, \"Koch\", n_gram_model_2))\n","print(verify(trainer_2, \"Rack\", n_gram_model_2))"]},{"cell_type":"code","execution_count":null,"id":"33dfc535","metadata":{"id":"33dfc535","outputId":"29e7f3a5-5869-40d9-f190-713cd2f9c38c"},"outputs":[{"data":{"text/plain":["[[0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.6850448508074591, 0.75, 0.75, 0.75, 0],\n"," [0.75,\n","  0.38409422131037535,\n","  0.4364834595076074,\n","  0.44368669244751446,\n","  0.49456958006947976,\n","  0.4758418591768404,\n","  0.5432620677200376,\n","  0.5889374387396005,\n","  0.5721489144783766,\n","  0.6248700104831192,\n","  0.00022023063195687254],\n"," [0.75,\n","  0,\n","  0,\n","  0,\n","  0.07090271169346975,\n","  0.2580670172289869,\n","  0.3898318945902937,\n","  0.4647159029352439,\n","  0.5597668408931874,\n","  0.567961018414197,\n","  0.3283024740495148],\n"," [0.75,\n","  0,\n","  0.0018261506595701968,\n","  0.18288404908030825,\n","  0.38100256242195457,\n","  0.5123989433527012,\n","  0.6013934670050458,\n","  0.6690310715422526,\n","  0.6850879957947016,\n","  0.7127280704797957,\n","  0.5436575850432567],\n"," [0.75,\n","  0,\n","  0.2504879866991498,\n","  0.4465826846203506,\n","  0.510607161145451,\n","  0.5728522828300227,\n","  0.6229852543617678,\n","  0.677999897204668,\n","  0.6848499927185444,\n","  0.7154580369806254,\n","  0.5376696016640183],\n"," [0.75,\n","  0,\n","  0.39500743908400676,\n","  0.516119825427136,\n","  0.5309930287467219,\n","  0.5685782413096472,\n","  0.6184963109752927,\n","  0.6770879885268545,\n","  0.684959100840458,\n","  0.7196451706002198,\n","  0.5358468880906523]]"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["beta_matrix"]},{"cell_type":"markdown","id":"66b3c34b","metadata":{"id":"66b3c34b"},"source":["Compare training and non training performance"]},{"cell_type":"code","execution_count":null,"id":"0a0075ed","metadata":{"id":"0a0075ed","outputId":"fbf62a9c-4093-456e-f2bd-4d6b6f44160d"},"outputs":[{"name":"stdout","output_type":"stream","text":["avg untrained: 0.41165258236321056, avg trained: 0.3019432506125171\n"]}],"source":["# test build model and train for linear equation pram method\n","MAX_F = 10\n","n = 5\n","\n","# load data\n","data = []\n","with open (\"words_manual_en.txt\", \"r\") as f :\n","    for word in f:\n","        data.append(word)\n","\n","alpha_matrix = [[0.5, 0] for _ in range(n+1)]\n","beta_matrix = [[0.75, 0] for _ in range(n+1)]\n","\n","trainer_1 = Blending_with_linear_param(MAX_F)\n","n_gram_model_1 = trainer_1.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","alpha_matrix_untrained = [[0.5, 0] for _ in range(n+1)]\n","beta_matrix_untrained = [[0.75, 0] for _ in range(n+1)]\n","\n","iter = 1000\n","count = 0\n","with open (\"words_manual_en.txt\", \"r\") as f :\n","    total_un = 0\n","    total = 0\n","    for word in f:\n","        word = word.strip()\n","        untrained_score = trainer_1.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_1, n)\n","        trained_score = trainer_1.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_1, n)\n","\n","        total_un += untrained_score\n","        total += trained_score\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"avg untrained: {-total_un/iter}, avg trained: {-total/iter}\")"]},{"cell_type":"code","execution_count":null,"id":"7231b37b","metadata":{"id":"7231b37b","outputId":"f26e474c-a238-4221-e9e3-74cfc497742c"},"outputs":[{"name":"stdout","output_type":"stream","text":["avg untrained: 1.0146175802848714, avg trained: 0.7400299799082519\n"]}],"source":["# test build model and train for matrix method\n","import random\n","\n","data_amount = 10000\n","count = 0\n","\n","# load data\n","data = []\n","with open (\"../checked_words.txt\", \"r\") as f :\n","    for word in f:\n","        if not word or word[0] == \"#\" or \" \" in word:\n","            continue\n","\n","        data.append(word)\n","\n","random.shuffle(data)\n","data = data[:data_amount]\n","\n","MAX_F = 10\n","n = 5\n","alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","trainer_2 = Blending_with_2d_matrix(MAX_F)\n","n_gram_model_2 = trainer_2.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","\n","total_un = 0\n","total = 0\n","for word in data:\n","    word = word.strip()\n","    untrained_score = trainer_2.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_2, n)\n","    trained_score = trainer_2.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_2, n)\n","\n","    total_un += untrained_score\n","    total += trained_score\n","\n","    count += 1\n","    if count == data_amount:\n","        break\n","\n","print(f\"avg untrained: {-total_un/data_amount}, avg trained: {-total/data_amount}\")"]},{"cell_type":"markdown","id":"1d4f80b1","metadata":{"id":"1d4f80b1"},"source":["Compare the effect of input sequence"]},{"cell_type":"code","execution_count":null,"id":"fd4cc0f8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fd4cc0f8","executionInfo":{"status":"ok","timestamp":1704290117732,"user_tz":-660,"elapsed":630942,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"78a5c46a-ff21-4844-b479-6fa9d03bbb43"},"outputs":[{"output_type":"stream","name":"stdout","text":["iter: 0, avg trained: 0.740998336636894\n","iter: 1, avg trained: 0.7392492446974981\n","iter: 2, avg trained: 0.7380248301671555\n","iter: 3, avg trained: 0.7403750717504713\n","iter: 4, avg trained: 0.7394730937184694\n","iter: 5, avg trained: 0.7382880159746086\n","iter: 6, avg trained: 0.739922362326643\n","iter: 7, avg trained: 0.7409449485654295\n","iter: 8, avg trained: 0.7418387884424199\n","iter: 9, avg trained: 0.740000657742856\n"]}],"source":["# test input sequence for matrix method\n","import random\n","MAX_F = 10\n","n = 5\n","data_amount = 10000\n","\n","for i in range(10):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_un = 0\n","    total = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score = trainer_linear.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_linear, n)\n","\n","        total += trained_score\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg trained: {-total/iter}\")"]},{"cell_type":"code","execution_count":null,"id":"5f8af533","metadata":{"id":"5f8af533","outputId":"cbb74d7a-9205-4fac-f458-ec5bba7680aa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704290781524,"user_tz":-660,"elapsed":297636,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["iter: 0, avg trained: 0.7549226877242979\n","iter: 1, avg trained: 0.7446704753398885\n","iter: 2, avg trained: 0.7474707032271138\n","iter: 3, avg trained: 0.7461749414704258\n","iter: 4, avg trained: 0.748631143059923\n","iter: 5, avg trained: 0.7501941398688521\n","iter: 6, avg trained: 0.748961446381134\n","iter: 7, avg trained: 0.7453006593200538\n","iter: 8, avg trained: 0.743932771226342\n","iter: 9, avg trained: 0.7492689048660606\n"]}],"source":["# test input sequence for matrix method\n","import random\n","MAX_F = 10\n","n = 5\n","\n","data_amount = 10000\n","\n","for i in range(10):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            data.append(word)\n","\n","    random.shuffle(data)\n","\n","    data = data[:data_amount]\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_un = 0\n","    total = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","\n","        total += trained_score\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg trained: {-total/iter}\")"]},{"cell_type":"markdown","source":["Compare Performance on different data amount"],"metadata":{"id":"cvjzXTEbnm6Q"},"id":"cvjzXTEbnm6Q"},{"cell_type":"code","source":["#@title 1000 words\n","\n","import random\n","MAX_F = 8\n","n = 5\n","data_amount = 1000\n","\n","for i in range(3):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtMbHyKKnvnj","executionInfo":{"status":"ok","timestamp":1704289363249,"user_tz":-660,"elapsed":26864,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"a9cc74d8-baba-4127-892d-794b574f272e","cellView":"form"},"id":"dtMbHyKKnvnj","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iter: 0, avg untrained: 0.8402053405709718, avg trained linear: 0.5074749763661885, avg trained matrix: 0.5138055600933474\n","iter: 1, avg untrained: 0.8446068929304558, avg trained linear: 0.5083719386733805, avg trained matrix: 0.515578565486036\n","iter: 2, avg untrained: 0.8495733317798241, avg trained linear: 0.5099657978540284, avg trained matrix: 0.515855327464602\n"]}]},{"cell_type":"code","source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),6))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),6))\n","\n","print(beta_matrix_display)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-Rbg3vLovfv","executionInfo":{"status":"ok","timestamp":1704289478350,"user_tz":-660,"elapsed":485,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"6147cde3-dc27-4059-9409-ab7db403bfe3","cellView":"form"},"id":"R-Rbg3vLovfv","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["alpha: \n","[[ 0.5       0.5       0.5       0.5       0.5       0.5       0.5       0.5       0.477942]\n"," [ 0.5       0.433724  0.463315  0.480665  0.472725  0.484867  0.487819  0.482833  0.428497]\n"," [ 0.5       0.        0.028889  0.252987  0.347411  0.384376  0.429591  0.446644  0.355195]\n"," [ 0.5       0.        0.        0.        0.244636  0.356425  0.411977  0.450191  0.415046]\n"," [ 0.5       0.       -0.000119  0.185681  0.326307  0.386342  0.42655   0.451353  0.429473]\n"," [ 0.5       0.        0.102556  0.331773  0.38668   0.420712  0.442723  0.455019  0.429501]]\n","beta: \n","[[0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.098965]\n"," [0.75     0.536362 0.565355 0.606157 0.579237 0.598968 0.631124 0.607074 0.00079 ]\n"," [0.75     0.       0.       0.       0.       0.008839 0.128911 0.20776  0.      ]\n"," [0.75     0.       0.       0.       0.       0.       0.216083 0.399318 0.289807]\n"," [0.75     0.       0.000119 0.203245 0.484738 0.596683 0.622932 0.653657 0.484271]\n"," [0.75     0.       0.281407 0.485544 0.544876 0.597461 0.618133 0.652378 0.481725]]\n"]}]},{"cell_type":"code","source":["#@title 5000 words\n","\n","import random\n","MAX_F = 8\n","n = 5\n","data_amount = 5000\n","\n","for i in range(3):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j-VqpKFWnych","executionInfo":{"status":"ok","timestamp":1704274925990,"user_tz":-660,"elapsed":230056,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"91d361ef-9c1d-46c5-e80a-4d22badc3303"},"id":"j-VqpKFWnych","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iter: 0, avg untrained: 0.9693999320088026, avg trained linear: 0.6639650325067626, avg trained matrix: 0.6651876337171544\n","iter: 1, avg untrained: 0.9660362334981374, avg trained linear: 0.6628228885157216, avg trained matrix: 0.6629567397292503\n","iter: 2, avg untrained: 0.9668027520628751, avg trained linear: 0.6646286169338222, avg trained matrix: 0.665901792696938\n"]}]},{"cell_type":"code","source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),6))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),6))\n","\n","print(beta_matrix_display)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHPeKaf6rQBp","executionInfo":{"status":"ok","timestamp":1704274925990,"user_tz":-660,"elapsed":14,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"a1ed431a-71ab-42b0-d793-c66c4d3a32e5"},"id":"UHPeKaf6rQBp","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["alpha: \n","[[0.5      0.5      0.5      0.5      0.5      0.5      0.5      0.5      0.473042]\n"," [0.5      0.422741 0.445967 0.476002 0.476926 0.483987 0.48104  0.489769 0.415594]\n"," [0.5      0.       0.       0.13631  0.263342 0.321436 0.368351 0.413834 0.202088]\n"," [0.5      0.       0.       0.       0.000419 0.000197 0.157025 0.295586 0.089942]\n"," [0.5      0.       0.       0.       0.000023 0.156946 0.280989 0.338602 0.276618]\n"," [0.5      0.       0.       0.000193 0.187012 0.297198 0.345003 0.380123 0.357913]]\n","beta: \n","[[0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.      ]\n"," [0.75     0.497352 0.523135 0.598617 0.580661 0.607304 0.59326  0.61551  0.003667]\n"," [0.75     0.       0.       0.       0.       0.       0.       0.000741 0.008904]\n"," [0.75     0.       0.       0.       0.000391 0.       0.       0.       0.000371]\n"," [0.75     0.       0.       0.       0.       0.       0.002752 0.196581 0.012236]\n"," [0.75     0.       0.       0.       0.176436 0.330746 0.392295 0.422073 0.117327]]\n"]}]},{"cell_type":"code","source":["#@title 10000 words\n","\n","import random\n","MAX_F = 8\n","n = 4\n","data_amount = 10000\n","\n","for i in range(2):\n","    n+=1\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYhYjW3-eiUF","executionInfo":{"status":"ok","timestamp":1704275301686,"user_tz":-660,"elapsed":375707,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"a19edc44-9b2c-428c-9536-b695ab660ce9","cellView":"form"},"id":"JYhYjW3-eiUF","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iter: 0, avg untrained: 1.0145477577726594, avg trained linear: 0.7396963256134947, avg trained matrix: 0.7401879083582934\n","iter: 1, avg untrained: 0.851108915156914, avg trained linear: 0.6285709543294501, avg trained matrix: 0.6290525500223288\n"]}]},{"cell_type":"code","source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),6))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),6))\n","\n","print(beta_matrix_display)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dektI8MzrR4Y","executionInfo":{"status":"ok","timestamp":1704275301686,"user_tz":-660,"elapsed":14,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"ecd3a5f8-9979-4494-ee42-b3dba7fd2e58","cellView":"form"},"id":"dektI8MzrR4Y","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["alpha: \n","[[ 0.5       0.5       0.5       0.5       0.5       0.5       0.5       0.497158  0.473376]\n"," [ 0.5       0.42087   0.441385  0.46379   0.481298  0.480161  0.485429  0.4889    0.400198]\n"," [ 0.5       0.        0.        0.097654  0.23505   0.313973  0.357209  0.395761  0.130881]\n"," [ 0.5       0.        0.       -0.000271  0.        0.       -0.000024  0.173344 -0.00005 ]\n"," [ 0.5       0.        0.       -0.000284  0.        0.000157  0.100113  0.237276  0.15784 ]\n"," [ 0.5       0.        0.000024  0.        0.035136  0.151986  0.272224  0.313389  0.326997]\n"," [ 0.5       0.        0.000502  0.088199  0.220262  0.29429   0.340362  0.355521  0.36485 ]]\n","beta: \n","[[0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.686549 0.      ]\n"," [0.75     0.48956  0.490635 0.53078  0.598935 0.584342 0.597283 0.608569 0.015595]\n"," [0.75     0.       0.       0.000153 0.       0.000222 0.000001 0.000728 0.00662 ]\n"," [0.75     0.       0.       0.000292 0.       0.       0.000024 0.000842 0.000328]\n"," [0.75     0.       0.       0.000284 0.       0.001662 0.001837 0.       0.002976]\n"," [0.75     0.       0.000447 0.       0.002342 0.048125 0.209006 0.297577 0.029191]\n"," [0.75     0.       0.000511 0.032583 0.109169 0.253392 0.266709 0.317317 0.051837]]\n"]}]},{"cell_type":"code","source":["#@title 50000 words\n","\n","import random\n","MAX_F = 8\n","n = 5\n","data_amount = 50000\n","\n","for i in range(1):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3a5yjvdPwrDM","executionInfo":{"status":"ok","timestamp":1704276112801,"user_tz":-660,"elapsed":811127,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"2f67a071-c98c-4200-f8ed-436c32939651","cellView":"form"},"id":"3a5yjvdPwrDM","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iter: 0, avg untrained: 1.1112304899030658, avg trained linear: 0.926577522097799, avg trained matrix: 0.9263261016973349\n"]}]},{"cell_type":"code","source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),6))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),6))\n","\n","print(beta_matrix_display)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TN3MPF-krS56","executionInfo":{"status":"ok","timestamp":1704276112801,"user_tz":-660,"elapsed":6,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"1975c57c-9189-4d78-bba0-39e94b1a1b91","cellView":"form"},"id":"TN3MPF-krS56","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["alpha: \n","[[ 0.5       0.5       0.5       0.5       0.5       0.5       0.5       0.497143  0.468532]\n"," [ 0.5       0.400328  0.434745  0.455257  0.469214  0.471378  0.484841  0.482849  0.392027]\n"," [ 0.5       0.       -0.000002 -0.00003   0.140007  0.233829  0.306843  0.357974  0.008133]\n"," [ 0.5       0.        0.000015  0.000028 -0.000055  0.000133 -0.000033 -0.000123  0.00014 ]\n"," [ 0.5       0.       -0.00012  -0.000242  0.        0.       -0.000066 -0.000255  0.000133]\n"," [ 0.5       0.       -0.        0.        0.        0.       -0.000001  0.106973  0.12396 ]]\n","beta: \n","[[0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.69187  0.      ]\n"," [0.75     0.441817 0.452762 0.473427 0.492014 0.525978 0.594012 0.562906 0.006045]\n"," [0.75     0.       0.000038 0.00003  0.       0.       0.       0.       0.006888]\n"," [0.75     0.       0.000244 0.000009 0.000309 0.000019 0.00004  0.002564 0.004218]\n"," [0.75     0.       0.00012  0.000494 0.       0.       0.000066 0.00077  0.001248]\n"," [0.75     0.       0.0004   0.       0.       0.       0.000181 0.000179 0.001146]]\n"]}]},{"cell_type":"code","source":["#@title 100000 words\n","\n","import random\n","MAX_F = 8\n","n = 5\n","data_amount = 100000\n","\n","for i in range(1):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Xm58jO6wpCm","executionInfo":{"status":"ok","timestamp":1704277811697,"user_tz":-660,"elapsed":1698900,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"7b502184-485f-4cb4-b863-7a70b4f0bf54","cellView":"form"},"id":"2Xm58jO6wpCm","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iter: 0, avg untrained: 1.1494670692868, avg trained linear: 1.0036818669649628, avg trained matrix: 1.0037602505001888\n"]}]},{"cell_type":"code","source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),6))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),6))\n","\n","print(beta_matrix_display)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-KTttozPrhb5","executionInfo":{"status":"ok","timestamp":1704277811697,"user_tz":-660,"elapsed":7,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"ae964d95-21bc-4889-de8c-2ac48771816e","cellView":"form"},"id":"-KTttozPrhb5","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["alpha: \n","[[ 0.5       0.5       0.5       0.5       0.5       0.497273  0.5       0.5       0.465175]\n"," [ 0.5       0.373589  0.441891  0.449966  0.459534  0.470508  0.481272  0.479496  0.395849]\n"," [ 0.5       0.        0.       -0.000001  0.090554  0.190307  0.264907  0.353419  0.000375]\n"," [ 0.5       0.        0.       -0.000084  0.        0.000134  0.        0.000027  0.      ]\n"," [ 0.5       0.        0.        0.        0.000193  0.000141  0.000357 -0.000313 -0.000192]\n"," [ 0.5       0.        0.        0.000122 -0.000028  0.000013 -0.000678 -0.000122 -0.000104]]\n","beta: \n","[[0.75     0.75     0.75     0.75     0.75     0.694872 0.75     0.75     0.      ]\n"," [0.75     0.401877 0.467307 0.436558 0.465392 0.495749 0.516013 0.515379 0.000329]\n"," [0.75     0.       0.       0.000041 0.000186 0.       0.000025 0.000332 0.001536]\n"," [0.75     0.       0.       0.000545 0.       0.000784 0.       0.000211 0.      ]\n"," [0.75     0.       0.       0.       0.000773 0.       0.001912 0.001052 0.004344]\n"," [0.75     0.       0.       0.       0.001641 0.001745 0.015804 0.000671 0.008087]]\n"]}]},{"cell_type":"markdown","source":["Vary max f and n"],"metadata":{"id":"omle8vjKrrDC"},"id":"omle8vjKrrDC"},{"cell_type":"code","source":["#@title vary n\n","\n","import random\n","MAX_F = 10\n","n = 4\n","data_amount = 10000\n","\n","for i in range(8):\n","    n+=1\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TVbJTJffnto6","executionInfo":{"status":"ok","timestamp":1704326715087,"user_tz":-660,"elapsed":4286720,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"c980b014-7f59-4c17-ca3c-0a43330217cc","cellView":"form"},"id":"TVbJTJffnto6","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["iter: 0, avg untrained: 1.012201485018517, avg trained linear: 0.7384294956786646, avg trained matrix: 0.739126202412177\n","iter: 1, avg untrained: 0.8495146729639187, avg trained linear: 0.6256500821663094, avg trained matrix: 0.6263357450493791\n","iter: 2, avg untrained: 0.7443254980407424, avg trained linear: 0.574677768889316, avg trained matrix: 0.5753518482574371\n","iter: 3, avg untrained: 0.6626572377635526, avg trained linear: 0.5332389412003412, avg trained matrix: 0.5335147449909117\n","iter: 4, avg untrained: 0.6015135922357661, avg trained linear: 0.5012216022354891, avg trained matrix: 0.501616837753367\n","iter: 5, avg untrained: 0.5518037948930371, avg trained linear: 0.47370959366659254, avg trained matrix: 0.47392188498178384\n","iter: 6, avg untrained: 0.5109415425439048, avg trained linear: 0.44929842780930984, avg trained matrix: 0.4498370147778219\n","iter: 7, avg untrained: 0.47731636600477395, avg trained linear: 0.4298003718484721, avg trained matrix: 0.4283618610886603\n"]}]},{"cell_type":"code","source":["#@title train test split\n","\n","import random\n","MAX_F = 10\n","n = 5\n","train_ratio = 0.8\n","data_amount = 10000\n","\n","for i in range(5):\n","    train_size = int(train_ratio * data_amount)\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    train_data = data[:train_size]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(train_data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(train_data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount*(1-train_ratio)\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data[train_size:]:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"],"metadata":{"cellView":"form","id":"v6PIbDRKeJOC"},"id":"v6PIbDRKeJOC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 5000 words\n","\n","import random\n","MAX_F = 8\n","n = 5\n","data_amount = 5000\n","\n","for i in range(3):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704329869225,"user_tz":-660,"elapsed":270185,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"370abd7a-38a7-4e3a-8d5c-91de07ccfcb5","id":"W_SA2_Q39bmN"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["iter: 0, avg untrained: 0.9670477230515387, avg trained linear: 0.6621703969303782, avg trained matrix: 0.6627072837052418\n","iter: 1, avg untrained: 0.9690150804140151, avg trained linear: 0.6631114039560725, avg trained matrix: 0.6645192464071084\n","iter: 2, avg untrained: 0.9682885439891415, avg trained linear: 0.6630247734612777, avg trained matrix: 0.6642708293790092\n"]}],"id":"W_SA2_Q39bmN"},{"cell_type":"code","source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix_linear),8))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix_linear),8))\n","\n","print(beta_matrix_display)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704329990667,"user_tz":-660,"elapsed":318,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"7f49f868-6395-4585-b1a1-8946066d0f4f","id":"lJXvlFzG9bmT"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["alpha: \n","[[ 0.47353782  0.        ]\n"," [ 0.20684438 -0.02577513]\n"," [-0.00119564  0.00024771]\n"," [ 0.00000418  0.00009018]\n"," [ 0.00017365  0.00052105]\n"," [ 0.          0.        ]]\n","beta: \n","[[0.         0.        ]\n"," [0.00076661 0.00076661]\n"," [0.00467083 0.00916044]\n"," [0.00072476 0.00194751]\n"," [0.00020572 0.00061716]\n"," [0.         0.        ]]\n"]}],"id":"lJXvlFzG9bmT"},{"cell_type":"code","source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),8))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),8))\n","\n","print(beta_matrix_display)"],"metadata":{"id":"Xf7zfrLd_ADp","executionInfo":{"status":"ok","timestamp":1704330016824,"user_tz":-660,"elapsed":345,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"cf94addd-f443-422b-de82-8d53fa8a2ad0","colab":{"base_uri":"https://localhost:8080/"}},"id":"Xf7zfrLd_ADp","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["alpha: \n","[[ 0.5         0.5         0.5         0.5         0.5         0.5         0.5         0.5         0.47353776]\n"," [ 0.5         0.43326074  0.45266128  0.47152618  0.4804843   0.48724664  0.48779497  0.48566135  0.40696124]\n"," [ 0.5         0.          0.          0.14755361  0.25419682  0.32894775  0.3694433   0.40575499  0.20929432]\n"," [ 0.5         0.          0.         -0.00005069  0.         -0.00013946  0.17928492  0.30040829  0.10880048]\n"," [ 0.5         0.          0.          0.          0.00064773  0.13726075  0.26679744  0.33140153  0.29766416]\n"," [ 0.5         0.          0.         -0.00035411  0.1964305   0.29180837  0.35830671  0.38057038  0.37050732]]\n","beta: \n","[[0.75       0.75       0.75       0.75       0.75       0.75       0.75       0.75       0.        ]\n"," [0.75       0.52848669 0.53129008 0.56782384 0.59052892 0.61829175 0.63136607 0.61760532 0.00076474]\n"," [0.75       0.         0.         0.         0.         0.00052375 0.         0.         0.00730998]\n"," [0.75       0.         0.         0.00005069 0.         0.00107286 0.00230333 0.00033304 0.00038741]\n"," [0.75       0.         0.         0.         0.0033881  0.00072074 0.00110051 0.23329054 0.00188098]\n"," [0.75       0.         0.         0.00035411 0.1649022  0.33741612 0.376209   0.4278862  0.11660271]]\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"provenance":[{"file_id":"https://github.com/czx20010217/pubmed_typos/blob/main/word_models/blending.ipynb","timestamp":1704179776075}]}},"nbformat":4,"nbformat_minor":5}