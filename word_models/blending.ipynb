{"cells":[{"cell_type":"code","execution_count":33,"id":"10e7724a","metadata":{"executionInfo":{"elapsed":460,"status":"ok","timestamp":1704321347365,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"10e7724a"},"outputs":[],"source":["TOTAL_CHAR = 2**16\n","import random\n","\n","# derivative first half (log P(s) / alpha)\n","import numpy as np\n","\n","import sys\n","import pickle\n","import math\n","import numpy as np\n","import pdb\n","\n","np.set_printoptions(suppress=True)\n","np.set_printoptions(linewidth=400)\n","# n = 4   # number of letters as context\n","MAX_N = 4\n","MAX_UPDATE_RATIO = 0.3\n","MAX_DECREASE_RATION = 0.5\n","\n","class Blending_with_linear_param:\n","    max_f = 0\n","    total_score = 0\n","    count = 0\n","\n","    def __init__(self, max_f):\n","        self.max_f = max_f\n","        return None\n","\n","    def derivative_of_P_to_alpha(self, alpha_matrix, beta_matrix, model, prefix, x):\n","\n","        n = len(prefix)\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix, model)\n","        if prefix in model:\n","            ms = sum(model[prefix].values())\n","            us = len(model[prefix])\n","            if x in model[prefix]:\n","                ms_x = model[prefix][x]\n","            else:\n","                ms_x = 0\n","        else:\n","            ms = 0\n","            us = 0\n","            ms_x = 0\n","\n","        eq1 = (beta-ms_x) / (ms+alpha)**2\n","\n","        if len(prefix) == 0:\n","            eq2 = (ms-us*beta) / (ms+alpha)**2 / TOTAL_CHAR\n","            eq3 = 0\n","        else:\n","            next_prefix = prefix[1:]\n","            eq2 = (ms-us*beta) / (ms+alpha)**2 * self.blend(alpha_matrix, beta_matrix, next_prefix, x, model, n)\n","            eq3 = (alpha + us*beta) / (ms+alpha) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, next_prefix, x)\n","\n","        return eq1 + eq2 + eq3\n","\n","    def derivative_of_P_to_beta(self, alpha_matrix, beta_matrix, model, prefix, x):\n","\n","        n = len(prefix)\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix, model)\n","        if prefix in model:\n","            ms = sum(model[prefix].values())\n","            us = len(model[prefix])\n","            if x in model[prefix]:\n","                ms_x = model[prefix][x]\n","            else:\n","                ms_x = 0\n","        else:\n","            ms = 0\n","            us = 0\n","            ms_x = 0\n","\n","        eq1 = -1 / (ms+alpha)\n","        if len(prefix) == 0:\n","            eq2 = (ms-us*beta) / (ms+alpha)**2 / TOTAL_CHAR\n","            eq3 = 0\n","        else:\n","            next_prefix = prefix[1:]\n","            eq2 = us / (ms+alpha) * self.blend(alpha_matrix, beta_matrix, next_prefix, x, model, n)\n","            eq3 = (alpha + us*beta) / (ms+alpha) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, next_prefix, x)\n","\n","        return eq1 + eq2 + eq3\n","\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x, model):\n","        learning_rate = 0.003\n","        n = len(prefix)\n","        # add a computation for alpha beta\n","        if prefix in model:\n","            f = len(model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","        param_derivative_mapping = [1, f]\n","            \n","        prob = self.blend(alpha_matrix, beta_matrix, prefix, x, model, n)\n","        # new\n","        derivative_alpha = 1 / prob * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, prefix, x)\n","        derivative_beta = 1 / prob * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, prefix, x)\n","        beta_update = learning_rate * derivative_beta\n","        alpha_update = learning_rate * derivative_alpha\n","        factor = 1\n","        \n","        # # reduce to premitted update ratio\n","        # max_change_alpha = alpha_update + alpha_update * f * self.max_f\n","        # max_change_beta = beta_update + beta_update * f * self.max_f\n","        \n","        # if max_change_alpha < 0:\n","        #     allowed_change_alpha = abs(alpha_matrix[n][0] + min(0, alpha_matrix[n][1]*self.max_f)) * MAX_UPDATE_RATIO # decrease case \n","        # else:\n","        #     allowed_change_alpha = abs(alpha_matrix[n][0] + max(0, alpha_matrix[n][1]*self.max_f)) * MAX_UPDATE_RATIO # increase case \n","            \n","        # if max_change_beta < 0:\n","        #     allowed_change_beta = abs(beta_matrix[n][0] + min(0, beta_matrix[n][1]*self.max_f)) * MAX_UPDATE_RATIO # decrease case \n","        # else:\n","        #     allowed_change_beta = abs(beta_matrix[n][0] + max(0, beta_matrix[n][1]*self.max_f)) * MAX_UPDATE_RATIO # increase case\n","            \n","            \n","        # factor_alpha = max(abs(max_change_alpha) / allowed_change_alpha, 1)\n","        # factor_beta = max(abs(max_change_beta) / allowed_change_beta, 1)\n","        \n","        # factor = max(factor_alpha, factor_beta)\n","        \n","        for ind, derivative in zip(range(len(param_derivative_mapping)), param_derivative_mapping):\n","            beta_matrix[n][ind] += beta_update * derivative / factor\n","            alpha_matrix[n][ind] += alpha_update * derivative / factor\n","\n","        # restrict param for beta\n","        beta_matrix[n][0] = max(0, min(1, beta_matrix[n][0]))\n","\n","        if beta_matrix[n][0] + self.max_f * beta_matrix[n][1] > 1:\n","            beta_matrix[n][1] = (1 - beta_matrix[n][0]) / self.max_f\n","        elif beta_matrix[n][0] + self.max_f * beta_matrix[n][1] < 0:\n","            beta_matrix[n][1] = -beta_matrix[n][0] / self.max_f\n","\n","        max_beta = max(beta_matrix[n][0], self.max_f*beta_matrix[n][1])\n","        # restrict param for alpha\n","        alpha_matrix[n][0] = max(alpha_matrix[n][0], -max_beta)\n","\n","        if alpha_matrix[n][0] + self.max_f * alpha_matrix[n][1] < -max_beta:\n","            alpha_matrix[n][1] = (-max_beta-alpha_matrix[n][0]) / self.max_f\n","\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix, model):\n","        n = len(prefix)\n","        if prefix in model:\n","            f = len(model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        param_derivative_mapping = [1, f]\n","        # alpha1 = np.dot(alpha_matrix[n], param_derivative_mapping)\n","        # beta1 = np.dot(beta_matrix[n], param_derivative_mapping)\n","\n","        alpha = alpha_matrix[n][0] + f*  alpha_matrix[n][1]\n","        beta = beta_matrix[n][0] + f*  beta_matrix[n][1]\n","\n","        #beta = max(0, min(1, beta))\n","        # alpha = max(-beta, alpha)\n","\n","\n","        return (alpha, beta)\n","\n","    def build_model_with_train(self, data, alpha_matrix, beta_matrix, word_file, n):\n","        \"\"\"\n","        Build n-gram model of the words in  words_lang.txt\n","        \"\"\"\n","        n_grams = {}\n","        # read in all n+1 grams\n","        n_plus_1_gram_counts = {}\n","        for word in data:\n","            # update n-gram\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","            word = \"^\" * n + word.strip() + \"$\"\n","            for i in range(len(word) - n):\n","                n_plus_1_gram = word[i:i + n + 1]\n","\n","                for N in range(n, -1, -1):\n","                    x = n_plus_1_gram[-1]\n","                    next_n = n_plus_1_gram[:-1]\n","\n","                    if next_n not in n_grams:\n","                        n_grams[next_n] = {}\n","\n","                    if x not in n_grams[next_n]:\n","                        n_grams[next_n][x] = 1\n","                    else:\n","                        n_grams[next_n][x] += 1\n","\n","                    n_plus_1_gram = n_plus_1_gram[1:]\n","\n","            # update alpha and beta\n","            for i in range(len(word) - n):\n","                n_plus_1_gram = word[i:i + n + 1]\n","\n","                for N in range(n, -1, -1):\n","                    x = n_plus_1_gram[-1]\n","                    next_n = n_plus_1_gram[:-1]\n","\n","                    self.update_param(alpha_matrix, beta_matrix, next_n, x, n_grams)\n","\n","                    n_plus_1_gram = n_plus_1_gram[1:]\n","\n","        return n_grams\n","\n","    def blend(self, alpha_matrix, beta_matrix, prefix, x, model, n):\n","        prob = 0\n","\n","        # compute alpha and beta\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix, model)\n","        possible_x = TOTAL_CHAR\n","\n","        # first half of algorithm\n","        if prefix in model:\n","            base = (len(model[prefix])*beta + alpha) / (sum(model[prefix].values()) + alpha)\n","            if x in model[prefix]:\n","                prob += (model[prefix][x] - beta) / (sum(model[prefix].values()) + alpha)\n","        else:\n","            base = (0 + alpha) / (0 + alpha)\n","\n","        if len(prefix) == 0:\n","            prob += base / possible_x\n","        else:\n","            prob += base * self.blend(alpha_matrix, beta_matrix, prefix[1:], x, model, n)\n","\n","        return prob\n","\n","\n","    def word_prob_blend(self, alpha_matrix, beta_matrix, word, model, n):\n","        word = \"^\" * n + word.strip() + \"$\"\n","        pos = n  # char after n ^\n","        log_likelihood = 0\n","        # print (\"   \", end=\"\")\n","        while pos < len(word):\n","            prefix = word[pos - n:pos]\n","            prob = self.blend(alpha_matrix, beta_matrix, prefix, word[pos], model, n)\n","            # print(prefix, word[pos], prob)\n","            if prob:\n","                log_likelihood += math.log (prob)\n","            pos += 1\n","\n","\n","        return log_likelihood / (len(word) - 1)  # didn't guess \"^\"\n","\n"]},{"cell_type":"code","execution_count":34,"id":"9b9e8a7a","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1704321347365,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"9b9e8a7a"},"outputs":[],"source":["# trainer with 2d matrix for d and f\n","\n","class Blending_with_2d_matrix(Blending_with_linear_param):\n","    def init(self, max_f):\n","        super(max_f)\n","        return None\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x, model):\n","        learning_rate = 0.003\n","        d = len(prefix)\n","        n = d\n","        if prefix in model:\n","            f = len(model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        # update\n","        derivative_beta = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, prefix, x)\n","        beta_update = learning_rate * derivative_beta\n","        derivative_alpha = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, prefix, x)\n","        alpha_update = learning_rate * derivative_alpha\n","        \n","        # compute factor need to be changed\n","        factor_alpha = max(abs(alpha_update) / (alpha_matrix[d][f] * MAX_UPDATE_RATIO), 1)\n","        factor_beta = max(abs(beta_update) / (beta_matrix[d][f] * MAX_UPDATE_RATIO), 1)\n","        \n","        factor = max(factor_alpha, factor_beta)\n","        \n","\n","        beta_matrix[d][f] += beta_update / factor\n","        alpha_matrix[d][f] += alpha_update / factor\n","        \n","        beta_matrix[d][f] = max(0, min(1, beta_matrix[d][f]))\n","\n","        derivative_alpha = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, prefix, x)\n","        alpha_matrix[d][f] += learning_rate * derivative_alpha\n","\n","        alpha_matrix[d][f] = max(-beta_matrix[d][f], alpha_matrix[d][f])\n","\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix, model):\n","        d = len(prefix)\n","\n","        if prefix in model:\n","            f = len(model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        return (alpha_matrix[d][f], beta_matrix[d][f])"]},{"cell_type":"code","execution_count":25,"id":"721ad0ff","metadata":{"id":"721ad0ff","outputId":"ea207261-89df-403a-b232-fa39a815f04e"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9999999736588921\n","0.9999999995177955\n","0.9999999347496129\n","0.9999999935293562\n","0.9999999864512745\n","0.9999999959064186\n"]}],"source":["# test build model and train\n","MAX_F = 10\n","n = 5\n","\n","# load data\n","data = []\n","with open (\"words_manual_en.txt\", \"r\") as f :\n","    for word in f:\n","        data.append(word)\n","\n","random.shuffle(data)\n","\n","alpha_matrix = [[0.5, 0] for _ in range(n+1)]\n","beta_matrix = [[0.75, 0] for _ in range(n+1)]\n","\n","trainer_1 = Blending_with_linear_param(MAX_F)\n","n_gram_model_1 = trainer_1.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","# verify\n","cand = [chr(i) for i in range(ord('A'), ord('Z') + 1)] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\"$\"]\n","\n","# alpha_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n","# beta_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n","def verify(trainer, prefix, model):\n","    s = 0\n","    for c in cand:\n","        prob = trainer.blend(alpha_matrix, beta_matrix, prefix, c, model, n)\n","        s += prob\n","\n","    return s\n","\n","print(verify(trainer_1, \"ralv\", n_gram_model_1))\n","print(verify(trainer_1, \"^All\", n_gram_model_1))\n","print(verify(trainer_1, \"Trum\", n_gram_model_1))\n","print(verify(trainer_1, \"Russ\", n_gram_model_1))\n","print(verify(trainer_1, \"Koch\", n_gram_model_1))\n","print(verify(trainer_1, \"Rack\", n_gram_model_1))"]},{"cell_type":"code","execution_count":27,"id":"b4f6cf3a","metadata":{"id":"b4f6cf3a","outputId":"4273adc3-2d15-40c4-e301-ce04f1c497b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n","0.9999999998014688\n","1.0\n","1.0\n","1.0\n","1.0\n"]}],"source":["# test build model and train\n","MAX_F = 10\n","n = 5\n","\n","# load data\n","data = []\n","with open (\"words_manual_en.txt\", \"r\") as f :\n","    \n","    for word in f:\n","        data.append(word)\n","\n","alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","trainer_2 = Blending_with_2d_matrix(MAX_F)\n","n_gram_model_2 = trainer_2.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","# verify\n","cand = [chr(i) for i in range(ord('A'), ord('Z') + 1)] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\"$\"]\n","\n","def verify(trainer, prefix, model, n=n):\n","    s = 0\n","    for c in cand:\n","        prob = trainer.blend(alpha_matrix, beta_matrix, prefix, c, model, n)\n","\n","        s += prob\n","\n","    return s\n","\n","print(verify(trainer_2, \"ralv\", n_gram_model_2))\n","print(verify(trainer_2, \"^All\", n_gram_model_2))\n","print(verify(trainer_2, \"Trum\", n_gram_model_2))\n","print(verify(trainer_2, \"Russ\", n_gram_model_2))\n","print(verify(trainer_2, \"Koch\", n_gram_model_2))\n","print(verify(trainer_2, \"Rack\", n_gram_model_2))"]},{"cell_type":"code","execution_count":8,"id":"33dfc535","metadata":{"id":"33dfc535","outputId":"29e7f3a5-5869-40d9-f190-713cd2f9c38c"},"outputs":[{"data":{"text/plain":["[[0.75,\n","  0.75,\n","  0.75,\n","  0.75,\n","  0.75,\n","  0.75,\n","  0.6850448472711004,\n","  0.75,\n","  0.75,\n","  0.75,\n","  4.088797326e-314],\n"," [0.75,\n","  0.38247396096448294,\n","  0.43568628678075844,\n","  0.44336822497264433,\n","  0.4943037304144454,\n","  0.47569553327321756,\n","  0.5431910266544931,\n","  0.5888828595044886,\n","  0.5721149685122415,\n","  0.6248501870035919,\n","  2.9925000848057995e-41],\n"," [0.75,\n","  1.2462463154202527e-218,\n","  4.98660624444688e-145,\n","  3.0627455649908885e-39,\n","  0.06947582386632441,\n","  0.2573964889655316,\n","  0.3895770178058906,\n","  0.46458308946669175,\n","  0.5597003889441051,\n","  0.5678989729438532,\n","  0.3281776730887915],\n"," [0.75,\n","  2.498721297e-315,\n","  7.833579022767356e-42,\n","  0.18643675194073778,\n","  0.38105903470736224,\n","  0.5121563882335111,\n","  0.601300358720672,\n","  0.6690120751597365,\n","  0.6850591836870964,\n","  0.7127157491226999,\n","  0.543630987095666],\n"," [0.75,\n","  6.559129694638736e-249,\n","  0.27136344356047476,\n","  0.4507632561250805,\n","  0.5110792239767713,\n","  0.5726689435708191,\n","  0.6228975824993276,\n","  0.6779724642548745,\n","  0.6848248613463084,\n","  0.71544803516803,\n","  0.5376360025543981],\n"," [0.75,\n","  0.0854620903701324,\n","  0.41082153353878664,\n","  0.5191015583828003,\n","  0.5313453368846174,\n","  0.56838598378093,\n","  0.6184015042814959,\n","  0.6770677778648377,\n","  0.684943475776383,\n","  0.7196403999638729,\n","  0.5358248381240098]]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["beta_matrix"]},{"cell_type":"markdown","id":"66b3c34b","metadata":{"id":"66b3c34b"},"source":["Compare training and non training performance"]},{"cell_type":"code","execution_count":28,"id":"0a0075ed","metadata":{"id":"0a0075ed","outputId":"fbf62a9c-4093-456e-f2bd-4d6b6f44160d"},"outputs":[{"name":"stdout","output_type":"stream","text":["avg untrained: 0.41165258236321056, avg trained: 0.34007333487570374\n"]}],"source":["# test build model and train for linear equation pram method\n","MAX_F = 10\n","n = 5\n","\n","# load data\n","data = []\n","with open (\"words_manual_en.txt\", \"r\") as f :\n","    for word in f:\n","        data.append(word)\n","\n","alpha_matrix = [[0.5, 0] for _ in range(n+1)]\n","beta_matrix = [[0.75, 0] for _ in range(n+1)]\n","\n","trainer_1 = Blending_with_linear_param(MAX_F)\n","n_gram_model_1 = trainer_1.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","alpha_matrix_untrained = [[0.5, 0] for _ in range(n+1)]\n","beta_matrix_untrained = [[0.75, 0] for _ in range(n+1)]\n","\n","iter = 1000\n","count = 0\n","with open (\"words_manual_en.txt\", \"r\") as f :\n","    total_un = 0\n","    total = 0\n","    for word in f:\n","        word = word.strip()\n","        untrained_score = trainer_1.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_1, n)\n","        trained_score = trainer_1.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_1, n)\n","\n","        total_un += untrained_score\n","        total += trained_score\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"avg untrained: {-total_un/iter}, avg trained: {-total/iter}\")"]},{"cell_type":"code","execution_count":29,"id":"7231b37b","metadata":{"id":"7231b37b","outputId":"f26e474c-a238-4221-e9e3-74cfc497742c"},"outputs":[{"name":"stdout","output_type":"stream","text":["avg untrained: 1.0104404746968692, avg trained: 0.7381261793703855\n"]}],"source":["# test build model and train for matrix method\n","import random\n","\n","data_amount = 10000\n","count = 0\n","\n","# load data\n","data = []\n","with open (\"../checked_words.txt\", \"r\") as f :\n","    for word in f:\n","        if not word or word[0] == \"#\" or \" \" in word:\n","            continue\n","\n","        data.append(word)\n","\n","random.shuffle(data)\n","data = data[:data_amount]\n","\n","MAX_F = 10\n","n = 5\n","alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","trainer_2 = Blending_with_2d_matrix(MAX_F)\n","n_gram_model_2 = trainer_2.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","\n","total_un = 0\n","total = 0\n","for word in data:\n","    word = word.strip()\n","    untrained_score = trainer_2.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_2, n)\n","    trained_score = trainer_2.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_2, n)\n","\n","    total_un += untrained_score\n","    total += trained_score\n","\n","    count += 1\n","    if count == data_amount:\n","        break\n","\n","print(f\"avg untrained: {-total_un/data_amount}, avg trained: {-total/data_amount}\")"]},{"cell_type":"markdown","id":"1d4f80b1","metadata":{"id":"1d4f80b1"},"source":["Compare the effect of input sequence"]},{"cell_type":"code","execution_count":null,"id":"fd4cc0f8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":630942,"status":"ok","timestamp":1704290117732,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"fd4cc0f8","outputId":"78a5c46a-ff21-4844-b479-6fa9d03bbb43"},"outputs":[{"name":"stdout","output_type":"stream","text":["iter: 0, avg trained: 0.740998336636894\n","iter: 1, avg trained: 0.7392492446974981\n","iter: 2, avg trained: 0.7380248301671555\n","iter: 3, avg trained: 0.7403750717504713\n","iter: 4, avg trained: 0.7394730937184694\n","iter: 5, avg trained: 0.7382880159746086\n","iter: 6, avg trained: 0.739922362326643\n","iter: 7, avg trained: 0.7409449485654295\n","iter: 8, avg trained: 0.7418387884424199\n","iter: 9, avg trained: 0.740000657742856\n"]}],"source":["# test input sequence for matrix method\n","import random\n","MAX_F = 10\n","n = 5\n","data_amount = 10000\n","\n","for i in range(10):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_un = 0\n","    total = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score = trainer_linear.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_linear, n)\n","\n","        total += trained_score\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg trained: {-total/iter}\")"]},{"cell_type":"code","execution_count":31,"id":"5f8af533","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297636,"status":"ok","timestamp":1704290781524,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"5f8af533","outputId":"cbb74d7a-9205-4fac-f458-ec5bba7680aa"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './checked_words.txt'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[31], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./checked_words.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f :\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     13\u001b[0m             data\u001b[38;5;241m.\u001b[39mappend(word)\n","File \u001b[1;32mc:\\Users\\18473\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './checked_words.txt'"]}],"source":["# test input sequence for matrix method\n","import random\n","MAX_F = 10\n","n = 5\n","\n","data_amount = 10000\n","\n","for i in range(10):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            data.append(word)\n","\n","    random.shuffle(data)\n","\n","    data = data[:data_amount]\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_un = 0\n","    total = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","\n","        total += trained_score\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg trained: {-total/iter}\")"]},{"cell_type":"markdown","id":"cvjzXTEbnm6Q","metadata":{"id":"cvjzXTEbnm6Q"},"source":["Compare Performance on different data amount"]},{"cell_type":"code","execution_count":35,"id":"dtMbHyKKnvnj","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26864,"status":"ok","timestamp":1704289363249,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"dtMbHyKKnvnj","outputId":"a9cc74d8-baba-4127-892d-794b574f272e"},"outputs":[{"name":"stdout","output_type":"stream","text":["iter: 0, avg untrained: 0.8481293110855963, avg trained linear: 0.5049754085479109, avg trained matrix: 0.5116977504727089\n","iter: 1, avg untrained: 0.8557247146806584, avg trained linear: 0.5088375874222333, avg trained matrix: 0.5157613059574715\n","iter: 2, avg untrained: 0.8497769057718843, avg trained linear: 0.5077528875385903, avg trained matrix: 0.5142404099262998\n"]}],"source":["#@title 1000 words\n","\n","import random\n","MAX_F = 8\n","n = 5\n","data_amount = 1000\n","\n","for i in range(3):\n","    # load data\n","    data = []\n","    with open (\"../checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"]},{"cell_type":"code","execution_count":null,"id":"R-Rbg3vLovfv","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":485,"status":"ok","timestamp":1704289478350,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"R-Rbg3vLovfv","outputId":"6147cde3-dc27-4059-9409-ab7db403bfe3"},"outputs":[{"name":"stdout","output_type":"stream","text":["alpha: \n","[[ 0.5       0.5       0.5       0.5       0.5       0.5       0.5       0.5       0.477942]\n"," [ 0.5       0.433724  0.463315  0.480665  0.472725  0.484867  0.487819  0.482833  0.428497]\n"," [ 0.5       0.        0.028889  0.252987  0.347411  0.384376  0.429591  0.446644  0.355195]\n"," [ 0.5       0.        0.        0.        0.244636  0.356425  0.411977  0.450191  0.415046]\n"," [ 0.5       0.       -0.000119  0.185681  0.326307  0.386342  0.42655   0.451353  0.429473]\n"," [ 0.5       0.        0.102556  0.331773  0.38668   0.420712  0.442723  0.455019  0.429501]]\n","beta: \n","[[0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.098965]\n"," [0.75     0.536362 0.565355 0.606157 0.579237 0.598968 0.631124 0.607074 0.00079 ]\n"," [0.75     0.       0.       0.       0.       0.008839 0.128911 0.20776  0.      ]\n"," [0.75     0.       0.       0.       0.       0.       0.216083 0.399318 0.289807]\n"," [0.75     0.       0.000119 0.203245 0.484738 0.596683 0.622932 0.653657 0.484271]\n"," [0.75     0.       0.281407 0.485544 0.544876 0.597461 0.618133 0.652378 0.481725]]\n"]}],"source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),6))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),6))\n","\n","print(beta_matrix_display)"]},{"cell_type":"code","execution_count":null,"id":"j-VqpKFWnych","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230056,"status":"ok","timestamp":1704274925990,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"j-VqpKFWnych","outputId":"91d361ef-9c1d-46c5-e80a-4d22badc3303"},"outputs":[{"name":"stdout","output_type":"stream","text":["iter: 0, avg untrained: 0.9693999320088026, avg trained linear: 0.6639650325067626, avg trained matrix: 0.6651876337171544\n","iter: 1, avg untrained: 0.9660362334981374, avg trained linear: 0.6628228885157216, avg trained matrix: 0.6629567397292503\n","iter: 2, avg untrained: 0.9668027520628751, avg trained linear: 0.6646286169338222, avg trained matrix: 0.665901792696938\n"]}],"source":["#@title 5000 words\n","\n","import random\n","MAX_F = 8\n","n = 5\n","data_amount = 5000\n","\n","for i in range(3):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"]},{"cell_type":"code","execution_count":null,"id":"UHPeKaf6rQBp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1704274925990,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"UHPeKaf6rQBp","outputId":"a1ed431a-71ab-42b0-d793-c66c4d3a32e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["alpha: \n","[[0.5      0.5      0.5      0.5      0.5      0.5      0.5      0.5      0.473042]\n"," [0.5      0.422741 0.445967 0.476002 0.476926 0.483987 0.48104  0.489769 0.415594]\n"," [0.5      0.       0.       0.13631  0.263342 0.321436 0.368351 0.413834 0.202088]\n"," [0.5      0.       0.       0.       0.000419 0.000197 0.157025 0.295586 0.089942]\n"," [0.5      0.       0.       0.       0.000023 0.156946 0.280989 0.338602 0.276618]\n"," [0.5      0.       0.       0.000193 0.187012 0.297198 0.345003 0.380123 0.357913]]\n","beta: \n","[[0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.      ]\n"," [0.75     0.497352 0.523135 0.598617 0.580661 0.607304 0.59326  0.61551  0.003667]\n"," [0.75     0.       0.       0.       0.       0.       0.       0.000741 0.008904]\n"," [0.75     0.       0.       0.       0.000391 0.       0.       0.       0.000371]\n"," [0.75     0.       0.       0.       0.       0.       0.002752 0.196581 0.012236]\n"," [0.75     0.       0.       0.       0.176436 0.330746 0.392295 0.422073 0.117327]]\n"]}],"source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),6))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),6))\n","\n","print(beta_matrix_display)"]},{"cell_type":"code","execution_count":null,"id":"JYhYjW3-eiUF","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":375707,"status":"ok","timestamp":1704275301686,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"JYhYjW3-eiUF","outputId":"a19edc44-9b2c-428c-9536-b695ab660ce9"},"outputs":[{"name":"stdout","output_type":"stream","text":["iter: 0, avg untrained: 1.0145477577726594, avg trained linear: 0.7396963256134947, avg trained matrix: 0.7401879083582934\n","iter: 1, avg untrained: 0.851108915156914, avg trained linear: 0.6285709543294501, avg trained matrix: 0.6290525500223288\n"]}],"source":["#@title 10000 words\n","\n","import random\n","MAX_F = 8\n","n = 4\n","data_amount = 10000\n","\n","for i in range(2):\n","    n+=1\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"]},{"cell_type":"code","execution_count":null,"id":"dektI8MzrR4Y","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1704275301686,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"dektI8MzrR4Y","outputId":"ecd3a5f8-9979-4494-ee42-b3dba7fd2e58"},"outputs":[{"name":"stdout","output_type":"stream","text":["alpha: \n","[[ 0.5       0.5       0.5       0.5       0.5       0.5       0.5       0.497158  0.473376]\n"," [ 0.5       0.42087   0.441385  0.46379   0.481298  0.480161  0.485429  0.4889    0.400198]\n"," [ 0.5       0.        0.        0.097654  0.23505   0.313973  0.357209  0.395761  0.130881]\n"," [ 0.5       0.        0.       -0.000271  0.        0.       -0.000024  0.173344 -0.00005 ]\n"," [ 0.5       0.        0.       -0.000284  0.        0.000157  0.100113  0.237276  0.15784 ]\n"," [ 0.5       0.        0.000024  0.        0.035136  0.151986  0.272224  0.313389  0.326997]\n"," [ 0.5       0.        0.000502  0.088199  0.220262  0.29429   0.340362  0.355521  0.36485 ]]\n","beta: \n","[[0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.686549 0.      ]\n"," [0.75     0.48956  0.490635 0.53078  0.598935 0.584342 0.597283 0.608569 0.015595]\n"," [0.75     0.       0.       0.000153 0.       0.000222 0.000001 0.000728 0.00662 ]\n"," [0.75     0.       0.       0.000292 0.       0.       0.000024 0.000842 0.000328]\n"," [0.75     0.       0.       0.000284 0.       0.001662 0.001837 0.       0.002976]\n"," [0.75     0.       0.000447 0.       0.002342 0.048125 0.209006 0.297577 0.029191]\n"," [0.75     0.       0.000511 0.032583 0.109169 0.253392 0.266709 0.317317 0.051837]]\n"]}],"source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),6))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),6))\n","\n","print(beta_matrix_display)"]},{"cell_type":"code","execution_count":null,"id":"3a5yjvdPwrDM","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":811127,"status":"ok","timestamp":1704276112801,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"3a5yjvdPwrDM","outputId":"2f67a071-c98c-4200-f8ed-436c32939651"},"outputs":[{"name":"stdout","output_type":"stream","text":["iter: 0, avg untrained: 1.1112304899030658, avg trained linear: 0.926577522097799, avg trained matrix: 0.9263261016973349\n"]}],"source":["#@title 50000 words\n","\n","import random\n","MAX_F = 8\n","n = 5\n","data_amount = 50000\n","\n","for i in range(1):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"]},{"cell_type":"code","execution_count":null,"id":"TN3MPF-krS56","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1704276112801,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"TN3MPF-krS56","outputId":"1975c57c-9189-4d78-bba0-39e94b1a1b91"},"outputs":[{"name":"stdout","output_type":"stream","text":["alpha: \n","[[ 0.5       0.5       0.5       0.5       0.5       0.5       0.5       0.497143  0.468532]\n"," [ 0.5       0.400328  0.434745  0.455257  0.469214  0.471378  0.484841  0.482849  0.392027]\n"," [ 0.5       0.       -0.000002 -0.00003   0.140007  0.233829  0.306843  0.357974  0.008133]\n"," [ 0.5       0.        0.000015  0.000028 -0.000055  0.000133 -0.000033 -0.000123  0.00014 ]\n"," [ 0.5       0.       -0.00012  -0.000242  0.        0.       -0.000066 -0.000255  0.000133]\n"," [ 0.5       0.       -0.        0.        0.        0.       -0.000001  0.106973  0.12396 ]]\n","beta: \n","[[0.75     0.75     0.75     0.75     0.75     0.75     0.75     0.69187  0.      ]\n"," [0.75     0.441817 0.452762 0.473427 0.492014 0.525978 0.594012 0.562906 0.006045]\n"," [0.75     0.       0.000038 0.00003  0.       0.       0.       0.       0.006888]\n"," [0.75     0.       0.000244 0.000009 0.000309 0.000019 0.00004  0.002564 0.004218]\n"," [0.75     0.       0.00012  0.000494 0.       0.       0.000066 0.00077  0.001248]\n"," [0.75     0.       0.0004   0.       0.       0.       0.000181 0.000179 0.001146]]\n"]}],"source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),6))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),6))\n","\n","print(beta_matrix_display)"]},{"cell_type":"code","execution_count":null,"id":"2Xm58jO6wpCm","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1698900,"status":"ok","timestamp":1704277811697,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"2Xm58jO6wpCm","outputId":"7b502184-485f-4cb4-b863-7a70b4f0bf54"},"outputs":[{"name":"stdout","output_type":"stream","text":["iter: 0, avg untrained: 1.1494670692868, avg trained linear: 1.0036818669649628, avg trained matrix: 1.0037602505001888\n"]}],"source":["#@title 100000 words\n","\n","import random\n","MAX_F = 8\n","n = 5\n","data_amount = 100000\n","\n","for i in range(1):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"]},{"cell_type":"code","execution_count":null,"id":"-KTttozPrhb5","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1704277811697,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"-KTttozPrhb5","outputId":"ae964d95-21bc-4889-de8c-2ac48771816e"},"outputs":[{"name":"stdout","output_type":"stream","text":["alpha: \n","[[ 0.5       0.5       0.5       0.5       0.5       0.497273  0.5       0.5       0.465175]\n"," [ 0.5       0.373589  0.441891  0.449966  0.459534  0.470508  0.481272  0.479496  0.395849]\n"," [ 0.5       0.        0.       -0.000001  0.090554  0.190307  0.264907  0.353419  0.000375]\n"," [ 0.5       0.        0.       -0.000084  0.        0.000134  0.        0.000027  0.      ]\n"," [ 0.5       0.        0.        0.        0.000193  0.000141  0.000357 -0.000313 -0.000192]\n"," [ 0.5       0.        0.        0.000122 -0.000028  0.000013 -0.000678 -0.000122 -0.000104]]\n","beta: \n","[[0.75     0.75     0.75     0.75     0.75     0.694872 0.75     0.75     0.      ]\n"," [0.75     0.401877 0.467307 0.436558 0.465392 0.495749 0.516013 0.515379 0.000329]\n"," [0.75     0.       0.       0.000041 0.000186 0.       0.000025 0.000332 0.001536]\n"," [0.75     0.       0.       0.000545 0.       0.000784 0.       0.000211 0.      ]\n"," [0.75     0.       0.       0.       0.000773 0.       0.001912 0.001052 0.004344]\n"," [0.75     0.       0.       0.       0.001641 0.001745 0.015804 0.000671 0.008087]]\n"]}],"source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),6))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),6))\n","\n","print(beta_matrix_display)"]},{"cell_type":"markdown","id":"omle8vjKrrDC","metadata":{"id":"omle8vjKrrDC"},"source":["Vary max f and n"]},{"cell_type":"code","execution_count":6,"id":"TVbJTJffnto6","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4286720,"status":"ok","timestamp":1704326715087,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"TVbJTJffnto6","outputId":"c980b014-7f59-4c17-ca3c-0a43330217cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["iter: 0, avg untrained: 1.012201485018517, avg trained linear: 0.7384294956786646, avg trained matrix: 0.739126202412177\n","iter: 1, avg untrained: 0.8495146729639187, avg trained linear: 0.6256500821663094, avg trained matrix: 0.6263357450493791\n","iter: 2, avg untrained: 0.7443254980407424, avg trained linear: 0.574677768889316, avg trained matrix: 0.5753518482574371\n","iter: 3, avg untrained: 0.6626572377635526, avg trained linear: 0.5332389412003412, avg trained matrix: 0.5335147449909117\n","iter: 4, avg untrained: 0.6015135922357661, avg trained linear: 0.5012216022354891, avg trained matrix: 0.501616837753367\n","iter: 5, avg untrained: 0.5518037948930371, avg trained linear: 0.47370959366659254, avg trained matrix: 0.47392188498178384\n","iter: 6, avg untrained: 0.5109415425439048, avg trained linear: 0.44929842780930984, avg trained matrix: 0.4498370147778219\n","iter: 7, avg untrained: 0.47731636600477395, avg trained linear: 0.4298003718484721, avg trained matrix: 0.4283618610886603\n"]}],"source":["#@title vary n\n","\n","import random\n","MAX_F = 10\n","n = 4\n","data_amount = 10000\n","\n","for i in range(8):\n","    n+=1\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"]},{"cell_type":"code","execution_count":null,"id":"v6PIbDRKeJOC","metadata":{"cellView":"form","id":"v6PIbDRKeJOC"},"outputs":[],"source":["#@title train test split\n","\n","import random\n","MAX_F = 10\n","n = 5\n","train_ratio = 0.8\n","data_amount = 10000\n","\n","for i in range(5):\n","    train_size = int(train_ratio * data_amount)\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    train_data = data[:train_size]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(train_data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(train_data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount*(1-train_ratio)\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data[train_size:]:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"]},{"cell_type":"code","execution_count":7,"id":"W_SA2_Q39bmN","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270185,"status":"ok","timestamp":1704329869225,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"W_SA2_Q39bmN","outputId":"370abd7a-38a7-4e3a-8d5c-91de07ccfcb5"},"outputs":[{"name":"stdout","output_type":"stream","text":["iter: 0, avg untrained: 0.9670477230515387, avg trained linear: 0.6621703969303782, avg trained matrix: 0.6627072837052418\n","iter: 1, avg untrained: 0.9690150804140151, avg trained linear: 0.6631114039560725, avg trained matrix: 0.6645192464071084\n","iter: 2, avg untrained: 0.9682885439891415, avg trained linear: 0.6630247734612777, avg trained matrix: 0.6642708293790092\n"]}],"source":["#@title 5000 words\n","\n","import random\n","MAX_F = 8\n","n = 5\n","data_amount = 5000\n","\n","for i in range(3):\n","    # load data\n","    data = []\n","    with open (\"./checked_words.txt\", \"r\") as f :\n","        for word in f:\n","            if not word or word[0] == \"#\" or \" \" in word:\n","                continue\n","\n","            data.append(word)\n","\n","    random.shuffle(data)\n","    data = data[:data_amount]\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(n+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","    iter = data_amount\n","    count = 0\n","\n","    total_linear = 0\n","    total_matrix = 0\n","    total_untrained = 0\n","    for word in data:\n","        word = word.strip()\n","        trained_score_linear = trainer_linear.word_prob_blend (alpha_matrix_linear, beta_matrix_linear, word, n_gram_model_linear, n)\n","        trained_score_matrix = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n","        score_untrained = trainer_matrix.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_matrix, n)\n","\n","        total_linear += trained_score_linear\n","        total_matrix += trained_score_matrix\n","        total_untrained += score_untrained\n","\n","        count += 1\n","        if count == iter:\n","            break\n","\n","    print(f\"iter: {i}, avg untrained: {-total_untrained/iter}, avg trained linear: {-total_linear/iter}, avg trained matrix: {-total_matrix/iter}\")"]},{"cell_type":"code","execution_count":9,"id":"lJXvlFzG9bmT","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":318,"status":"ok","timestamp":1704329990667,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"lJXvlFzG9bmT","outputId":"7f49f868-6395-4585-b1a1-8946066d0f4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["alpha: \n","[[ 0.47353782  0.        ]\n"," [ 0.20684438 -0.02577513]\n"," [-0.00119564  0.00024771]\n"," [ 0.00000418  0.00009018]\n"," [ 0.00017365  0.00052105]\n"," [ 0.          0.        ]]\n","beta: \n","[[0.         0.        ]\n"," [0.00076661 0.00076661]\n"," [0.00467083 0.00916044]\n"," [0.00072476 0.00194751]\n"," [0.00020572 0.00061716]\n"," [0.         0.        ]]\n"]}],"source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix_linear),8))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix_linear),8))\n","\n","print(beta_matrix_display)"]},{"cell_type":"code","execution_count":10,"id":"Xf7zfrLd_ADp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":345,"status":"ok","timestamp":1704330016824,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"Xf7zfrLd_ADp","outputId":"cf94addd-f443-422b-de82-8d53fa8a2ad0"},"outputs":[{"name":"stdout","output_type":"stream","text":["alpha: \n","[[ 0.5         0.5         0.5         0.5         0.5         0.5         0.5         0.5         0.47353776]\n"," [ 0.5         0.43326074  0.45266128  0.47152618  0.4804843   0.48724664  0.48779497  0.48566135  0.40696124]\n"," [ 0.5         0.          0.          0.14755361  0.25419682  0.32894775  0.3694433   0.40575499  0.20929432]\n"," [ 0.5         0.          0.         -0.00005069  0.         -0.00013946  0.17928492  0.30040829  0.10880048]\n"," [ 0.5         0.          0.          0.          0.00064773  0.13726075  0.26679744  0.33140153  0.29766416]\n"," [ 0.5         0.          0.         -0.00035411  0.1964305   0.29180837  0.35830671  0.38057038  0.37050732]]\n","beta: \n","[[0.75       0.75       0.75       0.75       0.75       0.75       0.75       0.75       0.        ]\n"," [0.75       0.52848669 0.53129008 0.56782384 0.59052892 0.61829175 0.63136607 0.61760532 0.00076474]\n"," [0.75       0.         0.         0.         0.         0.00052375 0.         0.         0.00730998]\n"," [0.75       0.         0.         0.00005069 0.         0.00107286 0.00230333 0.00033304 0.00038741]\n"," [0.75       0.         0.         0.         0.0033881  0.00072074 0.00110051 0.23329054 0.00188098]\n"," [0.75       0.         0.         0.00035411 0.1649022  0.33741612 0.376209   0.4278862  0.11660271]]\n"]}],"source":["#@title display\n","print(\"alpha: \")\n","alpha_matrix_display = np.array(np.around(np.array(alpha_matrix),8))\n","\n","print(alpha_matrix_display)\n","\n","print(\"beta: \")\n","beta_matrix_display = np.array(np.around(np.array(beta_matrix),8))\n","\n","print(beta_matrix_display)"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/czx20010217/pubmed_typos/blob/main/word_models/blending.ipynb","timestamp":1704179776075}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":5}
