{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e7724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_CHAR = 2**16\n",
    "\n",
    "\n",
    "# derivative first half (log P(s) / alpha)\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "import italian_vocab\n",
    "import chinese\n",
    "\n",
    "# n = 4   # number of letters as context\n",
    "MAX_N = 4\n",
    "\n",
    "\n",
    "class Blending_with_linear_param:\n",
    "    max_f = 0\n",
    "    \n",
    "    def __init__(self, max_f):\n",
    "        self.max_f = max_f\n",
    "        return None\n",
    "    \n",
    "    def derivative_of_P_to_alpha(self, alpha_matrix, beta_matrix, model, prefix, x):\n",
    "        \n",
    "        n = len(prefix)\n",
    "        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix, model)\n",
    "        if prefix in model:\n",
    "            ms = sum(model[prefix].values())\n",
    "            us = len(model[prefix])\n",
    "            if x in model[prefix]:\n",
    "                ms_x = model[prefix][x]\n",
    "            else:\n",
    "                ms_x = 0\n",
    "        else:\n",
    "            ms = 0\n",
    "            us = 0\n",
    "            ms_x = 0\n",
    "            \n",
    "        eq1 = (beta-ms_x) / (ms+alpha)**2\n",
    "        \n",
    "        if len(prefix) == 0:\n",
    "            eq2 = (ms-us*beta) / (ms+alpha)**2 / TOTAL_CHAR\n",
    "            eq3 = 0\n",
    "        else:\n",
    "            next_prefix = prefix[1:]\n",
    "            eq2 = (ms-us*beta) / (ms+alpha)**2 * self.blend(alpha_matrix, beta_matrix, next_prefix, x, model)\n",
    "            eq3 = (alpha + us*beta) / (ms+alpha) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, next_prefix, x)\n",
    "        \n",
    "        return eq1 + eq2 + eq3\n",
    "            \n",
    "    def derivative_of_P_to_beta(self, alpha_matrix, beta_matrix, model, prefix, x):\n",
    "        \n",
    "        n = len(prefix)\n",
    "        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix, model)\n",
    "        if prefix in model:\n",
    "            ms = sum(model[prefix].values())\n",
    "            us = len(model[prefix])\n",
    "            if x in model[prefix]:\n",
    "                ms_x = model[prefix][x]\n",
    "            else:\n",
    "                ms_x = 0\n",
    "        else:\n",
    "            ms = 0\n",
    "            us = 0\n",
    "            ms_x = 0\n",
    "            \n",
    "        eq1 = -1 / (ms+alpha)\n",
    "        if len(prefix) == 0:\n",
    "            eq2 = (ms-us*beta) / (ms+alpha)**2 / TOTAL_CHAR\n",
    "            eq3 = 0\n",
    "        else:\n",
    "            next_prefix = prefix[1:]\n",
    "            eq2 = us / (ms+alpha) * self.blend(alpha_matrix, beta_matrix, next_prefix, x, model, n)\n",
    "            eq3 = (alpha + us*beta) / (ms+alpha) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, next_prefix, x)\n",
    "        \n",
    "        return eq1 + eq2 + eq3\n",
    "            \n",
    "\n",
    "    def update_param(self, alpha_matrix, beta_matrix, prefix, x, model):\n",
    "        learning_rate = 0.003\n",
    "        n = len(prefix)\n",
    "        # add a computation for alpha beta\n",
    "        param_derivative_mapping = [1, len(set(prefix))]\n",
    "        \n",
    "        \n",
    "        for ind, alpha_derivative in zip(range(len(alpha_matrix[n])), param_derivative_mapping):\n",
    "            derivative_alpha = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, prefix, x) * alpha_derivative\n",
    "            alpha_matrix[n][ind] += learning_rate * derivative_alpha\n",
    "            \n",
    "        for ind, beta_derivative in zip(range(len(beta_matrix[n])), param_derivative_mapping):\n",
    "            derivative_beta = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, prefix, x) * beta_derivative\n",
    "            beta_matrix[n][ind] += learning_rate * derivative_beta\n",
    "            \n",
    "        # restrict param for beta\n",
    "        beta_matrix[n][0] = max(0, min(1, beta_matrix[n][0]))\n",
    "        \n",
    "        if beta_matrix[n][0] + self.max_f * beta_matrix[n][1] > 1:\n",
    "            beta_matrix[n][1] = (1 - beta_matrix[n][0]) / self.max_f\n",
    "        elif beta_matrix[n][0] + self.max_f * beta_matrix[n][1] < 0:\n",
    "            beta_matrix[n][1] = -beta_matrix[n][0] / self.max_f\n",
    "            \n",
    "        max_beta = max(beta_matrix[n][0], self.max_f*beta_matrix[n][1])\n",
    "        # restrict param for alpha\n",
    "        alpha_matrix[n][0] = max(alpha_matrix[n][0], -max_beta)\n",
    "        \n",
    "        if alpha_matrix[n][0] + self.max_f * alpha_matrix[n][1] < 0:\n",
    "            alpha_matrix[n][1] = -alpha_matrix[n][0] / self.max_f\n",
    "        \n",
    "            \n",
    "\n",
    "    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix, model):\n",
    "        n = len(prefix)\n",
    "        if prefix in model:\n",
    "            f = len(model[prefix])\n",
    "        else:\n",
    "            f = 0\n",
    "        \n",
    "        if f > self.max_f:\n",
    "            f = self.max_f\n",
    "            \n",
    "        param_derivative_mapping = [1, f]\n",
    "        alpha = np.dot(alpha_matrix[n], param_derivative_mapping)\n",
    "        beta = np.dot(beta_matrix[n], param_derivative_mapping)\n",
    "        \n",
    "        #beta = max(0, min(1, beta))\n",
    "        # alpha = max(-beta, alpha)\n",
    "        \n",
    "        \n",
    "        return (alpha, beta)\n",
    "\n",
    "    def build_model_with_train(self, data, alpha_matrix, beta_matrix, word_file, n):\n",
    "        \"\"\"\n",
    "        Build n-gram model of the words in  words_lang.txt\n",
    "        \"\"\"\n",
    "        n_grams = {}\n",
    "        # read in all n+1 grams\n",
    "        n_plus_1_gram_counts = {}\n",
    "        for word in data:\n",
    "            # update n-gram\n",
    "            if not word or word[0] == \"#\" or \" \" in word:\n",
    "                continue\n",
    "            word = \"^\" * n + word.strip() + \"$\"\n",
    "            for i in range(len(word) - n):\n",
    "                n_plus_1_gram = word[i:i + n + 1]\n",
    "                    \n",
    "                for N in range(n, -1, -1):\n",
    "                    x = n_plus_1_gram[-1]\n",
    "                    next_n = n_plus_1_gram[:-1]\n",
    "                    \n",
    "                    if next_n not in n_grams:\n",
    "                        n_grams[next_n] = {}\n",
    "                        \n",
    "                    if x not in n_grams[next_n]:\n",
    "                        n_grams[next_n][x] = 1\n",
    "                    else:\n",
    "                        n_grams[next_n][x] += 1\n",
    "                        \n",
    "                    n_plus_1_gram = n_plus_1_gram[1:]\n",
    "                    \n",
    "            # update alpha and beta\n",
    "            for i in range(len(word) - n):\n",
    "                n_plus_1_gram = word[i:i + n + 1]\n",
    "                    \n",
    "                for N in range(n, -1, -1):\n",
    "                    x = n_plus_1_gram[-1]\n",
    "                    next_n = n_plus_1_gram[:-1]\n",
    "                    \n",
    "                    self.update_param(alpha_matrix, beta_matrix, next_n, x, n_grams)\n",
    "                        \n",
    "                    n_plus_1_gram = n_plus_1_gram[1:]\n",
    "\n",
    "        return n_grams\n",
    "    \n",
    "    def blend(self, alpha_matrix, beta_matrix, prefix, x, model, n=4):\n",
    "        prob = 0\n",
    "        \n",
    "        # compute alpha and beta\n",
    "        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix, model)\n",
    "        possible_x = TOTAL_CHAR\n",
    "\n",
    "        # first half of algorithm\n",
    "        if prefix in model:\n",
    "            base = (len(model[prefix])*beta + alpha) / (sum(model[prefix].values()) + alpha)\n",
    "            if x in model[prefix]:\n",
    "                prob += (model[prefix][x] - beta) / (sum(model[prefix].values()) + alpha)\n",
    "        else:\n",
    "            base = (0 + alpha) / (0 + alpha)\n",
    "            \n",
    "        if len(prefix) == 0:\n",
    "            prob += base / possible_x\n",
    "        else:\n",
    "            prob += base * self.blend(alpha_matrix, beta_matrix, prefix[1:], x, model, n)\n",
    "            \n",
    "        return prob\n",
    "\n",
    "\n",
    "    def word_prob_blend(self, alpha_matrix, beta_matrix, word, model, n=4):\n",
    "        word = \"^\" * n + word.strip() + \"$\"\n",
    "        pos = n  # char after n ^\n",
    "        log_likelihood = 0\n",
    "        # print (\"   \", end=\"\")\n",
    "        while pos < len(word):\n",
    "            prefix = word[pos - n:pos]\n",
    "            prob = self.blend(alpha_matrix, beta_matrix, prefix, word[pos], model, n)\n",
    "            # print(prefix, word[pos], prob)\n",
    "            if prob:\n",
    "                log_likelihood += math.log (prob)\n",
    "            pos += 1\n",
    "\n",
    "\n",
    "        return log_likelihood / (len(word) - 1)  # didn't guess \"^\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9e8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer with 2d matrix for d and f\n",
    "\n",
    "class Blending_with_2d_matrix(Blending_with_linear_param):\n",
    "    def init(self, max_f):\n",
    "        super(max_f)\n",
    "        return None\n",
    "    \n",
    "    def update_param(self, alpha_matrix, beta_matrix, prefix, x, model):\n",
    "        learning_rate = 0.003\n",
    "        d = len(prefix)\n",
    "        n = d\n",
    "        if prefix in model:\n",
    "            f = len(model[prefix])\n",
    "        else:\n",
    "            f = 0\n",
    "        \n",
    "        if f > self.max_f:\n",
    "            f = self.max_f\n",
    "            \n",
    "        derivative_beta = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, model, prefix, x)\n",
    "        beta_matrix[d][f] += learning_rate * derivative_beta\n",
    "        \n",
    "        beta_matrix[d][f] = max(0, min(1, beta_matrix[d][f]))\n",
    "        \n",
    "        derivative_alpha = 1 / self.blend(alpha_matrix, beta_matrix, prefix, x, model, n) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, model, prefix, x)\n",
    "        alpha_matrix[d][f] += learning_rate * derivative_alpha\n",
    "        \n",
    "        alpha_matrix[d][f] = max(-beta_matrix[d][f], alpha_matrix[d][f])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix, model):\n",
    "        d = len(prefix)\n",
    "        \n",
    "        if prefix in model:\n",
    "            f = len(model[prefix])\n",
    "        else:\n",
    "            f = 0\n",
    "        \n",
    "        if f > self.max_f:\n",
    "            f = self.max_f\n",
    "        \n",
    "        return (alpha_matrix[d][f], beta_matrix[d][f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "721ad0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# test build model and train\n",
    "MAX_F = 10\n",
    "n = 5\n",
    "\n",
    "# load data\n",
    "data = []\n",
    "with open (\"words_manual_en.txt\", \"r\") as f :\n",
    "    for word in f:\n",
    "        data.append(word)\n",
    "\n",
    "alpha_matrix = [[0.5, 0] for _ in range(n+1)]\n",
    "beta_matrix = [[0.75, 0] for _ in range(n+1)]\n",
    "\n",
    "trainer_1 = Blending_with_linear_param(MAX_F)\n",
    "n_gram_model_1 = trainer_1.build_model_with_train(alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n",
    "\n",
    "# verify\n",
    "cand = [chr(i) for i in range(ord('A'), ord('Z') + 1)] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\"$\"]\n",
    "\n",
    "# alpha_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n",
    "# beta_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n",
    "def verify(trainer, prefix, model):\n",
    "    s = 0\n",
    "    for c in cand:\n",
    "        prob = trainer.blend(alpha_matrix, beta_matrix, prefix, c, model, n)\n",
    "        s += prob\n",
    "        \n",
    "    return s\n",
    "\n",
    "print(verify(trainer_1, \"ralv\", n_gram_model_1))\n",
    "print(verify(trainer_1, \"^All\", n_gram_model_1))\n",
    "print(verify(trainer_1, \"Trum\", n_gram_model_1))\n",
    "print(verify(trainer_1, \"Russ\", n_gram_model_1))\n",
    "print(verify(trainer_1, \"Koch\", n_gram_model_1))\n",
    "print(verify(trainer_1, \"Rack\", n_gram_model_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f6cf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.999999999744496\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# test build model and train\n",
    "MAX_F = 10\n",
    "n = 5\n",
    "\n",
    "# load data\n",
    "data = []\n",
    "with open (\"words_manual_en.txt\", \"r\") as f :\n",
    "    for word in f:\n",
    "        data.append(word)\n",
    "\n",
    "alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n",
    "beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n",
    "\n",
    "trainer_2 = Blending_with_2d_matrix(MAX_F)\n",
    "n_gram_model_2 = trainer_2.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n",
    "\n",
    "# verify\n",
    "cand = [chr(i) for i in range(ord('A'), ord('Z') + 1)] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\"$\"]\n",
    "\n",
    "def verify(trainer, prefix, model, n=n):\n",
    "    s = 0\n",
    "    for c in cand:\n",
    "        prob = trainer.blend(alpha_matrix, beta_matrix, prefix, c, model)\n",
    "        \n",
    "        s += prob\n",
    "        \n",
    "    return s\n",
    "\n",
    "print(verify(trainer_2, \"ralv\", n_gram_model_2))\n",
    "print(verify(trainer_2, \"^All\", n_gram_model_2))\n",
    "print(verify(trainer_2, \"Trum\", n_gram_model_2))\n",
    "print(verify(trainer_2, \"Russ\", n_gram_model_2))\n",
    "print(verify(trainer_2, \"Koch\", n_gram_model_2))\n",
    "print(verify(trainer_2, \"Rack\", n_gram_model_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfc535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.6850448508074591, 0.75, 0.75, 0.75, 0],\n",
       " [0.75,\n",
       "  0.38409422131037535,\n",
       "  0.4364834595076074,\n",
       "  0.44368669244751446,\n",
       "  0.49456958006947976,\n",
       "  0.4758418591768404,\n",
       "  0.5432620677200376,\n",
       "  0.5889374387396005,\n",
       "  0.5721489144783766,\n",
       "  0.6248700104831192,\n",
       "  0.00022023063195687254],\n",
       " [0.75,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.07090271169346975,\n",
       "  0.2580670172289869,\n",
       "  0.3898318945902937,\n",
       "  0.4647159029352439,\n",
       "  0.5597668408931874,\n",
       "  0.567961018414197,\n",
       "  0.3283024740495148],\n",
       " [0.75,\n",
       "  0,\n",
       "  0.0018261506595701968,\n",
       "  0.18288404908030825,\n",
       "  0.38100256242195457,\n",
       "  0.5123989433527012,\n",
       "  0.6013934670050458,\n",
       "  0.6690310715422526,\n",
       "  0.6850879957947016,\n",
       "  0.7127280704797957,\n",
       "  0.5436575850432567],\n",
       " [0.75,\n",
       "  0,\n",
       "  0.2504879866991498,\n",
       "  0.4465826846203506,\n",
       "  0.510607161145451,\n",
       "  0.5728522828300227,\n",
       "  0.6229852543617678,\n",
       "  0.677999897204668,\n",
       "  0.6848499927185444,\n",
       "  0.7154580369806254,\n",
       "  0.5376696016640183],\n",
       " [0.75,\n",
       "  0,\n",
       "  0.39500743908400676,\n",
       "  0.516119825427136,\n",
       "  0.5309930287467219,\n",
       "  0.5685782413096472,\n",
       "  0.6184963109752927,\n",
       "  0.6770879885268545,\n",
       "  0.684959100840458,\n",
       "  0.7196451706002198,\n",
       "  0.5358468880906523]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b3c34b",
   "metadata": {},
   "source": [
    "Compare training and non training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a0075ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg untrained: 0.41165258236321056, avg trained: 0.3019432506125171\n"
     ]
    }
   ],
   "source": [
    "# test build model and train for linear equation pram method\n",
    "MAX_F = 10\n",
    "n = 5\n",
    "\n",
    "# load data\n",
    "data = []\n",
    "with open (\"words_manual_en.txt\", \"r\") as f :\n",
    "    for word in f:\n",
    "        data.append(word)\n",
    "\n",
    "alpha_matrix = [[0.5, 0] for _ in range(n+1)]\n",
    "beta_matrix = [[0.75, 0] for _ in range(n+1)]\n",
    "\n",
    "trainer_1 = Blending_with_linear_param(MAX_F)\n",
    "n_gram_model_1 = trainer_1.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n",
    "\n",
    "alpha_matrix_untrained = [[0.5, 0] for _ in range(n+1)]\n",
    "beta_matrix_untrained = [[0.75, 0] for _ in range(n+1)]\n",
    "\n",
    "iter = 1000\n",
    "count = 0\n",
    "with open (\"words_manual_en.txt\", \"r\") as f :\n",
    "    total_un = 0\n",
    "    total = 0\n",
    "    for word in f:\n",
    "        word = word.strip()\n",
    "        untrained_score = trainer_1.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_1, n)\n",
    "        trained_score = trainer_1.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_1, n)\n",
    "        \n",
    "        total_un += untrained_score\n",
    "        total += trained_score\n",
    "        \n",
    "        count += 1\n",
    "        if count == iter:\n",
    "            break\n",
    "        \n",
    "    print(f\"avg untrained: {-total_un/iter}, avg trained: {-total/iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7231b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg untrained: 1.0146175802848714, avg trained: 0.7400299799082519\n"
     ]
    }
   ],
   "source": [
    "# test build model and train for matrix method\n",
    "import random\n",
    "\n",
    "data_amount = 10000\n",
    "count = 0\n",
    "\n",
    "# load data\n",
    "data = []\n",
    "with open (\"../checked_words.txt\", \"r\") as f :\n",
    "    for word in f:\n",
    "        if not word or word[0] == \"#\" or \" \" in word:\n",
    "            continue\n",
    "        \n",
    "        data.append(word)\n",
    "        \n",
    "random.shuffle(data)\n",
    "data = data[:data_amount]\n",
    "\n",
    "MAX_F = 10\n",
    "n = 5\n",
    "alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n",
    "beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n",
    "\n",
    "trainer_2 = Blending_with_2d_matrix(MAX_F)\n",
    "n_gram_model_2 = trainer_2.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n",
    "\n",
    "alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n",
    "beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n",
    "    \n",
    "    \n",
    "total_un = 0\n",
    "total = 0\n",
    "for word in data:\n",
    "    word = word.strip()\n",
    "    untrained_score = trainer_2.word_prob_blend (alpha_matrix_untrained, beta_matrix_untrained, word, n_gram_model_2, n)\n",
    "    trained_score = trainer_2.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_2, n)\n",
    "    \n",
    "    total_un += untrained_score\n",
    "    total += trained_score\n",
    "    \n",
    "    count += 1\n",
    "    if count == data_amount:\n",
    "        break\n",
    "    \n",
    "print(f\"avg untrained: {-total_un/data_amount}, avg trained: {-total/data_amount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f80b1",
   "metadata": {},
   "source": [
    "Compare the effect of input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd4cc0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, avg trained: 0.07399423562446077\n"
     ]
    }
   ],
   "source": [
    "# test input sequence for matrix method\n",
    "import random\n",
    "MAX_F = 10\n",
    "n = 5\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    # load data\n",
    "    data = []\n",
    "    with open (\"../checked_words.txt\", \"r\") as f :\n",
    "        for word in f:\n",
    "            if not word or word[0] == \"#\" or \" \" in word:\n",
    "                continue\n",
    "            \n",
    "            data.append(word)\n",
    "            \n",
    "    random.shuffle(data)\n",
    "    data = data[:10000]\n",
    "\n",
    "    alpha_matrix = [[0.5, 0] for _ in range(n+1)]\n",
    "    beta_matrix = [[0.75, 0] for _ in range(n+1)]\n",
    "\n",
    "    trainer_linear = Blending_with_linear_param(MAX_F)\n",
    "    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n",
    "    \n",
    "    iter = 100000\n",
    "    count = 0\n",
    "    \n",
    "    total_un = 0\n",
    "    total = 0\n",
    "    for word in data:\n",
    "        word = word.strip()\n",
    "        trained_score = trainer_linear.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_linear, n)\n",
    "        \n",
    "        total += trained_score\n",
    "        \n",
    "        count += 1\n",
    "        if count == iter:\n",
    "            break\n",
    "        \n",
    "    print(f\"iter: {i}, avg trained: {-total/iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8af533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, avg trained: 0.7501159899187466\n"
     ]
    }
   ],
   "source": [
    "# test input sequence for matrix method\n",
    "import random\n",
    "MAX_F = 10\n",
    "n = 5\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    # load data\n",
    "    data = []\n",
    "    with open (\"../checked_words.txt\", \"r\") as f :\n",
    "        for word in f:\n",
    "            data.append(word)\n",
    "            \n",
    "    random.shuffle(data)\n",
    "    \n",
    "    data = data[:10000]\n",
    "\n",
    "    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n",
    "    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n",
    "\n",
    "    trainer_matrix = Blending_with_2d_matrix(MAX_F)\n",
    "    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n",
    "    \n",
    "    iter = 10000\n",
    "    count = 0\n",
    "    \n",
    "    total_un = 0\n",
    "    total = 0\n",
    "    for word in data:\n",
    "        word = word.strip()\n",
    "        trained_score = trainer_matrix.word_prob_blend (alpha_matrix, beta_matrix, word, n_gram_model_matrix, n)\n",
    "        \n",
    "        total += trained_score\n",
    "        \n",
    "        count += 1\n",
    "        if count == iter:\n",
    "            break\n",
    "        \n",
    "    print(f\"iter: {i}, avg trained: {-total/iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7855719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused code\n",
    "def build_model(self, word_file, n):\n",
    "        \"\"\"\n",
    "        Build n-gram model of the words in  words_lang.txt\n",
    "        \"\"\"\n",
    "        # read in all n+1 grams\n",
    "        n_plus_1_gram_counts = {}\n",
    "        with open(word_file, \"r\") as f:\n",
    "            for word in f:\n",
    "                if not word or word[0] == \"#\" or \" \" in word:\n",
    "                    continue\n",
    "                word = \"^\" * n + word.strip() + \"$\"\n",
    "                for i in range(len(word) - n):\n",
    "                    n_plus_1_gram = word[i:i + n + 1]\n",
    "                    if n_plus_1_gram not in n_plus_1_gram_counts:\n",
    "                        n_plus_1_gram_counts[n_plus_1_gram] = 1\n",
    "                    else:\n",
    "                        n_plus_1_gram_counts[n_plus_1_gram] += 1\n",
    "        # with open (\"tmpppp\", \"w\") as f :\n",
    "        #  for key in sorted(n_plus_1_gram_counts) :\n",
    "        #    print (str({key: n_plus_1_gram_counts[key]})[1:-1], file=f)\n",
    "\n",
    "        # Find conditional probability of n+1st from previous n-gram\n",
    "        counts = {}\n",
    "        n_grams = {x: {} for x in range(0, n + 1)}\n",
    "        for N in range(n, -1, -1):\n",
    "            n_gram = None\n",
    "            for ng in sorted(n_plus_1_gram_counts):\n",
    "                next_n = ng[:-1]\n",
    "                if n_gram != next_n:\n",
    "                    if n_gram == None:\n",
    "                        n_gram = next_n\n",
    "                    else:\n",
    "                        s = sum([counts[x] for x in counts])\n",
    "                        if not n_gram.startswith('^^'):  # at most one ^ at the start\n",
    "                            n_grams[N][n_gram] = {c: counts[c] for c in counts}\n",
    "\n",
    "                        n_gram = next_n\n",
    "                        counts = {}\n",
    "\n",
    "                counts[ng[-1]] = n_plus_1_gram_counts[ng]\n",
    "\n",
    "            # process last case.  Should we put a sentinel in n_plus_1_gram_counts?\n",
    "            s = sum([counts[x] for x in counts])\n",
    "            n_grams[N][n_gram] = {c: counts[c] for c in counts}\n",
    "\n",
    "            if N > 0:\n",
    "                new_n = {}\n",
    "                for key in n_plus_1_gram_counts:\n",
    "                    suff = key[1:]\n",
    "                    if suff in new_n:\n",
    "                        new_n[suff] += 1\n",
    "                    else:\n",
    "                        new_n[suff] = 1\n",
    "                n_plus_1_gram_counts = new_n\n",
    "\n",
    "        # compress to variable n?\n",
    "        # print(n_grams)\n",
    "\n",
    "        for N in range(n):\n",
    "            n_grams[N + 1].update(n_grams[N])\n",
    "\n",
    "        return n_grams[n]\n",
    "    \n",
    "def word_prob(self, word, model):\n",
    "    word = \"^\" + word + \"$\"\n",
    "    pos = 1  # char after ^\n",
    "    log_likelihood = 0\n",
    "    # print (\"   \", end=\"\")\n",
    "    while pos < len(word):\n",
    "        done = False\n",
    "        for i in range(pos):\n",
    "            history = word[i:pos]\n",
    "            if history in model:\n",
    "                if history not in model:\n",
    "                    # Should penalize this\n",
    "                    history = history.lower()\n",
    "                try:\n",
    "                    log_likelihood += math.log(model[history][word[pos]] / sum(model[history].values()))\n",
    "                except:\n",
    "                    low = word[pos].lower()\n",
    "                    if low in model[history]:\n",
    "                        # Should penalize this\n",
    "                        log_likelihood += math.log(model[history][low] / sum(model[history].values()))\n",
    "                    else:\n",
    "                        # pdb.set_trace ()\n",
    "                        log_likelihood += -20\n",
    "                # print (int(log_likelihood), end = \" \")\n",
    "                log_likelihood -= 3 * i  # penalize shorter histories\n",
    "                done = True\n",
    "                pos += 1\n",
    "                break\n",
    "\n",
    "        if not done:  # Transisition so unlikely, it was never seen\n",
    "            # Should use \"smoothing\", but this will disappear\n",
    "            # when the variable-length prefixes are implemented\n",
    "            log_likelihood += -20\n",
    "            # print (int(log_likelihood), end = \" \")\n",
    "            pos += 1\n",
    "\n",
    "    # print()\n",
    "\n",
    "    return log_likelihood / (len(word) - 1)  # didn't guess \"^\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
