{"cells":[{"cell_type":"code","execution_count":1,"id":"qk_9RqHebmTH","metadata":{"id":"qk_9RqHebmTH","executionInfo":{"status":"ok","timestamp":1707888368570,"user_tz":-660,"elapsed":369,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}}},"outputs":[],"source":["alice = []\n","\n","with open (\"./cantrbry/alice29.txt\", \"r\") as f :\n","    alice = f.read()\n","\n","lcet = []\n","\n","with open (\"./cantrbry/lcet10.txt\", \"r\") as f :\n","    lcet = f.read()\n","\n","plrabn12 = []\n","\n","with open (\"./cantrbry/plrabn12.txt\", \"r\") as f :\n","    plrabn12 = f.read()\n","\n","asyoulik = []\n","\n","with open (\"./cantrbry/asyoulik.txt\", \"r\") as f :\n","    asyoulik = f.read()\n"]},{"cell_type":"code","execution_count":null,"id":"3c45UY7HKoal","metadata":{"id":"3c45UY7HKoal","cellView":"form"},"outputs":[],"source":["#@title read binary files\n","\n","kennedy = []\n","ptt5 = []\n","sum_file = []\n","\n","with open (\"./kennedy.xls\", \"rb\") as f :\n","    kennedy = f.read()\n","\n","with open (\"./ptt5\", \"rb\") as f :\n","    ptt5 = f.read()\n","\n","with open (\"./sum\", \"rb\") as f :\n","    byte = f.read(1)\n","    while byte:\n","        sum_file.append(byte)\n","        # Read the next byte\n","        byte = f.read(1)\n"]},{"cell_type":"code","execution_count":102,"id":"10e7724a","metadata":{"cellView":"form","executionInfo":{"elapsed":351,"status":"ok","timestamp":1707916659609,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"10e7724a"},"outputs":[],"source":["#@title Model: linear\n","\n","TOTAL_CHAR = 2**16\n","import random\n","\n","# derivative first half (log P(s) / alpha)\n","\n","import sys\n","import pickle\n","import math\n","import pdb\n","import numpy as np\n","np.set_printoptions(suppress=True)\n","np.set_printoptions(linewidth=400)\n","\n","# n = 4   # number of letters as context\n","MAX_N = 4\n","MAX_UPDATE_RATIO = 0.4\n","MINIMUM_ALPHA = 1e-10\n","\n","class Blending_with_linear_param:\n","    max_f = 0\n","    max_d = 0\n","    total_score = 0\n","    count = 0\n","    logs = []\n","    logs_score_individual = []\n","    zero_prob = 0\n","    n_gram_model = {}\n","    learning_rate = 0.003\n","\n","    def __init__(self, max_f, max_d, learning_rate):\n","        self.max_f = max_f\n","        self.max_d = max_d\n","        self.logs = []\n","        self.logs_score_individual = []\n","        self.zero_prob = 0\n","        self.total_score = 0\n","        self.n_gram_model = {}\n","        self.learning_rate = learning_rate\n","        return None\n","\n","    def derivative_of_P_to_alpha(self, alpha_matrix, beta_matrix, prefix, x):\n","\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","        if prefix in self.n_gram_model:\n","            ms = sum(self.n_gram_model[prefix].values())\n","            us = len(self.n_gram_model[prefix])\n","            if x in self.n_gram_model[prefix]:\n","                ms_x = self.n_gram_model[prefix][x]\n","            else:\n","                ms_x = 0\n","        else:\n","            ms = 0\n","            us = 0\n","            ms_x = 0\n","\n","        eq1 = (beta-ms_x) / (ms+alpha)**2\n","\n","        if len(prefix) == 0:\n","            eq2 = (ms-us*beta) / (ms+alpha)**2 / TOTAL_CHAR\n","            eq3 = 0\n","        else:\n","            next_prefix = prefix[1:]\n","            eq2 = (ms-us*beta) / (ms+alpha)**2 * self.blend(alpha_matrix, beta_matrix, next_prefix, x)\n","            eq3 = (alpha + us*beta) / (ms+alpha) * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, next_prefix, x)\n","\n","        return eq1 + eq2 + eq3\n","\n","    def derivative_of_P_to_beta(self, alpha_matrix, beta_matrix, prefix, x):\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","        if prefix in self.n_gram_model:\n","            ms = sum(self.n_gram_model[prefix].values())\n","            us = len(self.n_gram_model[prefix])\n","            if x in self.n_gram_model[prefix]:\n","                ms_x = self.n_gram_model[prefix][x]\n","            else:\n","                ms_x = 0\n","        else:\n","            ms = 0\n","            us = 0\n","            ms_x = 0\n","\n","        eq1 = -1 / (ms+alpha)\n","        if len(prefix) == 0:\n","            eq2 = us / (ms+alpha)/ TOTAL_CHAR\n","            eq3 = 0\n","        else:\n","            next_prefix = prefix[1:]\n","            eq2 = us / (ms+alpha) * self.blend(alpha_matrix, beta_matrix, next_prefix, x)\n","            eq3 = (alpha + us*beta) / (ms+alpha) * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, next_prefix, x)\n","\n","        return eq1 + eq2 + eq3\n","\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x):\n","        learning_rate = self.learning_rate\n","\n","        n = len(prefix)\n","        if n > self.max_d:\n","            n = self.max_d\n","\n","        # add a computation for alpha beta\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","        param_derivative_mapping = [1, f]\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","\n","        prob = self.blend(alpha_matrix, beta_matrix, prefix, x)\n","        if prob == 0:\n","            return\n","        allowed_beta_minimum = 0\n","        allowed_beta_maximum = 1\n","\n","        # new\n","        derivative_alpha = 1 / prob * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, prefix, x)\n","        derivative_beta = 1 / prob * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, prefix, x)\n","        beta_update = learning_rate * derivative_beta\n","        alpha_update = learning_rate * derivative_alpha\n","        factor = 1\n","\n","        allowed_alpha_maximum = (alpha_matrix[n][0] + max(0, alpha_matrix[n][1]*self.max_f)) * (1+MAX_UPDATE_RATIO)\n","        allowed_alpha_minimum = (alpha_matrix[n][0] + min(0, alpha_matrix[n][1]*self.max_f)) * (1-MAX_UPDATE_RATIO)\n","        allowed_alpha_minimum = max(MINIMUM_ALPHA, allowed_alpha_minimum)\n","\n","        allowed_beta_minimum = (beta_matrix[n][0] + min(0, beta_matrix[n][1]*self.max_f)) * (1-MAX_UPDATE_RATIO)\n","        allowed_beta_maximum = (beta_matrix[n][0] + max(0, beta_matrix[n][1]*self.max_f)) * (1+MAX_UPDATE_RATIO)\n","        allowed_beta_maximum = min(allowed_beta_maximum, 1)\n","        allowed_beta_minimum = max(MINIMUM_ALPHA, allowed_beta_minimum)\n","\n","        # # print(allowed_alpha_minimum, allowed_alpha_maximum)\n","\n","        # old_alpha_0 = alpha_matrix[n][0]\n","        # old_alpha_1 = alpha_matrix[n][1]\n","        # old_beta_0 = beta_matrix[n][0]\n","        # old_beta_1 = beta_matrix[n][1]\n","\n","        for ind, derivative in zip(range(len(param_derivative_mapping)), param_derivative_mapping):\n","            beta_matrix[n][ind] += beta_update * derivative\n","            alpha_matrix[n][ind] += alpha_update * derivative\n","\n","        # if n == 5:\n","        #     print(allowed_alpha_minimum, allowed_alpha_maximum)\n","        #     print(\"values:\", alpha, beta, prob, alpha_update, beta_update, f)\n","\n","        # restrict param for beta\n","        beta_matrix[n][0] = max(allowed_beta_minimum, min(allowed_beta_maximum, beta_matrix[n][0]))\n","\n","        if beta_matrix[n][0] + self.max_f * beta_matrix[n][1] > allowed_beta_maximum:\n","            beta_matrix[n][1] = (allowed_beta_maximum - beta_matrix[n][0]) / self.max_f\n","        elif beta_matrix[n][0] + self.max_f * beta_matrix[n][1] < allowed_beta_minimum:\n","            beta_matrix[n][1] = (allowed_beta_minimum-beta_matrix[n][0]) / self.max_f\n","\n","        # max_beta = max(beta_matrix[n][0], beta_matrix[n][0]+self.max_f*beta_matrix[n][1])\n","        # restrict param for alpha\n","        alpha_matrix[n][0] = max(allowed_alpha_minimum, min(allowed_alpha_maximum, alpha_matrix[n][0]))\n","\n","        if alpha_matrix[n][0] + self.max_f * alpha_matrix[n][1] < allowed_alpha_minimum:\n","            alpha_matrix[n][1] = (allowed_alpha_minimum-alpha_matrix[n][0]) / self.max_f\n","        elif alpha_matrix[n][0] + self.max_f * alpha_matrix[n][1] > allowed_alpha_maximum:\n","            alpha_matrix[n][1] = (allowed_alpha_maximum-alpha_matrix[n][0]) / self.max_f\n","\n","        # if beta_update == 0 or alpha_update == 0:\n","        #     return\n","\n","        # # compute factor\n","        # if f == 0:\n","        #     factor = min((alpha_matrix[n][0] - old_alpha_0) / (alpha_update), (beta_matrix[n][0] - old_beta_0) / (beta_update),)\n","        # else:\n","        #     factor = min((alpha_matrix[n][0] - old_alpha_0) / (alpha_update), (alpha_matrix[n][1] - old_alpha_1) / (alpha_update*f),\n","        #                 (beta_matrix[n][0] - old_beta_0) / (beta_update), (beta_matrix[n][1] - old_beta_1) / (beta_update*f))\n","\n","        # assert(factor > 0)\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix):\n","        n = len(prefix)\n","        if n > self.max_d:\n","            n = self.max_d\n","\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        param_derivative_mapping = [1, f]\n","\n","        alpha = alpha_matrix[n][0] + f * alpha_matrix[n][1]\n","        beta = beta_matrix[n][0] + f * beta_matrix[n][1]\n","\n","        return (alpha, beta)\n","\n","    def build_model_with_train(self, data, alpha_matrix, beta_matrix, n):\n","        \"\"\"\n","        Build n-gram model of the words in  words_lang.txt\n","        \"\"\"\n","        # read in all n+1 grams\n","        word_org = data\n","        # update n-gram\n","        # word = \"^\" * n + word + \"$\"\n","        for i in range(len(data) - n):\n","\n","            n_plus_1_gram = data[i:i + n + 1]\n","            score = self.letter_prob_score (alpha_matrix, beta_matrix, n_plus_1_gram[:-1], n_plus_1_gram[-1], n)\n","            self.total_score += score\n","            # self.total_score += self.blend (alpha_matrix, beta_matrix, n_plus_1_gram[:-1], n_plus_1_gram[-1], n_grams, n)\n","            self.count += 1\n","            self.logs.append(-self.total_score/self.count)\n","            self.logs_score_individual.append(score)\n","\n","            # n_plus_1_gram = word[i:i + n + 1]\n","            # for N in range(n, -1, -1):\n","            #     x = n_plus_1_gram[-1]\n","            #     next_n = n_plus_1_gram[:-1]\n","\n","            #     self.update_param(alpha_matrix, beta_matrix, next_n, x, n_grams)\n","\n","            #     n_plus_1_gram = n_plus_1_gram[1:]\n","\n","            n_plus_1_gram = data[i:i + n + 1]\n","            for N in range(n+1):\n","                x = n_plus_1_gram[-1]\n","                next_n = n_plus_1_gram[-1-N:-1]\n","                self.update_param(alpha_matrix, beta_matrix, next_n, x)\n","\n","\n","\n","            n_plus_1_gram = data[i:i + n + 1]\n","            for N in range(n, -1, -1):\n","                x = n_plus_1_gram[-1]\n","                next_n = n_plus_1_gram[:-1]\n","\n","                if next_n not in self.n_gram_model:\n","                    self.n_gram_model[next_n] = {}\n","\n","                if x not in self.n_gram_model[next_n]:\n","                    self.n_gram_model[next_n][x] = 1\n","                else:\n","                    self.n_gram_model[next_n][x] += 1\n","\n","                n_plus_1_gram = n_plus_1_gram[1:]\n","\n","\n","        return self.n_gram_model\n","\n","    def offline_optimisation(self, data, alpha_matrix, beta_matrix, word_file, n):\n","         for word in data:\n","            for i in range(len(word) - n):\n","                n_plus_1_gram = word[i:i + n + 1]\n","                for N in range(n+1):\n","                    x = n_plus_1_gram[-1]\n","                    next_n = n_plus_1_gram[-1-N:-1]\n","                    self.update_param(alpha_matrix, beta_matrix, next_n, x)\n","\n","    def blend(self, alpha_matrix, beta_matrix, prefix, x):\n","        prob = 0\n","\n","        # compute alpha and beta\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","        possible_x = TOTAL_CHAR\n","\n","        # first half of algorithm\n","        if prefix in self.n_gram_model:\n","            us = len(self.n_gram_model[prefix])\n","            ms = sum(self.n_gram_model[prefix].values())\n","            base = (len(self.n_gram_model[prefix])*beta + alpha) / (sum(self.n_gram_model[prefix].values()) + alpha)\n","            if x in self.n_gram_model[prefix]:\n","                prob += (self.n_gram_model[prefix][x] - beta) / (sum(self.n_gram_model[prefix].values()) + alpha)\n","        else:\n","            base = (0 + alpha) / (0 + alpha)\n","\n","        if len(prefix) == 0:\n","            prob += base / possible_x\n","        else:\n","            prev = self.blend(alpha_matrix, beta_matrix, prefix[1:], x)\n","            prob += base * prev\n","\n","        prob = prob\n","\n","        return prob\n","\n","\n","    def word_prob_blend(self, alpha_matrix, beta_matrix, word, n):\n","        word = \"^\" * n + word.strip() + \"$\"\n","        pos = n  # char after n ^\n","        log_likelihood = 0\n","        # print (\"   \", end=\"\")\n","        while pos < len(word):\n","            prefix = word[pos - n:pos]\n","            prob = self.blend(alpha_matrix, beta_matrix, prefix, word[pos])\n","            # print(prefix, word[pos], prob)\n","            if prob:\n","                log_likelihood += math.log2(prob)\n","            pos += 1\n","\n","\n","        return log_likelihood / (len(word) - 1)  # didn't guess \"^\"\n","\n","    def letter_prob_score(self, alpha_matrix, beta_matrix, prefix, x, n):\n","        prob = self.blend(alpha_matrix, beta_matrix, prefix, x)\n","        # print(prefix, word[pos], prob)\n","        if prob:\n","            return math.log2(prob)\n","        else:\n","            self.zero_prob += 1\n","            print(\"error\")\n","            return 0\n"]},{"cell_type":"code","execution_count":103,"id":"Tonw1zzBovYP","metadata":{"cellView":"form","executionInfo":{"elapsed":3,"status":"ok","timestamp":1707916660328,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"Tonw1zzBovYP"},"outputs":[],"source":["#@title Model: linear v2\n","\n","class Blending_with_linear_param_v2(Blending_with_linear_param):\n","    max_f = 0\n","    max_n = 0\n","\n","    def __init__(self, max_f, max_n, learning_rate):\n","        self.max_n = max_n\n","        super().__init__(max_f, max_n, learning_rate)\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x):\n","        learning_rate = self.learning_rate\n","\n","        n = len(prefix)\n","        if n > self.max_d:\n","            n = self.max_d\n","\n","        # add a computation for alpha beta\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","        param_derivative_mapping = [1, f, n]\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","\n","        prob = self.blend(alpha_matrix, beta_matrix, prefix, x)\n","        if prob == 0:\n","            return\n","\n","        # new\n","        derivative_alpha = 1 / prob * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, prefix, x)\n","        derivative_beta = 1 / prob * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, prefix, x)\n","        beta_update = learning_rate * derivative_beta\n","        alpha_update = learning_rate * derivative_alpha\n","        factor = 1\n","\n","        allowed_alpha_maximum = (alpha_matrix[0] + max(0, alpha_matrix[1]*self.max_f) + max(0, alpha_matrix[2]*self.max_n)) * (1+MAX_UPDATE_RATIO)\n","        allowed_alpha_minimum = (alpha_matrix[0] + min(0, alpha_matrix[1]*self.max_f) + min(0, alpha_matrix[2]*self.max_n)) * (1-MAX_UPDATE_RATIO)\n","        allowed_alpha_minimum = max(MINIMUM_ALPHA, allowed_alpha_minimum)\n","\n","        allowed_beta_maximum = (beta_matrix[0] + max(0, beta_matrix[1]*self.max_f) + max(0, beta_matrix[2]*self.max_n)) * (1+MAX_UPDATE_RATIO)\n","        allowed_beta_minimum = (beta_matrix[0] + min(0, beta_matrix[1]*self.max_f) + min(0, beta_matrix[2]*self.max_n)) * (1-MAX_UPDATE_RATIO)\n","        allowed_beta_maximum = min(allowed_beta_maximum, 1)\n","        allowed_beta_minimum = max(MINIMUM_ALPHA, allowed_beta_minimum)\n","\n","        for ind, derivative in zip(range(len(param_derivative_mapping)), param_derivative_mapping):\n","            beta_matrix[ind] += beta_update * derivative\n","            alpha_matrix[ind] += alpha_update * derivative\n","\n","        # restrict param for beta\n","        beta_matrix[0] = max(allowed_beta_minimum, min(allowed_beta_maximum, beta_matrix[0]))\n","\n","        if beta_matrix[0] + self.max_f * beta_matrix[1] > allowed_beta_maximum:\n","            beta_matrix[1] = (allowed_beta_maximum - beta_matrix[0]) / self.max_f\n","        elif beta_matrix[0] + self.max_f * beta_matrix[1] < allowed_beta_minimum:\n","            beta_matrix[1] = (allowed_beta_minimum-beta_matrix[0]) / self.max_f\n","\n","        if beta_matrix[0] + max(self.max_f * beta_matrix[1], 0) + beta_matrix[2]*self.max_n > allowed_beta_maximum:\n","            beta_matrix[2] = (allowed_beta_maximum - beta_matrix[0] - max(self.max_f * beta_matrix[1], 0)) / self.max_n\n","        elif beta_matrix[0] + min(self.max_f * beta_matrix[1], 0) + beta_matrix[2]*self.max_n < allowed_beta_minimum:\n","            beta_matrix[2] = (allowed_beta_minimum - beta_matrix[0] - min(self.max_f * beta_matrix[1], 0)) / self.max_n\n","\n","        # restrict param for alpha\n","        alpha_matrix[0] = max(allowed_alpha_minimum, min(allowed_alpha_maximum, alpha_matrix[0]))\n","\n","        if alpha_matrix[0] + self.max_f * alpha_matrix[1] < allowed_alpha_minimum:\n","            alpha_matrix[1] = (allowed_alpha_minimum-alpha_matrix[0]) / self.max_f\n","        elif alpha_matrix[0] + self.max_f * alpha_matrix[1] > allowed_alpha_maximum:\n","            alpha_matrix[1] = (allowed_alpha_maximum-alpha_matrix[0]) / self.max_f\n","\n","        if alpha_matrix[0] + max(self.max_f * alpha_matrix[1], 0) + alpha_matrix[2]*self.max_n > allowed_alpha_maximum:\n","            alpha_matrix[2] = (allowed_alpha_maximum - alpha_matrix[0] - max(self.max_f * alpha_matrix[1], 0)) / self.max_n\n","        elif alpha_matrix[0] + min(self.max_f * alpha_matrix[1], 0) + alpha_matrix[2]*self.max_n < allowed_alpha_minimum:\n","            alpha_matrix[2] = (allowed_alpha_minimum - alpha_matrix[0] - min(self.max_f * alpha_matrix[1], 0)) / self.max_n\n","\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix):\n","        n = len(prefix)\n","        if n > self.max_d:\n","            n = self.max_d\n","\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        param_derivative_mapping = [1, f, n]\n","        # alpha1 = np.dot(alpha_matrix[n], param_derivative_mapping)\n","        # beta1 = np.dot(beta_matrix[n], param_derivative_mapping)\n","\n","        alpha = alpha_matrix[0] + f * alpha_matrix[1] + n * alpha_matrix[2]\n","        beta = beta_matrix[0] + f* beta_matrix[1] + n * beta_matrix[2]\n","\n","        #beta = max(0, min(1, beta))\n","        # alpha = max(-beta, alpha)\n","\n","        return (alpha, beta)\n","\n"]},{"cell_type":"code","execution_count":104,"id":"UG45KTYaOLq-","metadata":{"cellView":"form","executionInfo":{"elapsed":3,"status":"ok","timestamp":1707916660328,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"UG45KTYaOLq-"},"outputs":[],"source":["#@title Model: linear v2 with piecewise linear on d\n","\n","class Blending_with_linear_param_v2_piecewise_d(Blending_with_linear_param):\n","    max_f = 0\n","    d_split = 0\n","\n","    def __init__(self, max_f, max_d, d_split, learning_rate):\n","        self.d_split = d_split\n","        super().__init__(max_f, max_d, learning_rate)\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x):\n","        learning_rate = self.learning_rate\n","        n = len(prefix)\n","        # add a computation for alpha beta\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        if n < self.d_split:\n","            d = 0\n","        else:\n","            d = 1\n","\n","        param_derivative_mapping = [1, f, n]\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","\n","        prob = self.blend(alpha_matrix, beta_matrix, prefix, x)\n","        if prob == 0:\n","            return\n","\n","        # new\n","        derivative_alpha = 1 / prob * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, prefix, x)\n","        derivative_beta = 1 / prob * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, prefix, x)\n","        beta_update = learning_rate * derivative_beta\n","        alpha_update = learning_rate * derivative_alpha\n","        factor = 1\n","\n","        allowed_alpha_maximum = (alpha_matrix[d][0] + max(0, alpha_matrix[d][1]*self.max_f) + max(0, alpha_matrix[d][2]*self.max_d)) * (1+MAX_UPDATE_RATIO)\n","        allowed_alpha_minimum = (alpha_matrix[d][0] + min(0, alpha_matrix[d][1]*self.max_f) + min(0, alpha_matrix[d][2]*self.max_d)) * (1-MAX_UPDATE_RATIO)\n","        allowed_alpha_minimum = max(MINIMUM_ALPHA, allowed_alpha_minimum)\n","\n","        allowed_beta_maximum = (beta_matrix[d][0] + max(0, beta_matrix[d][1]*self.max_f) + max(0, beta_matrix[d][2]*self.max_d)) * (1+MAX_UPDATE_RATIO)\n","        allowed_beta_minimum = (beta_matrix[d][0] + min(0, beta_matrix[d][1]*self.max_f) + min(0, beta_matrix[d][2]*self.max_d)) * (1-MAX_UPDATE_RATIO)\n","        allowed_beta_maximum = min(allowed_beta_maximum, 1)\n","        allowed_beta_minimum = max(MINIMUM_ALPHA, allowed_beta_minimum)\n","\n","        for ind, derivative in zip(range(len(param_derivative_mapping)), param_derivative_mapping):\n","            beta_matrix[d][ind] += beta_update * derivative\n","            alpha_matrix[d][ind] += alpha_update * derivative\n","\n","        # restrict param for beta\n","        beta_matrix[d][0] = max(allowed_beta_minimum, min(allowed_beta_maximum, beta_matrix[d][0]))\n","\n","        if beta_matrix[d][0] + self.max_f * beta_matrix[d][1] > allowed_beta_maximum:\n","            beta_matrix[d][1] = (allowed_beta_maximum - beta_matrix[d][0]) / self.max_f\n","        elif beta_matrix[d][0] + self.max_f * beta_matrix[d][1] < allowed_beta_minimum:\n","            beta_matrix[d][1] = (allowed_beta_minimum-beta_matrix[d][0]) / self.max_f\n","\n","        if beta_matrix[d][0] + max(self.max_f * beta_matrix[d][1], 0) + beta_matrix[d][2]*self.max_d > allowed_beta_maximum:\n","            beta_matrix[d][2] = (allowed_beta_maximum - beta_matrix[d][0] - max(self.max_f * beta_matrix[d][1], 0)) / self.max_d\n","        elif beta_matrix[d][0] + min(self.max_f * beta_matrix[d][1], 0) + beta_matrix[d][2]*self.max_d < allowed_beta_minimum:\n","            beta_matrix[d][2] = (allowed_beta_minimum - beta_matrix[d][0] - min(self.max_f * beta_matrix[d][1], 0)) / self.max_d\n","\n","        # restrict param for alpha\n","        alpha_matrix[d][0] = max(allowed_alpha_minimum, min(allowed_alpha_maximum, alpha_matrix[d][0]))\n","\n","        if alpha_matrix[d][0] + self.max_f * alpha_matrix[d][1] < allowed_alpha_minimum:\n","            alpha_matrix[d][1] = (allowed_alpha_minimum-alpha_matrix[d][0]) / self.max_f\n","        elif alpha_matrix[d][0] + self.max_f * alpha_matrix[d][1] > allowed_alpha_maximum:\n","            alpha_matrix[d][1] = (allowed_alpha_maximum-alpha_matrix[d][0]) / self.max_f\n","\n","        if alpha_matrix[d][0] + max(self.max_f * alpha_matrix[d][1], 0) + alpha_matrix[d][2]*self.max_d > allowed_alpha_maximum:\n","            alpha_matrix[d][2] = (allowed_alpha_maximum - alpha_matrix[d][0] - max(self.max_f * alpha_matrix[d][1], 0)) / self.max_d\n","        elif alpha_matrix[d][0] + min(self.max_f * alpha_matrix[d][1], 0) + alpha_matrix[d][2]*self.max_d < allowed_alpha_minimum:\n","            alpha_matrix[d][2] = (allowed_alpha_minimum - alpha_matrix[d][0] - min(self.max_f * alpha_matrix[d][1], 0)) / self.max_d\n","\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix):\n","        n = len(prefix)\n","        if n > self.max_d:\n","            n = self.max_d\n","\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        if n < self.d_split:\n","            d = 0\n","        else:\n","            d = 1\n","\n","        param_derivative_mapping = [1, f, n]\n","\n","        alpha = alpha_matrix[d][0] + f * alpha_matrix[d][1] + n * alpha_matrix[d][2]\n","        beta = beta_matrix[d][0] + f* beta_matrix[d][1] + n * beta_matrix[d][2]\n","\n","        return (alpha, beta)\n","\n"]},{"cell_type":"code","execution_count":105,"id":"dpodGPuObaPL","metadata":{"cellView":"form","executionInfo":{"elapsed":3,"status":"ok","timestamp":1707916660328,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"dpodGPuObaPL"},"outputs":[],"source":["#@title Model: linear v2 with piecewise linear on f\n","\n","class Blending_with_linear_param_v2_piecewise_f(Blending_with_linear_param):\n","    max_f = 0\n","    f_split = 0\n","\n","    def __init__(self, max_f, max_d, f_split, learning_rate):\n","        self.f_split = f_split\n","        super().__init__(max_f, max_d, learning_rate)\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x):\n","        learning_rate = self.learning_rate\n","        n = len(prefix)\n","        # add a computation for alpha beta\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        if f < self.f_split:\n","            d = 0\n","        else:\n","            d = 1\n","\n","        param_derivative_mapping = [1, f, n]\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","\n","        prob = self.blend(alpha_matrix, beta_matrix, prefix, x)\n","        if prob == 0:\n","            return\n","\n","        # new\n","        derivative_alpha = 1 / prob * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, prefix, x)\n","        derivative_beta = 1 / prob * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, prefix, x)\n","        beta_update = learning_rate * derivative_beta\n","        alpha_update = learning_rate * derivative_alpha\n","        factor = 1\n","\n","        allowed_alpha_maximum = (alpha_matrix[d][0] + max(0, alpha_matrix[d][1]*self.max_f) + max(0, alpha_matrix[d][2]*self.max_d)) * (1+MAX_UPDATE_RATIO)\n","        allowed_alpha_minimum = (alpha_matrix[d][0] + min(0, alpha_matrix[d][1]*self.max_f) + min(0, alpha_matrix[d][2]*self.max_d)) * (1-MAX_UPDATE_RATIO)\n","        allowed_alpha_minimum = max(MINIMUM_ALPHA, allowed_alpha_minimum)\n","\n","        allowed_beta_maximum = (beta_matrix[d][0] + max(0, beta_matrix[d][1]*self.max_f) + max(0, beta_matrix[d][2]*self.max_d)) * (1+MAX_UPDATE_RATIO)\n","        allowed_beta_minimum = (beta_matrix[d][0] + min(0, beta_matrix[d][1]*self.max_f) + min(0, beta_matrix[d][2]*self.max_d)) * (1-MAX_UPDATE_RATIO)\n","        allowed_beta_maximum = min(allowed_beta_maximum, 1)\n","        allowed_beta_minimum = max(MINIMUM_ALPHA, allowed_beta_minimum)\n","\n","        for ind, derivative in zip(range(len(param_derivative_mapping)), param_derivative_mapping):\n","            beta_matrix[d][ind] += beta_update * derivative\n","            alpha_matrix[d][ind] += alpha_update * derivative\n","\n","        # restrict param for beta\n","        beta_matrix[d][0] = max(allowed_beta_minimum, min(allowed_beta_maximum, beta_matrix[d][0]))\n","\n","        if beta_matrix[d][0] + self.max_f * beta_matrix[d][1] > allowed_beta_maximum:\n","            beta_matrix[d][1] = (allowed_beta_maximum - beta_matrix[d][0]) / self.max_f\n","        elif beta_matrix[d][0] + self.max_f * beta_matrix[d][1] < allowed_beta_minimum:\n","            beta_matrix[d][1] = (allowed_beta_minimum-beta_matrix[d][0]) / self.max_f\n","\n","        if beta_matrix[d][0] + max(self.max_f * beta_matrix[d][1], 0) + beta_matrix[d][2]*self.max_d > allowed_beta_maximum:\n","            beta_matrix[d][2] = (allowed_beta_maximum - beta_matrix[d][0] - max(self.max_f * beta_matrix[d][1], 0)) / self.max_d\n","        elif beta_matrix[d][0] + min(self.max_f * beta_matrix[d][1], 0) + beta_matrix[d][2]*self.max_d < allowed_beta_minimum:\n","            beta_matrix[d][2] = (allowed_beta_minimum - beta_matrix[d][0] - min(self.max_f * beta_matrix[d][1], 0)) / self.max_d\n","\n","        # restrict param for alpha\n","        alpha_matrix[d][0] = max(allowed_alpha_minimum, min(allowed_alpha_maximum, alpha_matrix[d][0]))\n","\n","        if alpha_matrix[d][0] + self.max_f * alpha_matrix[d][1] < allowed_alpha_minimum:\n","            alpha_matrix[d][1] = (allowed_alpha_minimum-alpha_matrix[d][0]) / self.max_f\n","        elif alpha_matrix[d][0] + self.max_f * alpha_matrix[d][1] > allowed_alpha_maximum:\n","            alpha_matrix[d][1] = (allowed_alpha_maximum-alpha_matrix[d][0]) / self.max_f\n","\n","        if alpha_matrix[d][0] + max(self.max_f * alpha_matrix[d][1], 0) + alpha_matrix[d][2]*self.max_d > allowed_alpha_maximum:\n","            alpha_matrix[d][2] = (allowed_alpha_maximum - alpha_matrix[d][0] - max(self.max_f * alpha_matrix[d][1], 0)) / self.max_d\n","        elif alpha_matrix[d][0] + min(self.max_f * alpha_matrix[d][1], 0) + alpha_matrix[d][2]*self.max_d < allowed_alpha_minimum:\n","            alpha_matrix[d][2] = (allowed_alpha_minimum - alpha_matrix[d][0] - min(self.max_f * alpha_matrix[d][1], 0)) / self.max_d\n","\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix):\n","        n = len(prefix)\n","        if n > self.max_d:\n","            n = self.max_d\n","\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        if f < self.f_split:\n","            d = 0\n","        else:\n","            d = 1\n","\n","        param_derivative_mapping = [1, f, n]\n","        # alpha1 = np.dot(alpha_matrix[n], param_derivative_mapping)\n","        # beta1 = np.dot(beta_matrix[n], param_derivative_mapping)\n","\n","        alpha = alpha_matrix[d][0] + f * alpha_matrix[d][1] + n * alpha_matrix[d][2]\n","        beta = beta_matrix[d][0] + f* beta_matrix[d][1] + n * beta_matrix[d][2]\n","\n","        #beta = max(0, min(1, beta))\n","        # alpha = max(-beta, alpha)\n","\n","        return (alpha, beta)\n","\n"]},{"cell_type":"code","execution_count":106,"id":"UtUL5cClUbEY","metadata":{"cellView":"form","executionInfo":{"elapsed":2,"status":"ok","timestamp":1707916660328,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"UtUL5cClUbEY"},"outputs":[],"source":["#@title Model: trainer with 1 alpha and beta for all calculation\n","\n","class Blending_with_one_alpha_beta(Blending_with_linear_param):\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x):\n","        learning_rate = self.learning_rate\n","        d = len(prefix)\n","        n = d\n","        d = 0\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","\n","        prob = self.blend(alpha_matrix, beta_matrix, prefix, x)\n","        # if prob == 0:\n","        #     return\n","\n","        # update\n","        derivative_beta = 1 / prob * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, prefix, x)\n","        beta_update = learning_rate * derivative_beta\n","        derivative_alpha = 1 / prob * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, prefix, x)\n","        alpha_update = learning_rate * derivative_alpha\n","\n","        # print(\"old\", alpha_matrix[d][f], beta_matrix[d][f])\n","        # if n == 5:\n","        #     print(\"values:\", alpha, beta, prob, alpha_update, beta_update, f)\n","\n","        # compute factor need to be changed\n","        factor_alpha = max(abs(alpha_update) / (alpha_matrix[0][0] * MAX_UPDATE_RATIO), 1)\n","        factor_beta = max(abs(beta_update) / (beta_matrix[0][0] * MAX_UPDATE_RATIO), 1)\n","\n","        factor = max(factor_alpha, factor_beta)\n","\n","\n","\n","        beta_matrix[0][0] += beta_update / factor\n","        alpha_matrix[0][0] += alpha_update / factor\n","\n","        beta_matrix[0][0] = max(MINIMUM_ALPHA, min(1, beta_matrix[0][0]))\n","        alpha_matrix[0][0] = max(MINIMUM_ALPHA, alpha_matrix[0][0])\n","\n","\n","        # print(\"new\", alpha_matrix[d][f], beta_matrix[d][f])\n","\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix):\n","\n","        return (alpha_matrix[0][0], beta_matrix[0][0])"]},{"cell_type":"code","execution_count":107,"id":"9b9e8a7a","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1707916660328,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"9b9e8a7a","cellView":"form"},"outputs":[],"source":["#@title Model: trainer with 2d matrix for d and f\n","\n","class Blending_with_2d_matrix(Blending_with_linear_param):\n","    alpha_trend = []\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x):\n","        learning_rate = self.learning_rate\n","        d = len(prefix)\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        if d > self.max_d:\n","            d = self.max_d\n","\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","\n","        prob = self.blend(alpha_matrix, beta_matrix, prefix, x)\n","        # if prob == 0:\n","        #     return\n","\n","        # update\n","        derivative_beta = 1 / prob * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, prefix, x)\n","        beta_update = learning_rate * derivative_beta\n","        derivative_alpha = 1 / prob * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, prefix, x)\n","        alpha_update = learning_rate * derivative_alpha\n","        # print(alpha_update)\n","\n","        # compute factor need to be changed\n","        factor_alpha = max(abs(alpha_update) / (alpha_matrix[d][f] * MAX_UPDATE_RATIO), 1)\n","        factor_beta = max(abs(beta_update) / (beta_matrix[d][f] * MAX_UPDATE_RATIO), 1)\n","        # factor = max(factor_alpha, factor_beta)\n","\n","        beta_matrix[d][f] += beta_update / factor_beta\n","        alpha_matrix[d][f] += alpha_update / factor_alpha\n","\n","        beta_matrix[d][f] = max(MINIMUM_ALPHA, min(1, beta_matrix[d][f]))\n","        alpha_matrix[d][f] = max(MINIMUM_ALPHA, alpha_matrix[d][f])\n","        # print(\"new\", alpha_matrix[d][f], beta_matrix[d][f])\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix):\n","        d = len(prefix)\n","        if d > self.max_d:\n","            d = self.max_d\n","\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        return (alpha_matrix[d][f], beta_matrix[d][f])"]},{"cell_type":"code","execution_count":108,"id":"p8Kij4aaokiM","metadata":{"cellView":"form","executionInfo":{"elapsed":2,"status":"ok","timestamp":1707916660328,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"p8Kij4aaokiM"},"outputs":[],"source":["#@title Model: trainer with 2d matrix for d and m\n","\n","class Blending_with_2d_matrix_v2(Blending_with_linear_param):\n","    max_m = 0\n","\n","    def __init__(self, max_f, max_d, max_m, learning_rate):\n","        self.max_m = max_m\n","        super().__init__(max_f, max_d, learning_rate)\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x):\n","        learning_rate = self.learning_rate\n","        d = len(prefix)\n","        if prefix in self.n_gram_model:\n","            m = sum(self.n_gram_model[prefix].values())\n","        else:\n","            m = 0\n","\n","        if m > self.max_m:\n","            m = self.max_m\n","\n","        if d > self.max_d:\n","            d = self.max_d\n","\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","\n","        prob = self.blend(alpha_matrix, beta_matrix, prefix, x)\n","        # if prob == 0:\n","        #     return\n","\n","        # update\n","        derivative_beta = 1 / prob * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, prefix, x)\n","        beta_update = learning_rate * derivative_beta\n","        derivative_alpha = 1 / prob * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, prefix, x)\n","        alpha_update = learning_rate * derivative_alpha\n","\n","        # print(\"old\", alpha_matrix[d][f], beta_matrix[d][f])\n","        # if n == 5:\n","        #     print(\"values:\", alpha, beta, prob, alpha_update, beta_update, f)\n","\n","        # compute factor need to be changed\n","        factor_alpha = max(abs(alpha_update) / (alpha_matrix[d][m] * MAX_UPDATE_RATIO), 1)\n","        factor_beta = max(abs(beta_update) / (beta_matrix[d][m] * MAX_UPDATE_RATIO), 1)\n","\n","        factor = max(factor_alpha, factor_beta)\n","\n","\n","\n","        beta_matrix[d][m] += beta_update / factor\n","        alpha_matrix[d][m] += alpha_update / factor\n","\n","        beta_matrix[d][m] = max(MINIMUM_ALPHA, min(1, beta_matrix[d][m]))\n","        alpha_matrix[d][m] = max(MINIMUM_ALPHA, alpha_matrix[d][m])\n","\n","\n","        # print(\"new\", alpha_matrix[d][f], beta_matrix[d][f])\n","\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix):\n","        d = len(prefix)\n","        if d > self.max_d:\n","            d = self.max_d\n","\n","        if prefix in self.n_gram_model:\n","            m = sum(self.n_gram_model[prefix].values())\n","        else:\n","            m = 0\n","\n","        if m > self.max_m:\n","            m = self.max_m\n","\n","        return (alpha_matrix[d][m], beta_matrix[d][m])"]},{"cell_type":"code","execution_count":109,"id":"H7PRzkrFba8X","metadata":{"cellView":"form","executionInfo":{"elapsed":438,"status":"ok","timestamp":1707916660764,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"H7PRzkrFba8X"},"outputs":[],"source":["#@title Model: trainer with 3d matrix for d and f and m\n","\n","class Blending_with_2d_matrix_v3(Blending_with_linear_param):\n","    max_m = 0\n","\n","    def __init__(self, max_f, max_d, max_m, learning_rate):\n","        self.max_m = max_m\n","        super().__init__(max_f, max_d, learning_rate)\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x):\n","        learning_rate = self.learning_rate\n","        d = len(prefix)\n","\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        if prefix in self.n_gram_model:\n","            m = sum(self.n_gram_model[prefix].values())\n","        else:\n","            m = 0\n","\n","        if m > self.max_m:\n","            m = self.max_m\n","\n","        if d > self.max_d:\n","            d = self.max_d\n","\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","\n","        prob = self.blend(alpha_matrix, beta_matrix, prefix, x)\n","        # if prob == 0:\n","        #     return\n","\n","        # update\n","        derivative_beta = 1 / prob * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, prefix, x)\n","        beta_update = learning_rate * derivative_beta\n","        derivative_alpha = 1 / prob * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, prefix, x)\n","        alpha_update = learning_rate * derivative_alpha\n","\n","        # print(\"old\", alpha_matrix[d][f], beta_matrix[d][f])\n","        # if n == 5:\n","        #     print(\"values:\", alpha, beta, prob, alpha_update, beta_update, f)\n","\n","        # compute factor need to be changed\n","        factor_alpha = max(abs(alpha_update) / (alpha_matrix[d][f][m] * MAX_UPDATE_RATIO), 1)\n","        factor_beta = max(abs(beta_update) / (beta_matrix[d][f][m] * MAX_UPDATE_RATIO), 1)\n","\n","        factor = max(factor_alpha, factor_beta)\n","\n","\n","\n","        beta_matrix[d][f][m] += beta_update / factor\n","        alpha_matrix[d][f][m] += alpha_update / factor\n","\n","        beta_matrix[d][f][m] = max(MINIMUM_ALPHA, min(1, beta_matrix[d][f][m]))\n","        alpha_matrix[d][f][m] = max(MINIMUM_ALPHA, alpha_matrix[d][f][m])\n","\n","\n","        # print(\"new\", alpha_matrix[d][f], beta_matrix[d][f])\n","\n","\n","\n","    def compute_alpha_beta(self, alpha_matrix, beta_matrix, prefix):\n","        d = len(prefix)\n","        if d > self.max_d:\n","            d = self.max_d\n","\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        if prefix in self.n_gram_model:\n","            m = sum(self.n_gram_model[prefix].values())\n","        else:\n","            m = 0\n","\n","        if m > self.max_m:\n","            m = self.max_m\n","\n","        return (alpha_matrix[d][f][m], beta_matrix[d][f][m])"]},{"cell_type":"code","execution_count":110,"id":"bbf198df","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1707916660765,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"bbf198df","cellView":"form"},"outputs":[],"source":["#@title Model: constant alpha and beta\n","\n","import matplotlib.pyplot as plt\n","\n","# Your list of numbers\n","def plot_score(data):\n","    plt.clf()\n","    # Plotting the list of numbers\n","    plt.plot(data)\n","\n","    # Adding labels to the plot\n","    plt.xlabel('Number of words')\n","    plt.ylabel('Average Negative log probability')\n","    plt.title('Average Negative log probability change')\n","\n","    # Display the plot\n","    plt.show()\n","\n","class Blending_with_no_training(Blending_with_2d_matrix):\n","    logs = []\n","\n","    def update_param(self, alpha_matrix, beta_matrix, prefix, x):\n","        learning_rate = self.learning_rate\n","        d = len(prefix)\n","        n = d\n","        if prefix in self.n_gram_model:\n","            f = len(self.n_gram_model[prefix])\n","        else:\n","            f = 0\n","\n","        if f > self.max_f:\n","            f = self.max_f\n","\n","        alpha, beta = self.compute_alpha_beta(alpha_matrix, beta_matrix, prefix)\n","\n","        prob = self.blend(alpha_matrix, beta_matrix, prefix, x)\n","        # if prob == 0:\n","        #     return\n","\n","        # update\n","        derivative_beta = 1 / prob * self.derivative_of_P_to_beta(alpha_matrix, beta_matrix, prefix, x)\n","        beta_update = learning_rate * derivative_beta\n","        derivative_alpha = 1 / prob * self.derivative_of_P_to_alpha(alpha_matrix, beta_matrix, prefix, x)\n","        alpha_update = learning_rate * derivative_alpha\n","\n","        beta_matrix[d][f] += beta_update\n","        alpha_matrix[d][f] += alpha_update\n","\n","        beta_matrix[d][f] = max(MINIMUM_ALPHA, min(1, beta_matrix[d][f]))\n","        alpha_matrix[d][f] = max(MINIMUM_ALPHA, alpha_matrix[d][f])\n","\n","\n","        # print(\"new\", alpha_matrix[d][f], beta_matrix[d][f])\n","\n","    def build_model_with_train(self, data, alpha_matrix, beta_matrix, n):\n","        # read in all n+1 grams\n","        word_org = data\n","        # update n-gram\n","        for i in range(len(data) - n):\n","            n_plus_1_gram = data[i:i + n + 1]\n","            score = self.letter_prob_score (alpha_matrix, beta_matrix, n_plus_1_gram[:-1], n_plus_1_gram[-1], n)\n","            self.total_score += score\n","            self.count += 1\n","            self.logs.append(-self.total_score/self.count)\n","            self.logs_score_individual.append(score)\n","\n","            n_plus_1_gram = data[i:i + n + 1]\n","            for N in range(n, -1, -1):\n","                x = n_plus_1_gram[-1]\n","                next_n = n_plus_1_gram[:-1]\n","\n","                if next_n not in self.n_gram_model:\n","                    self.n_gram_model[next_n] = {}\n","\n","                if x not in self.n_gram_model[next_n]:\n","                    self.n_gram_model[next_n][x] = 1\n","                else:\n","                    self.n_gram_model[next_n][x] += 1\n","\n","                n_plus_1_gram = n_plus_1_gram[1:]\n","\n","\n","\n","\n","        return self.n_gram_model"]},{"cell_type":"code","source":["MAX_F = 5\n","MAX_M = 100\n","learning_rate = 0.003\n","\n","n = 1\n","# data = [alice]\n","\n","# data = [''.join([\"aba\"+chr(i) for i in range(32, 127)])]\n","data = ''.join([\"aba\" + chr(i) for i in range(32, 127)])\n","\n","alpha_matrix = [[1 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","alpha_matrix_untrained = [[1 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","trainer_untrained = Blending_with_no_training(MAX_F, n, learning_rate)\n","trainer_untrained.build_model_with_train(data, alpha_matrix_untrained, beta_matrix_untrained, n)\n","print(f\"constant value avg score: {-trainer_untrained.total_score/trainer_untrained.count}\")\n","# plot_score(trainer_untrained.logs_score_individual)\n","\n","for i in range(10):\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F, n, learning_rate)\n","    trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix(2d matrix for d and f) avg score: {-trainer_matrix.total_score/trainer_matrix.count}\")\n","    # plot_score(trainer_matrix.logs_score_individual)\n","\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ri1HJ7FQNBlY","executionInfo":{"status":"ok","timestamp":1707916691929,"user_tz":-660,"elapsed":546,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"21afa5a4-be64-4af4-91e7-7817107442a4"},"id":"ri1HJ7FQNBlY","execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["constant value avg score: 5.397209407092712\n","training end, matrix(2d matrix for d and f) avg score: 6.774011031372941\n","training end, matrix(2d matrix for d and f) avg score: 6.76901220180939\n","training end, matrix(2d matrix for d and f) avg score: 6.763112282524745\n","training end, matrix(2d matrix for d and f) avg score: 6.7580587025013585\n","training end, matrix(2d matrix for d and f) avg score: 6.754559451697848\n","training end, matrix(2d matrix for d and f) avg score: 6.74623031052489\n","training end, matrix(2d matrix for d and f) avg score: 6.737636815613202\n","training end, matrix(2d matrix for d and f) avg score: 6.728959483226247\n","training end, matrix(2d matrix for d and f) avg score: 6.720542395299666\n","training end, matrix(2d matrix for d and f) avg score: 6.712383240798072\n"]}]},{"cell_type":"code","execution_count":null,"id":"721ad0ff","metadata":{"cellView":"form","id":"721ad0ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707805675234,"user_tz":-660,"elapsed":574,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"566c61f8-4489-4504-9424-a2091ecd2ec2"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.9729503305033382\n","0.9729503305033382\n","0.9729503305033382\n","0.9729503305033382\n","0.9729503305033382\n","0.9286857116215051\n"]}],"source":["#@title verify\n","MAX_F = 15\n","n = 5\n","MAX_N = n\n","\n","# load data\n","data = [alice[:1000]]\n","\n","\n","# alpha_matrix = [[0.5, 0, 0] for _ in range(n+1)]\n","# beta_matrix = [[0.75, 0, 0] for _ in range(n+1)]\n","\n","alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","trainer_1 = Blending_with_2d_matrix(MAX_F, MAX_N, 0.003)\n","n_gram_model_1 = trainer_1.build_model_with_train(data, alpha_matrix, beta_matrix, \"words_manual_en.txt\", n)\n","\n","# verify\n","cand = [chr(i) for i in range(ord('A'), ord('Z') + 1)] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\"$\"]\n","\n","# alpha_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n","# beta_matrix = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n","def verify(trainer, prefix, model):\n","    s = 0\n","    for c in cand:\n","        prob = trainer.blend(alpha_matrix, beta_matrix, prefix, c)\n","        assert(prob > 0 and prob <=1)\n","        s += prob\n","\n","    return s\n","\n","print(verify(trainer_1, \"ralvc\", n_gram_model_1))\n","print(verify(trainer_1, \"^allc\", n_gram_model_1))\n","print(verify(trainer_1, \"trumc\", n_gram_model_1))\n","print(verify(trainer_1, \"russc\", n_gram_model_1))\n","print(verify(trainer_1, \"kochc\", n_gram_model_1))\n","print(verify(trainer_1, \"racka\", n_gram_model_1))"]},{"cell_type":"code","execution_count":114,"id":"WCWG_o1NRR5Q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":557078,"status":"ok","timestamp":1707917478885,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"},"user_tz":-660},"id":"WCWG_o1NRR5Q","outputId":"6e5dda49-b178-479e-f00e-d6b33cdbef7b","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["training end, matrix(2d matrix for d and f) with offline optimisation avg score: 2.288849608707172\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.235479155450066\n"]}],"source":["#@title Sample Usage: try compression on alice\n","\n","# Hyper-parameter settings\n","MAX_F = 11\n","MAX_M = 100\n","n = 8\n","MAX_N = n\n","learning_rate = 0.003\n","\n","# specify the data\n","data = alice\n","\n","alpha_matrix = [[10 for _ in range(MAX_F+1)] for _ in range(MAX_N+1)]\n","beta_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(MAX_N+1)]\n","\n","trainer_matrix = Blending_with_2d_matrix(MAX_F, MAX_N, learning_rate)\n","trainer_matrix.alpha_trend = []\n","n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","print(f\"training end, matrix(2d matrix for d and f) with offline optimisation avg score: {-trainer_matrix.total_score/trainer_matrix.count}\")\n","# plot_score(trainer_matrix.logs)\n","\n","alpha_matrix_linear = [[10, 0] for _ in range(MAX_N+1)]\n","beta_matrix_linear = [[0.75, 0] for _ in range(MAX_N+1)]\n","\n","trainer_linear = Blending_with_linear_param(MAX_F, MAX_N, learning_rate)\n","n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","print(f\"training end, linear(alpha = p0d + p1d * f) avg score: {-trainer_linear.total_score/trainer_linear.count}\")\n","# plot_score(trainer_linear.logs)"]},{"cell_type":"code","execution_count":null,"id":"37bba6b3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"37bba6b3","executionInfo":{"status":"ok","timestamp":1707830828989,"user_tz":-660,"elapsed":15991092,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"19a2213d-d779-40e1-baa4-bfc9125a1b98","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["runing on text files\n","runing on file: alice29\n","\n","training end, one param(1 alpha and beta for all calculation) avg score: 3.303965884432459\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.227033392267877\n","training end, linear v2(alpha = p0 + p1 * d + p2 * f) avg score: 2.256471944380474\n","training end, linear v2 piecewise on d, first one row(alpha = p0 + p1 * d + p2 * f) avg score: 2.6849955546593853\n","training end, linear v2 piecewise on d, first two rows(alpha = p0 + p1 * d + p2 * f) avg score: 2.416043431148422\n","training end, linear v2 piecewise on f, first one column(alpha = p0 + p1 * d + p2 * f) avg score: 2.253525644750256\n","training end, matrix(2d matrix for d and f) avg score: 2.5035575760369553\n","training end, matrix v2(2d matrix for d and m) avg score: 3.6406403529788056\n","training end, matrix v3(3d matrix for d, f, m) avg score: 3.4630906129258054\n","constant value avg score: 2.4666009182437447\n","\n","runing on file: lcet10\n","\n","training end, one param(1 alpha and beta for all calculation) avg score: 3.006706435189737\n","training end, linear(alpha = p0d + p1d * f) avg score: 1.9622703519195754\n","training end, linear v2(alpha = p0 + p1 * d + p2 * f) avg score: 1.9844434883991082\n","training end, linear v2 piecewise on d, first one row(alpha = p0 + p1 * d + p2 * f) avg score: 2.833476200105526\n","training end, linear v2 piecewise on d, first two rows(alpha = p0 + p1 * d + p2 * f) avg score: 2.405274946967259\n","training end, linear v2 piecewise on f, first one column(alpha = p0 + p1 * d + p2 * f) avg score: 1.916241851146813\n","training end, matrix(2d matrix for d and f) avg score: 2.1336657846731435\n","training end, matrix v2(2d matrix for d and m) avg score: 3.2414546408812592\n","training end, matrix v3(3d matrix for d, f, m) avg score: 3.1416128761568083\n","constant value avg score: 2.1847498471787747\n","\n","runing on file: plrabn12\n","\n","training end, one param(1 alpha and beta for all calculation) avg score: 3.5574348701254577\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.3990706561600557\n","training end, linear v2(alpha = p0 + p1 * d + p2 * f) avg score: 2.29375007877067\n","training end, linear v2 piecewise on d, first one row(alpha = p0 + p1 * d + p2 * f) avg score: 2.884494616973269\n","training end, linear v2 piecewise on d, first two rows(alpha = p0 + p1 * d + p2 * f) avg score: 2.5071854708576917\n","training end, linear v2 piecewise on f, first one column(alpha = p0 + p1 * d + p2 * f) avg score: 2.289346040216174\n","training end, matrix(2d matrix for d and f) avg score: 2.5638346185992984\n","training end, matrix v2(2d matrix for d and m) avg score: 3.9733878735184724\n","training end, matrix v3(3d matrix for d, f, m) avg score: 3.8862084730665343\n","constant value avg score: 2.6100103286076575\n","\n","runing on file: asyoulik\n","\n","training end, one param(1 alpha and beta for all calculation) avg score: 3.6311277240689703\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.4144669015447806\n","training end, linear v2(alpha = p0 + p1 * d + p2 * f) avg score: 2.3961611985063223\n","training end, linear v2 piecewise on d, first one row(alpha = p0 + p1 * d + p2 * f) avg score: 3.2245471557823087\n","training end, linear v2 piecewise on d, first two rows(alpha = p0 + p1 * d + p2 * f) avg score: 2.7285384630931677\n","training end, linear v2 piecewise on f, first one column(alpha = p0 + p1 * d + p2 * f) avg score: 2.3887594670195615\n","training end, matrix(2d matrix for d and f) avg score: 2.8223389692824115\n","training end, matrix v2(2d matrix for d and m) avg score: 4.010041030956093\n","training end, matrix v3(3d matrix for d, f, m) avg score: 3.826454699564182\n","constant value avg score: 2.670741987111461\n","\n"]}],"source":["#@title Sample Usage: words from different texts\n","\n","import random\n","MAX_F = 11\n","MAX_M = 100\n","n = 8\n","MAX_D = 7\n","learning_rate = 0.003\n","\n","print(\"runing on text files\")\n","\n","texts = {\"alice29\": alice, \"lcet10\": lcet, \"plrabn12\": plrabn12, \"asyoulik\": asyoulik}\n","# load data\n","for id in texts:\n","\n","    data = texts[id]\n","    print(f\"runing on file: {id}\")\n","    print()\n","\n","    alpha_matrix_linear = [[0.5]]\n","    beta_matrix_linear = [[0.75]]\n","\n","    trainer_one_param = Blending_with_one_alpha_beta(MAX_F, MAX_D, learning_rate)\n","    n_gram_model_linear = trainer_one_param.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, one param(1 alpha and beta for all calculation) avg score: {-trainer_one_param.total_score/trainer_one_param.count}\")\n","    # plot_score(trainer_one_param.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(MAX_D+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(MAX_D+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F, MAX_D, learning_rate)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear(alpha = p0d + p1d * f) avg score: {-trainer_linear.total_score/trainer_linear.count}\")\n","    # plot_score(trainer_linear.logs)\n","\n","    alpha_matrix_linear = [0.5, 0, 0]\n","    beta_matrix_linear = [0.75, 0, 0]\n","\n","    trainer_linear_v2 = Blending_with_linear_param_v2(MAX_F, MAX_D, learning_rate)\n","    n_gram_model_linear = trainer_linear_v2.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2.total_score/trainer_linear_v2.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0, 0] for _ in range(2)]\n","    beta_matrix_linear = [[0.75, 0, 0] for _ in range(2)]\n","\n","    trainer_linear_v2_piecewise = Blending_with_linear_param_v2_piecewise_d(MAX_F, MAX_D, 1, learning_rate)\n","    n_gram_model_linear = trainer_linear_v2_piecewise.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2 piecewise on d, first one row(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2_piecewise.total_score/trainer_linear_v2_piecewise.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0, 0] for _ in range(2)]\n","    beta_matrix_linear = [[0.75, 0, 0] for _ in range(2)]\n","\n","    trainer_linear_v2_piecewise = Blending_with_linear_param_v2_piecewise_d(MAX_F, MAX_D, 2, learning_rate)\n","    n_gram_model_linear = trainer_linear_v2_piecewise.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2 piecewise on d, first two rows(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2_piecewise.total_score/trainer_linear_v2_piecewise.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0, 0] for _ in range(2)]\n","    beta_matrix_linear = [[0.75, 0, 0] for _ in range(2)]\n","\n","    trainer_linear_v2_piecewise = Blending_with_linear_param_v2_piecewise_f(MAX_F, MAX_D, 1, learning_rate)\n","    n_gram_model_linear = trainer_linear_v2_piecewise.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2 piecewise on f, first one column(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2_piecewise.total_score/trainer_linear_v2_piecewise.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F, MAX_D, learning_rate)\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix(2d matrix for d and f) avg score: {-trainer_matrix.total_score/trainer_matrix.count}\")\n","    # plot_score(trainer_matrix.logs)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_M+1)] for _ in range(MAX_D+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_M+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_matrix_v2 = Blending_with_2d_matrix_v2(MAX_F, MAX_D, MAX_M, learning_rate)\n","    n_gram_model_matrix = trainer_matrix_v2.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix v2(2d matrix for d and m) avg score: {-trainer_matrix_v2.total_score/trainer_matrix_v2.count}\")\n","    # plot_score(trainer_matrix_v2.logs)\n","\n","    alpha_matrix = [[[0.5 for _ in range(MAX_M+1)] for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","    beta_matrix = [[[0.75 for _ in range(MAX_M+1)] for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_matrix_v3 = Blending_with_2d_matrix_v3(MAX_F, MAX_D, MAX_M, learning_rate)\n","    n_gram_model_matrix = trainer_matrix_v3.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix v3(3d matrix for d, f, m) avg score: {-trainer_matrix_v3.total_score/trainer_matrix_v3.count}\")\n","    # plot_score(trainer_matrix_v3.logs)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_untrained = Blending_with_no_training(MAX_F, MAX_D, learning_rate)\n","    n_gram_model_untrained = trainer_untrained.build_model_with_train(data, alpha_matrix_untrained, beta_matrix_untrained, n)\n","    print(f\"constant value avg score: {-trainer_untrained.total_score/trainer_untrained.count}\")\n","    # plot_score(trainer_untrained.logs)\n","\n","    print()"]},{"cell_type":"code","source":["#@title try repeat compression on asyoulik\n","\n","MAX_F = 11\n","MAX_M = 100\n","n = 8\n","learning_rate = 0.03\n","\n","data = [asyoulik]\n","\n","alpha_matrix = [[10 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","beta_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(n+1)]\n","\n","alpha_matrix_linear = [[10, 0] for _ in range(n+1)]\n","beta_matrix_linear = [[0.75, 0] for _ in range(n+1)]\n","# plot_score(trainer_matrix.logs)\n","for i in range(5):\n","    print(\"iter\", i)\n","\n","    data = alice\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F, learning_rate)\n","    trainer_matrix.alpha_trend = []\n","    n_gram_model_matrix = trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix(2d matrix for d and f) with offline optimisation avg score: {-trainer_matrix.total_score/trainer_matrix.count}\")\n","    # plot_score(trainer_matrix.logs)\n","\n","\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F, learning_rate)\n","    n_gram_model_linear = trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear(alpha = p0d + p1d * f) avg score: {-trainer_linear.total_score/trainer_linear.count}\")\n","    # plot_score(trainer_linear.logs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_58GyVrcoXJz","executionInfo":{"status":"ok","timestamp":1707311725884,"user_tz":-660,"elapsed":1300612,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"e590c0c0-3332-408e-b4fd-262ad803ac7f","cellView":"form"},"id":"_58GyVrcoXJz","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iter 0\n","training end, matrix(2d matrix for d and f) with offline optimisation avg score: 2.301309730499128\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.2356767724679263\n","iter 1\n","training end, matrix(2d matrix for d and f) with offline optimisation avg score: 2.272196677644992\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.2992846899742245\n","iter 2\n","training end, matrix(2d matrix for d and f) with offline optimisation avg score: 2.257498297575527\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.3479919948761205\n","iter 3\n","training end, matrix(2d matrix for d and f) with offline optimisation avg score: 2.246361617314264\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.3678244814179017\n","iter 4\n","training end, matrix(2d matrix for d and f) with offline optimisation avg score: 2.237116257308087\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.377335595063113\n"]}]},{"cell_type":"code","source":["#@title words from alice, dynamic alpha and beta\n","\n","import random\n","MAX_F = 11\n","MAX_M = 100\n","n = 8\n","MAX_D = 7\n","learning_rate = 0.003\n","\n","print(\"runing on text files\")\n","\n","# texts = {\"alice29\": alice, \"lcet10\": lcet, \"plrabn12\": plrabn12, \"asyoulik\": asyoulik}\n","texts = {\"alice29\": alice}\n","# load data\n","for id in texts:\n","\n","    data = texts[id]\n","    print(f\"runing on file: {id}\")\n","    print()\n","\n","    alpha_matrix_linear = [[0.5]]\n","    beta_matrix_linear = [[0.75]]\n","\n","    trainer_one_param = Blending_with_one_alpha_beta(MAX_F, MAX_D, learning_rate)\n","    trainer_one_param.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, one param(1 alpha and beta for all calculation) avg score: {-trainer_one_param.total_score/trainer_one_param.count}\")\n","    # plot_score(trainer_one_param.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(MAX_D+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(MAX_D+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F, MAX_D, learning_rate)\n","    trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear(alpha = p0d + p1d * f) avg score: {-trainer_linear.total_score/trainer_linear.count}\")\n","    # plot_score(trainer_linear.logs)\n","\n","    alpha_matrix_linear = [0.5, 0, 0]\n","    beta_matrix_linear = [0.75, 0, 0]\n","\n","    trainer_linear_v2 = Blending_with_linear_param_v2(MAX_F, MAX_D, learning_rate)\n","    trainer_linear_v2.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2.total_score/trainer_linear_v2.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0, 0] for _ in range(2)]\n","    beta_matrix_linear = [[0.75, 0, 0] for _ in range(2)]\n","\n","    trainer_linear_v2_piecewise = Blending_with_linear_param_v2_piecewise_d(MAX_F, MAX_D, 1, learning_rate)\n","    trainer_linear_v2_piecewise.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2 piecewise on d, first one row(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2_piecewise.total_score/trainer_linear_v2_piecewise.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0, 0] for _ in range(2)]\n","    beta_matrix_linear = [[0.75, 0, 0] for _ in range(2)]\n","\n","    trainer_linear_v2_piecewise = Blending_with_linear_param_v2_piecewise_d(MAX_F, MAX_D, 2, learning_rate)\n","    trainer_linear_v2_piecewise.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2 piecewise on d, first two rows(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2_piecewise.total_score/trainer_linear_v2_piecewise.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0, 0] for _ in range(2)]\n","    beta_matrix_linear = [[0.75, 0, 0] for _ in range(2)]\n","\n","    trainer_linear_v2_piecewise = Blending_with_linear_param_v2_piecewise_f(MAX_F, MAX_D, 1, learning_rate)\n","    trainer_linear_v2_piecewise.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2 piecewise on f, first one column(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2_piecewise.total_score/trainer_linear_v2_piecewise.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F, MAX_D, learning_rate)\n","    trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix(2d matrix for d and f) avg score: {-trainer_matrix.total_score/trainer_matrix.count}\")\n","    # plot_score(trainer_matrix.logs)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_M+1)] for _ in range(MAX_D+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_M+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_matrix_v2 = Blending_with_2d_matrix_v2(MAX_F, MAX_D, MAX_M, learning_rate)\n","    trainer_matrix_v2.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix v2(2d matrix for d and m) avg score: {-trainer_matrix_v2.total_score/trainer_matrix_v2.count}\")\n","    # plot_score(trainer_matrix_v2.logs)\n","\n","    alpha_matrix = [[[0.5 for _ in range(MAX_M+1)] for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","    beta_matrix = [[[0.75 for _ in range(MAX_M+1)] for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_matrix_v3 = Blending_with_2d_matrix_v3(MAX_F, MAX_D, MAX_M, learning_rate)\n","    trainer_matrix_v3.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix v3(3d matrix for d, f, m) avg score: {-trainer_matrix_v3.total_score/trainer_matrix_v3.count}\")\n","    # plot_score(trainer_matrix_v3.logs)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_untrained = Blending_with_no_training(MAX_F, MAX_D, learning_rate)\n","    trainer_untrained.build_model_with_train(data, alpha_matrix_untrained, beta_matrix_untrained, n)\n","    print(f\"constant value avg score: {-trainer_untrained.total_score/trainer_untrained.count}\")\n","    # plot_score(trainer_untrained.logs)\n","\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"r7-2zh1ZKqfo","executionInfo":{"status":"ok","timestamp":1707909396609,"user_tz":-660,"elapsed":2079242,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"0be75ce5-e21a-4b82-9247-a40d275918d0"},"id":"r7-2zh1ZKqfo","execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["runing on text files\n","runing on file: alice29\n","\n","training end, one param(1 alpha and beta for all calculation) avg score: 3.3039662783547805\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.225081718814546\n","training end, linear v2(alpha = p0 + p1 * d + p2 * f) avg score: 2.256384699076594\n","training end, linear v2 piecewise on d, first one row(alpha = p0 + p1 * d + p2 * f) avg score: 2.6834404732503883\n","training end, linear v2 piecewise on d, first two rows(alpha = p0 + p1 * d + p2 * f) avg score: 2.412189822786566\n","training end, linear v2 piecewise on f, first one column(alpha = p0 + p1 * d + p2 * f) avg score: 2.2525296461344833\n","training end, matrix(2d matrix for d and f) avg score: 2.5035696075807228\n","training end, matrix v2(2d matrix for d and m) avg score: 3.640640317950737\n","training end, matrix v3(3d matrix for d, f, m) avg score: 3.4630905820352367\n","constant value avg score: 2.4666009182437447\n","\n"]}]},{"cell_type":"code","source":["#@title words from alice, dynamic alpha and constant beta\n","\n","import random\n","MAX_F = 11\n","MAX_M = 100\n","n = 8\n","MAX_D = 7\n","learning_rate = 0.003\n","\n","print(\"runing on text files\")\n","\n","# texts = {\"alice29\": alice, \"lcet10\": lcet, \"plrabn12\": plrabn12, \"asyoulik\": asyoulik}\n","texts = {\"alice29\": alice}\n","# load data\n","for id in texts:\n","\n","    data = texts[id]\n","    print(f\"runing on file: {id}\")\n","    print()\n","\n","    alpha_matrix_linear = [[0.5]]\n","    beta_matrix_linear = [[0.75]]\n","\n","    trainer_one_param = Blending_with_one_alpha_beta(MAX_F, MAX_D, learning_rate)\n","    trainer_one_param.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, one param(1 alpha and beta for all calculation) avg score: {-trainer_one_param.total_score/trainer_one_param.count}\")\n","    # plot_score(trainer_one_param.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0] for _ in range(MAX_D+1)]\n","    beta_matrix_linear = [[0.75, 0] for _ in range(MAX_D+1)]\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F, MAX_D, learning_rate)\n","    trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear(alpha = p0d + p1d * f) avg score: {-trainer_linear.total_score/trainer_linear.count}\")\n","    # plot_score(trainer_linear.logs)\n","\n","    alpha_matrix_linear = [0.5, 0, 0]\n","    beta_matrix_linear = [0.75, 0, 0]\n","\n","    trainer_linear_v2 = Blending_with_linear_param_v2(MAX_F, MAX_D, learning_rate)\n","    trainer_linear_v2.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2.total_score/trainer_linear_v2.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0, 0] for _ in range(2)]\n","    beta_matrix_linear = [[0.75, 0, 0] for _ in range(2)]\n","\n","    trainer_linear_v2_piecewise = Blending_with_linear_param_v2_piecewise_d(MAX_F, MAX_D, 1, learning_rate)\n","    trainer_linear_v2_piecewise.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2 piecewise on d, first one row(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2_piecewise.total_score/trainer_linear_v2_piecewise.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0, 0] for _ in range(2)]\n","    beta_matrix_linear = [[0.75, 0, 0] for _ in range(2)]\n","\n","    trainer_linear_v2_piecewise = Blending_with_linear_param_v2_piecewise_d(MAX_F, MAX_D, 2, learning_rate)\n","    trainer_linear_v2_piecewise.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2 piecewise on d, first two rows(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2_piecewise.total_score/trainer_linear_v2_piecewise.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix_linear = [[0.5, 0, 0] for _ in range(2)]\n","    beta_matrix_linear = [[0.75, 0, 0] for _ in range(2)]\n","\n","    trainer_linear_v2_piecewise = Blending_with_linear_param_v2_piecewise_f(MAX_F, MAX_D, 1, learning_rate)\n","    trainer_linear_v2_piecewise.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear v2 piecewise on f, first one column(alpha = p0 + p1 * d + p2 * f) avg score: {-trainer_linear_v2_piecewise.total_score/trainer_linear_v2_piecewise.count}\")\n","    # plot_score(trainer_linear_v2.logs)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F, MAX_D, learning_rate)\n","    trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix(2d matrix for d and f) avg score: {-trainer_matrix.total_score/trainer_matrix.count}\")\n","    # plot_score(trainer_matrix.logs)\n","\n","    alpha_matrix = [[0.5 for _ in range(MAX_M+1)] for _ in range(MAX_D+1)]\n","    beta_matrix = [[0.75 for _ in range(MAX_M+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_matrix_v2 = Blending_with_2d_matrix_v2(MAX_F, MAX_D, MAX_M, learning_rate)\n","    trainer_matrix_v2.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix v2(2d matrix for d and m) avg score: {-trainer_matrix_v2.total_score/trainer_matrix_v2.count}\")\n","    # plot_score(trainer_matrix_v2.logs)\n","\n","    alpha_matrix = [[[0.5 for _ in range(MAX_M+1)] for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","    beta_matrix = [[[0.75 for _ in range(MAX_M+1)] for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_matrix_v3 = Blending_with_2d_matrix_v3(MAX_F, MAX_D, MAX_M, learning_rate)\n","    trainer_matrix_v3.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix v3(3d matrix for d, f, m) avg score: {-trainer_matrix_v3.total_score/trainer_matrix_v3.count}\")\n","    # plot_score(trainer_matrix_v3.logs)\n","\n","    alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","    beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","    trainer_untrained = Blending_with_no_training(MAX_F, MAX_D, learning_rate)\n","    trainer_untrained.build_model_with_train(data, alpha_matrix_untrained, beta_matrix_untrained, n)\n","    print(f\"constant value avg score: {-trainer_untrained.total_score/trainer_untrained.count}\")\n","    # plot_score(trainer_untrained.logs)\n","\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"dAQBDio2VphV","executionInfo":{"status":"ok","timestamp":1707910874446,"user_tz":-660,"elapsed":1333084,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"9abfe387-4305-47b4-dbb6-353139dbaa22"},"id":"dAQBDio2VphV","execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["runing on text files\n","runing on file: alice29\n","\n","training end, one param(1 alpha and beta for all calculation) avg score: 2.8707573856870794\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.8189385585852538\n","training end, linear v2(alpha = p0 + p1 * d + p2 * f) avg score: 3.085739661895135\n","training end, linear v2 piecewise on d, first one row(alpha = p0 + p1 * d + p2 * f) avg score: 3.287650989737517\n","training end, linear v2 piecewise on d, first two rows(alpha = p0 + p1 * d + p2 * f) avg score: 3.3033853462067055\n","training end, linear v2 piecewise on f, first one column(alpha = p0 + p1 * d + p2 * f) avg score: 2.993828712358989\n","training end, matrix(2d matrix for d and f) avg score: 2.419605443704602\n","training end, matrix v2(2d matrix for d and m) avg score: 2.393137077158454\n","training end, matrix v3(3d matrix for d, f, m) avg score: 2.4175730870431464\n","constant value avg score: 2.4666009182437447\n","\n"]}]},{"cell_type":"code","source":["#@title repeat compression on alice with dynamic alpha and beta\n","\n","import random\n","MAX_F = 11\n","MAX_M = 100\n","n = 8\n","MAX_D = 7\n","learning_rate = 0.003\n","\n","print(\"runing on text files\")\n","\n","texts = {\"alice29\": alice}\n","\n","\n","alpha_matrix_linear = [[0.5, 0] for _ in range(MAX_D+1)]\n","beta_matrix_linear = [[0.75, 0] for _ in range(MAX_D+1)]\n","\n","alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","# load data\n","for i in range(4):\n","\n","    data = texts[\"alice29\"]\n","    print(f\"runing on iter: {i}\")\n","    print()\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F, MAX_D, learning_rate)\n","    trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear(alpha = p0d + p1d * f) avg score: {-trainer_linear.total_score/trainer_linear.count}\")\n","    # plot_score(trainer_linear.logs)\n","\n","\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F, MAX_D, learning_rate)\n","    trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix(2d matrix for d and f) avg score: {-trainer_matrix.total_score/trainer_matrix.count}\")\n","    # plot_score(trainer_matrix.logs)\n","\n","\n","\n","    trainer_untrained = Blending_with_no_training(MAX_F, MAX_D, learning_rate)\n","    trainer_untrained.build_model_with_train(data, alpha_matrix_untrained, beta_matrix_untrained, n)\n","    print(f\"constant value avg score: {-trainer_untrained.total_score/trainer_untrained.count}\")\n","    # plot_score(trainer_untrained.logs)\n","\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"qPr-eKhwpF_-","executionInfo":{"status":"ok","timestamp":1707916332961,"user_tz":-660,"elapsed":1739856,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"a92b1115-60a9-4eb7-e09f-ee757c1e1b4c"},"id":"qPr-eKhwpF_-","execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["runing on text files\n","runing on iter: 0\n","\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.225081718814546\n","training end, matrix(2d matrix for d and f) avg score: 2.5035696075807228\n","constant value avg score: 2.4666009182437447\n","\n","runing on iter: 1\n","\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.3580345435570402\n","training end, matrix(2d matrix for d and f) avg score: 2.3595574280499867\n","constant value avg score: 2.4666009182437447\n","\n","runing on iter: 2\n","\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.420724513143194\n","training end, matrix(2d matrix for d and f) avg score: 2.325065849474958\n","constant value avg score: 2.4666009182437447\n","\n","runing on iter: 3\n","\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.4440285672903777\n","training end, matrix(2d matrix for d and f) avg score: 2.3074723956746257\n","constant value avg score: 2.4666009182437447\n","\n"]}]},{"cell_type":"code","source":["#@title repeat compression on alice with dynamic alpha and constant beta\n","\n","import random\n","MAX_F = 11\n","MAX_M = 100\n","n = 8\n","MAX_D = 7\n","learning_rate = 0.003\n","\n","print(\"runing on text files\")\n","\n","texts = {\"alice29\": alice}\n","\n","\n","alpha_matrix_linear = [[0.5, 0] for _ in range(MAX_D+1)]\n","beta_matrix_linear = [[0.75, 0] for _ in range(MAX_D+1)]\n","\n","alpha_matrix = [[0.5 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","beta_matrix = [[0.75 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","alpha_matrix_untrained = [[0.5 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","beta_matrix_untrained = [[0.75 for _ in range(MAX_F+1)] for _ in range(MAX_D+1)]\n","\n","# load data\n","for i in range(4):\n","\n","    data = texts[\"alice29\"]\n","    print(f\"runing on iter: {i}\")\n","    print()\n","\n","    trainer_linear = Blending_with_linear_param(MAX_F, MAX_D, learning_rate)\n","    trainer_linear.build_model_with_train(data, alpha_matrix_linear, beta_matrix_linear, n)\n","    print(f\"training end, linear(alpha = p0d + p1d * f) avg score: {-trainer_linear.total_score/trainer_linear.count}\")\n","    # plot_score(trainer_linear.logs)\n","\n","\n","\n","    trainer_matrix = Blending_with_2d_matrix(MAX_F, MAX_D, learning_rate)\n","    trainer_matrix.build_model_with_train(data, alpha_matrix, beta_matrix, n)\n","    print(f\"training end, matrix(2d matrix for d and f) avg score: {-trainer_matrix.total_score/trainer_matrix.count}\")\n","    # plot_score(trainer_matrix.logs)\n","\n","\n","\n","    trainer_untrained = Blending_with_no_training(MAX_F, MAX_D, learning_rate)\n","    trainer_untrained.build_model_with_train(data, alpha_matrix_untrained, beta_matrix_untrained, n)\n","    print(f\"constant value avg score: {-trainer_untrained.total_score/trainer_untrained.count}\")\n","    # plot_score(trainer_untrained.logs)\n","\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"QXI3VQ76kxwn","executionInfo":{"status":"ok","timestamp":1707912296443,"user_tz":-660,"elapsed":1040487,"user":{"displayName":"Zixuan Cheng","userId":"09295786405145080952"}},"outputId":"724b8892-e4ab-4ad0-a680-d811f780fefa"},"id":"QXI3VQ76kxwn","execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["runing on text files\n","runing on iter: 0\n","\n","training end, linear(alpha = p0d + p1d * f) avg score: 2.8189385585852538\n","training end, matrix(2d matrix for d and f) avg score: 2.419605443704602\n","constant value avg score: 2.4666009182437447\n","\n","runing on iter: 1\n","\n","training end, linear(alpha = p0d + p1d * f) avg score: 3.0922692395760696\n","training end, matrix(2d matrix for d and f) avg score: 2.4951052792922566\n","constant value avg score: 2.4666009182437447\n","\n","runing on iter: 2\n","\n","training end, linear(alpha = p0d + p1d * f) avg score: 3.15923471530505\n","training end, matrix(2d matrix for d and f) avg score: 2.5513171255342626\n","constant value avg score: 2.4666009182437447\n","\n","runing on iter: 3\n","\n","training end, linear(alpha = p0d + p1d * f) avg score: 3.1959247860623123\n","training end, matrix(2d matrix for d and f) avg score: 2.587484260695528\n","constant value avg score: 2.4666009182437447\n","\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/czx20010217/pubmed_typos/blob/main/word_models/blending.ipynb","timestamp":1704179776075}],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":5}